<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Ben Chuanlong Du's Blog - AI</title><link href="https://misc.legendu.net/" rel="alternate"></link><link href="https://misc.legendu.net/feeds/ai.atom.xml" rel="self"></link><id>https://misc.legendu.net/</id><updated>2025-06-04T14:28:06-07:00</updated><subtitle>It is never too late to learn.</subtitle><entry><title>Data Sources</title><link href="https://misc.legendu.net/blog/data-sources/" rel="alternate"></link><published>2019-02-27T12:25:08-08:00</published><updated>2025-06-04T14:28:06-07:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2019-02-27:/blog/data-sources/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="data-platforms"&gt;Data Platforms&lt;/h2&gt;
&lt;p&gt;https://www.oxen.ai/&lt;/p&gt;
&lt;p&gt;https://github.com/quiltdata/quilt&lt;/p&gt;
&lt;p&gt;https://registry.opendata.aws/&lt;/p&gt;
&lt;p&gt;https://www.google.com/publicdata/directory&lt;/p&gt;
&lt;p&gt;https://proxycrawl.com&lt;/p&gt;
&lt;h2 id="data-for-computer-vison"&gt;&lt;a href="http://www.legendu.net/misc/blog/data-for-computer-vision-research/"&gt;Data for Computer Vison&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="data-for-nlp"&gt;&lt;a href="http://www.legendu.net/misc/blog/data-for-nlp-research/"&gt;Data for NLP …&lt;/a&gt;&lt;/h2&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="data-platforms"&gt;Data Platforms&lt;/h2&gt;
&lt;p&gt;https://www.oxen.ai/&lt;/p&gt;
&lt;p&gt;https://github.com/quiltdata/quilt&lt;/p&gt;
&lt;p&gt;https://registry.opendata.aws/&lt;/p&gt;
&lt;p&gt;https://www.google.com/publicdata/directory&lt;/p&gt;
&lt;p&gt;https://proxycrawl.com&lt;/p&gt;
&lt;h2 id="data-for-computer-vison"&gt;&lt;a href="http://www.legendu.net/misc/blog/data-for-computer-vision-research/"&gt;Data for Computer Vison&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="data-for-nlp"&gt;&lt;a href="http://www.legendu.net/misc/blog/data-for-nlp-research/"&gt;Data for NLP&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="data-for-learning-to-rank"&gt;&lt;a href="http://www.legendu.net/misc/blog/data-for-learning-to-rank"&gt;Data for Learning to Rank&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="financial-data"&gt;&lt;a href="http://www.legendu.net/misc/blog/financial-data"&gt;Financial Data&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="curated-datasets-observable"&gt;&lt;a href="https://observablehq.com/@observablehq/curated-datasets"&gt;Curated Datasets @ Observable&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="education"&gt;Education&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://nces.ed.gov/programs/edge/Geographic/DistrictBoundaries"&gt;Educational Demographic and Geographic Estimates&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="misc"&gt;Misc&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://www2.informatik.uni-freiburg.de/~cziegler/BX/"&gt;Book-Crossing Dataset&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;https://divvy-tripdata.s3.amazonaws.com/index.html&lt;/p&gt;
&lt;h2 id="data-sources"&gt;Data Sources&lt;/h2&gt;
&lt;h3 id="the-us-national-center"&gt;The US National Center&lt;/h3&gt;
&lt;p&gt;Education Statistics Data on educational institutions and education demographics from the US and around the world.&lt;/p&gt;
&lt;h3 id="the-uk-data-centre"&gt;The UK Data Centre&lt;/h3&gt;
&lt;p&gt;The UK’s largest collection of social, economic and population data.&lt;/p&gt;
&lt;h3 id="fivethirtyeight"&gt;FiveThirtyEight&lt;/h3&gt;
&lt;p&gt;A large number of polls providing data on public opinion of political and sporting issues.&lt;/p&gt;
&lt;h3 id="fbi-uniform-crime-reporting"&gt;FBI Uniform Crime Reporting&lt;/h3&gt;
&lt;p&gt;The FBI is responsible for compiling and publishing national crime statistics, with free data available at national, state and county level.&lt;/p&gt;
&lt;h3 id="bureau-of-justice"&gt;Bureau of Justice&lt;/h3&gt;
&lt;p&gt;Here you can find data on law enforcement agencies, jails, parole and probation agencies and courts.&lt;/p&gt;
&lt;h3 id="qlick-data-market"&gt;Qlick Data Market&lt;/h3&gt;
&lt;p&gt;Offers a free package with access to datasets covering world population, currencies, development indicators and weather data.&lt;/p&gt;
&lt;h3 id="nasa-exoplanet-archive"&gt;NASA Exoplanet Archive&lt;/h3&gt;
&lt;p&gt;Public datasets covering planets and stars gathered by NASA’s space exploration missions.&lt;/p&gt;
&lt;h3 id="un-comtrade-database"&gt;UN Comtrade Database&lt;/h3&gt;
&lt;p&gt;Statistics compiled and published by the United Nations on international trade. 
Includes Comtrade Lab which is a showcase of how cutting edge analytics and tools are used to extract value from the data.&lt;/p&gt;
&lt;h3 id="financial-times-market-data"&gt;Financial Times Market Data&lt;/h3&gt;
&lt;p&gt;Up to date information on financial markets from around the world, including stock price indexes, commodities and foreign exchange.&lt;/p&gt;
&lt;h3 id="google-trends"&gt;Google Trends&lt;/h3&gt;
&lt;p&gt;Examine and analyze data on internet search activity and trending news stories around the world.&lt;/p&gt;
&lt;h3 id="twitter"&gt;Twitter&lt;/h3&gt;
&lt;p&gt;The advantage Twitter has over the others are that most conversations are public. 
This means that huge amounts of data is available through their API on who is talking about what, where, when and why.&lt;/p&gt;
&lt;h3 id="google-scholar"&gt;Google Scholar&lt;/h3&gt;
&lt;p&gt;Entire texts of academic papers, journals, books and legal case law.&lt;/p&gt;
&lt;h3 id="instagram"&gt;Instagram&lt;/h3&gt;
&lt;p&gt;As with Twitter, 
Instagram posts and conversations are public by default. 
Their APIs allow likes, mentions and business details to be analyzed.&lt;/p&gt;
&lt;h3 id="opencorporates"&gt;OpenCorporates&lt;/h3&gt;
&lt;p&gt;The world’s largest open database of companies.&lt;/p&gt;
&lt;h3 id="glassdoor-api"&gt;Glassdoor API&lt;/h3&gt;
&lt;p&gt;Information about job vacancies, candidates, salaries and employee satisfaction is available through their developer API.&lt;/p&gt;
&lt;h3 id="imdb-datasets"&gt;IMDB Datasets&lt;/h3&gt;
&lt;p&gt;Datasets in a number of formats drawn from the web’s largest resource on movies, television and people working in those industries.&lt;/p&gt;
&lt;h3 id="openlibrary"&gt;OpenLibrary&lt;/h3&gt;
&lt;p&gt;Data Dumps Datasets on books including catalogs from libraries around the world&lt;/p&gt;
&lt;h3 id="labelled-faces-in-the-wild"&gt;Labelled Faces in the Wild&lt;/h3&gt;
&lt;p&gt;13,000 collated and labeled images of human faces, for use in developing applications involving facial recognition.&lt;/p&gt;
&lt;h3 id="microsoft-marco"&gt;Microsoft Marco&lt;/h3&gt;
&lt;p&gt;Microsoft’s open machine learning datasets for training systems in reading comprehension and question answering.&lt;/p&gt;
&lt;h3 id="machine-learning-dataset-repository"&gt;Machine Learning Dataset Repository&lt;/h3&gt;
&lt;p&gt;Collection of open datasets contributed by data scientists involved in machine learning projects.&lt;/p&gt;
&lt;h3 id="ebay-market-data"&gt;eBay Market Data&lt;/h3&gt;
&lt;p&gt;Insights Data on millions of online sales and auctions from eBay&lt;/p&gt;
&lt;h3 id="natural-history-museum-data-portal"&gt;Natural History Museum Data Portal&lt;/h3&gt;
&lt;p&gt;Information on nearly 4 million historical specimens in the London museum’s collection, 
as well as scientific sound recordings of the natural world.&lt;/p&gt;
&lt;h3 id="cern-open-data"&gt;CERN Open Data&lt;/h3&gt;
&lt;p&gt;More than one petabyte of data from particle physics experiments carried out by CERN.&lt;/p&gt;
&lt;h3 id="one-million-audio-cover-images"&gt;One Million Audio Cover Images&lt;/h3&gt;
&lt;p&gt;Dataset hosted at archive.org covering music released around the world, for use in image processing research&lt;/p&gt;
&lt;h3 id="complete-public-reddit-comments-corpus"&gt;Complete Public Reddit Comments Corpus&lt;/h3&gt;
&lt;p&gt;Over one billion public comments posted to Reddit between 2007 and 2015, for training language algorithms&lt;/p&gt;
&lt;h3 id="microsoft-azure-data-markets-free-datasets"&gt;Microsoft Azure Data Markets Free Datasets&lt;/h3&gt;
&lt;p&gt;Freely available datasets covering everything from agriculture to weather&lt;/p&gt;
&lt;h3 id="irish-electric-vehicle-charge-point-status"&gt;Irish Electric Vehicle Charge Point Status&lt;/h3&gt;
&lt;p&gt;Collates data from the body which oversees the network of EV charge points across the Republic of Ireland and Northern Ireland.&lt;/p&gt;
&lt;h3 id="londonair"&gt;LondonAir&lt;/h3&gt;
&lt;p&gt;Pollution and air quality data from across London&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/awesomedata/awesome-public-datasets"&gt;awesome-public-datasets&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;https://www.forbes.com/sites/bernardmarr/2018/02/26/big-data-and-ai-30-amazing-and-free-public-data-sources-for-2018/#116850d65f8a&lt;/p&gt;
&lt;p&gt;https://www.forbes.com/sites/bernardmarr/2016/02/12/big-data-35-brilliant-and-free-data-sources-for-2016/#1cd2291ab54d&lt;/p&gt;
&lt;p&gt;https://www.columnfivemedia.com/100-best-free-data-sources-infographic&lt;/p&gt;
&lt;p&gt;https://infogram.com/blog/free-data-sources/&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="machine learning"></category><category term="data"></category><category term="resource"></category><category term="financial data"></category></entry><entry><title>Rust for Machine Learning</title><link href="https://misc.legendu.net/blog/rust-for-machine-learning/" rel="alternate"></link><published>2020-01-27T22:43:48-08:00</published><updated>2023-08-12T13:11:33-07:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-01-27:/blog/rust-for-machine-learning/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="candle"&gt;&lt;a href="https://github.com/huggingface/candle"&gt;candle&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/huggingface/candle"&gt;Candle&lt;/a&gt;
is a minimalist ML framework for Rust with a focus on easiness of use and on performance (including GPU support).&lt;/p&gt;
&lt;h2 id="dfdx"&gt;&lt;a href="https://github.com/coreylowman/dfdx"&gt;dfdx&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/coreylowman/dfdx"&gt;dfdx&lt;/a&gt;
is an ergonomics &amp;amp; safety focused deep learning …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="candle"&gt;&lt;a href="https://github.com/huggingface/candle"&gt;candle&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/huggingface/candle"&gt;Candle&lt;/a&gt;
is a minimalist ML framework for Rust with a focus on easiness of use and on performance (including GPU support).&lt;/p&gt;
&lt;h2 id="dfdx"&gt;&lt;a href="https://github.com/coreylowman/dfdx"&gt;dfdx&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/coreylowman/dfdx"&gt;dfdx&lt;/a&gt;
is an ergonomics &amp;amp; safety focused deep learning in Rust.&lt;/p&gt;
&lt;h2 id="burn"&gt;&lt;a href="https://github.com/burn-rs/burn"&gt;burn&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/burn-rs/burn"&gt;burn&lt;/a&gt;
is a flexible and comprehensive deep learning framework in Rust
.
It can leverage multiple backends so you can choose the best engine for your workload.&lt;/p&gt;
&lt;h2 id="tch-rs"&gt;&lt;a href="https://github.com/LaurentMazare/tch-rs"&gt;tch-rs&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/LaurentMazare/tch-rs"&gt;tch-rs&lt;/a&gt;
is a Rust bindings for the C++ API of PyTorch.&lt;/p&gt;
&lt;h2 id="linfa"&gt;&lt;a href="https://github.com/rust-ml/linfa"&gt;linfa&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="bastionai"&gt;&lt;a href="https://github.com/mithril-security/bastionai"&gt;BastionAI&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;BastionAI is a confidential deep learning framework 
that enables data scientists to train their models on sensitive data, 
while ensuring the data providers no third party will have access to these.&lt;/p&gt;
&lt;h2 id="tract"&gt;&lt;a href="https://github.com/sonos/tract"&gt;tract&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/sonos/tract"&gt;tract&lt;/a&gt;
is a Neural Network inference toolkit developed in Rust. 
It can read Tensorflow 1, ONNX or NNEF, 
optimize them and run data through them.&lt;/p&gt;
&lt;h2 id="tensorflowrust"&gt;&lt;a href="https://github.com/tensorflow/rust"&gt;tensorflow/rust&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/tensorflow/rust"&gt;tensorflow/rust&lt;/a&gt;
is a Rust language binding for TensorFlow.&lt;/p&gt;
&lt;h2 id="other-python-libraries-based-on-rust-implementation"&gt;Other Python Libraries Based-on Rust Implementation&lt;/h2&gt;
&lt;h2 id="safetensors"&gt;&lt;a href="https://github.com/huggingface/safetensors"&gt;safetensors&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/huggingface/safetensors"&gt;SafeTensors&lt;/a&gt;
implements a new simple format for storing tensors safely (as opposed to pickle) and that is still fast (zero-copy).&lt;/p&gt;
&lt;h2 id="tokenizers"&gt;&lt;a href="https://github.com/huggingface/tokenizers"&gt;tokenizers&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/huggingface/tokenizers"&gt;Tokenizers&lt;/a&gt;
provides an implementation of today's most used tokenizers, with a focus on performance and versatility.&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://rustrepo.com/catalog/rust-machine-learning_newest_1"&gt;Rust Machine learning Resources&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/e-tornike/best-of-ml-rust"&gt;Best of ML Rust&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/vaaaaanquish/Awesome-Rust-MachineLearning#deep-neural-network"&gt;vaaaaanquish/Awesome-Rust-MachineLearning&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.arewelearningyet.com/"&gt;Are We Learning Yes&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="Rust"></category><category term="deep learning"></category></entry><entry><title>Hardware for AI</title><link href="https://misc.legendu.net/blog/hardware-for-ai/" rel="alternate"></link><published>2020-03-11T11:56:25-07:00</published><updated>2023-07-30T22:22:48-07:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-03-11:/blog/hardware-for-ai/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/apache/incubator-tvm"&gt;TVM&lt;/a&gt;
for deep learning is kind of like LLVM for programming languages.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://developer.nvidia.com/tensorrt"&gt;Nvidia TensorRT&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="embeddededge-ai"&gt;Embedded/Edge AI&lt;/h2&gt;
&lt;p&gt;Jetson Nano&lt;/p&gt;
&lt;p&gt;Google Coral&lt;/p&gt;
&lt;p&gt;Intel Neural Compute Stick 2&lt;/p&gt;
&lt;p&gt;https://heartbeat.fritz.ai/edge-tpu-google-coral-usb-accelerator-cf0d79c7ec56 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/apache/incubator-tvm"&gt;TVM&lt;/a&gt;
for deep learning is kind of like LLVM for programming languages.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://developer.nvidia.com/tensorrt"&gt;Nvidia TensorRT&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="embeddededge-ai"&gt;Embedded/Edge AI&lt;/h2&gt;
&lt;p&gt;Jetson Nano&lt;/p&gt;
&lt;p&gt;Google Coral&lt;/p&gt;
&lt;p&gt;Intel Neural Compute Stick 2&lt;/p&gt;
&lt;p&gt;https://heartbeat.fritz.ai/edge-tpu-google-coral-usb-accelerator-cf0d79c7ec56&lt;/p&gt;
&lt;p&gt;https://blog.usejournal.com/google-coral-edge-tpu-vs-nvidia-jetson-nano-a-quick-deep-dive-into-edgeai-performance-bc7860b8d87a&lt;/p&gt;
&lt;p&gt;https://mp.weixin.qq.com/s/NtnTQIecFq1L1ffPnirMIA&lt;/p&gt;
&lt;h2 id="tutorials"&gt;Tutorials&lt;/h2&gt;
&lt;p&gt;https://mp.weixin.qq.com/s/mId2Y4Do2k67pKq1WDVm8A&lt;/p&gt;
&lt;p&gt;https://mp.weixin.qq.com/s/IraqkibhGciJ37sKoGxyvg&lt;/p&gt;
&lt;p&gt;CPU：i9-10920X
显卡GPU：七彩虹RTX3090 Advance
内存：芝奇幻光戟16G x 4共64G
主板：华硕X299-DELUXE PRIME
固态硬盘：1TB西数NVME SSD + 1TB三星870QVO SATA SSD
机械硬盘：希捷EXOS 12TB氦气盘
电源：海盗船AX1200i 1200W模组电源
散热器：海盗船H100X240水冷 + 若干120机箱风扇
机箱：海盗船AIR540 E-ATX机箱&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;最佳性价比GPU：RTX 2070&lt;/li&gt;
&lt;li&gt;避免的坑：所有Tesla、Quadro、创始人版（Founders Edition）的显卡，还有Titan RTX、Titan V、Titan XP&lt;/li&gt;
&lt;li&gt;高性价比：RTX 2070（高端），RTX 2060或GTX 1060 (6GB)（中低端）&lt;/li&gt;
&lt;li&gt;穷人之选：GTX 1060 (6GB)&lt;/li&gt;
&lt;li&gt;破产之选：GTX 1050 Ti（4GB），或者CPU（原型）+ AWS / TPU（训练），或者Colab&lt;/li&gt;
&lt;li&gt;Kaggle竞赛：RTX 2070&lt;/li&gt;
&lt;li&gt;计算机视觉或机器翻译研究人员：采用鼓风设计的GTX 2080 Ti，如果训练非常大的网络，请选择RTX Titans&lt;/li&gt;
&lt;li&gt;NLP研究人员：RTX 2080 Ti&lt;/li&gt;
&lt;li&gt;已经开始研究深度学习：RTX 2070起步，以后按需添置更多RTX 2070&lt;/li&gt;
&lt;li&gt;尝试入门深度学习：GTX 1050 Ti（2GB或4GB显存）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;https://www.digitalstorm.com/configurator.asp?id=3137888&lt;/p&gt;
&lt;p&gt;Need UPS in case of power interruption &lt;/p&gt;
&lt;p&gt;https://list.jd.com/Search?keyword=%E5%8F%B0%E5%BC%8F%E6%9C%BA%E6%96%AD%E7%94%B5%E4%BF%9D%E6%8A%A4%E7%94%B5%E6%BA%90&amp;amp;enc=utf-8&amp;amp;spm=2.1.11&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=mOY_Dbyq6OY"&gt;Google Just Turned the Raspberry Pi into a Supercomputer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="AI"></category><category term="AI"></category><category term="deep learning"></category><category term="machine learning"></category><category term="data science"></category><category term="hardware"></category><category term="GPU"></category><category term="TPU"></category><category term="Jetson Nano"></category><category term="Google Coral"></category></entry><entry><title>Tips on FeatureTools</title><link href="https://misc.legendu.net/blog/tips-on-featuretools/" rel="alternate"></link><published>2020-01-18T15:12:37-08:00</published><updated>2022-08-08T11:09:04-07:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-01-18:/blog/tips-on-featuretools/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://medium.com/dataexplorations/tool-review-can-featuretools-simplify-the-process-of-feature-engineering-5d165100b0c3"&gt;Tool Review: Lessons learned from using FeatureTools to simplify the process of Feature Engineering&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/FeatureLabs/predict-restaurant-rating/blob/master/predict-restaurant-rating.ipynb"&gt;Predicting the rating a reviewer will give a restaurant using Featuretools and the nlp-primitives library&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://blog.featurelabs.com/natural-language-processing-featuretools/"&gt;Natural Language …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://medium.com/dataexplorations/tool-review-can-featuretools-simplify-the-process-of-feature-engineering-5d165100b0c3"&gt;Tool Review: Lessons learned from using FeatureTools to simplify the process of Feature Engineering&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/FeatureLabs/predict-restaurant-rating/blob/master/predict-restaurant-rating.ipynb"&gt;Predicting the rating a reviewer will give a restaurant using Featuretools and the nlp-primitives library&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://blog.featurelabs.com/natural-language-processing-featuretools/"&gt;Natural Language Processing for Automated Feature Engineering&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/FeatureLabs/nlp_primitives"&gt;NLP Primitives&lt;/a&gt;&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="feature"></category><category term="FeatureTools"></category><category term="nlp_primitive"></category></entry><entry><title>Loss Functions for Machine Learning Models</title><link href="https://misc.legendu.net/blog/ai-loss-function/" rel="alternate"></link><published>2013-03-07T11:12:36-08:00</published><updated>2022-05-08T16:26:29-07:00</updated><author><name>Ben Chuanlong Du</name></author><id>tag:misc.legendu.net,2013-03-07:/blog/ai-loss-function/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="tips-and-traps"&gt;Tips and Traps&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;A Loss function is always non-negative. 
    If you get a negative loss when training a model,
    there must be something wrong with the code. 
    For example, 
    maybe you …&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="tips-and-traps"&gt;Tips and Traps&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;A Loss function is always non-negative. 
    If you get a negative loss when training a model,
    there must be something wrong with the code. 
    For example, 
    maybe you chosed a loss function incorrectly.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="loss-functions"&gt;Loss Functions&lt;/h2&gt;
&lt;h3 id="0-1-loss"&gt;0-1 Loss&lt;/h3&gt;
&lt;p&gt;Or sometimes called binary loss function.&lt;/p&gt;
&lt;h3 id="sse"&gt;SSE&lt;/h3&gt;
&lt;h3 id="cross-entropy"&gt;Cross-entropy&lt;/h3&gt;
&lt;h3 id="negative-log-likelihood"&gt;Negative Log Likelihood&lt;/h3&gt;
&lt;h2 id="comparisons-of-loss-functions"&gt;Comparisons of Loss Functions&lt;/h2&gt;
&lt;h3 id="cross-entropy-vs-negative-log-likelihood"&gt;Cross Entropy vs Negative Log Likelihood&lt;/h3&gt;
&lt;p&gt;Please refer to
&lt;a href="https://www.legendu.net/misc/blog/entropy"&gt;Entropy&lt;/a&gt;
for detailed discussions.&lt;/p&gt;
&lt;h3 id="mse-l2-loss-vs-l1-loss"&gt;MSE (L2 Loss) vs L1 Loss&lt;/h3&gt;
&lt;p&gt;MSE is a L2 loss function. 
Both L1 and L2 loss functions are special cases of &lt;span class="math"&gt;\(L_p\)&lt;/span&gt; (&lt;span class="math"&gt;\(p&amp;gt;0\)&lt;/span&gt;) loss functions.
&lt;span class="math"&gt;\(L_p\)&lt;/span&gt; loss functions are typicall used for regression problems.
Compared to L1 loss, 
L2 loss gives larger weights on larger (absolute) errors.
In many real applications, 
we often observe the following 2 phenomena.
    1. Real/training data is not uniformly distribution across all scenarios.
        There are often a lot more samples which generate small respoonse values. 
    2. During training, large response values often have larger errors.
If samples generating larger response values (errors)
are not important (or can be treated as outliers)
then a L1 loss (or even &lt;span class="math"&gt;\(L_p\)&lt;/span&gt; where &lt;span class="math"&gt;\(0&amp;lt;p&amp;lt;1\)&lt;/span&gt;) is a better choice than a L2 loss.
If samples generating larger response values (errors)
cannot be treated as outliers 
or even more important, 
then a L2 loss (or even &lt;span class="math"&gt;\(L_p\)&lt;/span&gt; where &lt;span class="math"&gt;\(p&amp;gt;1\)&lt;/span&gt;) is better than a L1 loss.&lt;/p&gt;
&lt;h2 id="loss-functions-in-pytorch"&gt;Loss Functions in PyTorch&lt;/h2&gt;
&lt;p&gt;nn.CrossEntropyLoss&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pytorch.org/docs/stable/nn.html#nllloss"&gt;nn.NLLLoss&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;https://discuss.pytorch.org/t/what-is-the-difference-between-using-the-cross-entropy-loss-and-using-log-softmax-followed-by-nll-loss/14825&lt;/p&gt;
&lt;p&gt;https://discuss.pytorch.org/t/build-your-own-loss-function-in-pytorch/235&lt;/p&gt;
&lt;p&gt;https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;https://pytorch.org/docs/stable/nn.html&lt;/p&gt;
&lt;p&gt;&lt;a href="https://neptune.ai/blog/pytorch-loss-functions"&gt;PyTorch Loss Functions: The Ultimate Guide&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;https://gombru.github.io/2019/04/03/ranking_loss/&lt;/p&gt;
&lt;p&gt;https://discuss.pytorch.org/t/writing-warp-loss-layer/3715&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js','mhchem.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI"></category><category term="AI"></category><category term="loss function"></category><category term="statistics"></category><category term="machine learning"></category><category term="cross entry"></category><category term="log likelihood"></category></entry><entry><title>Entropy</title><link href="https://misc.legendu.net/blog/entropy/" rel="alternate"></link><published>2013-04-22T12:21:46-07:00</published><updated>2022-05-08T16:26:29-07:00</updated><author><name>Ben Chuanlong Du</name></author><id>tag:misc.legendu.net,2013-04-22:/blog/entropy/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="entropy-and-related-concepts"&gt;Entropy and Related Concepts&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Entropy&lt;/li&gt;
&lt;li&gt;Shannon Entropy&lt;/li&gt;
&lt;li&gt;Cross Entropy&lt;/li&gt;
&lt;li&gt;K-L divergence&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="tips"&gt;Tips&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The entropy concept was first introduced for discrete distributions (called Shannon entropy),
    which is defined as
    &lt;div class="math"&gt;$$H(X) = E …&lt;/div&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="entropy-and-related-concepts"&gt;Entropy and Related Concepts&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Entropy&lt;/li&gt;
&lt;li&gt;Shannon Entropy&lt;/li&gt;
&lt;li&gt;Cross Entropy&lt;/li&gt;
&lt;li&gt;K-L divergence&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="tips"&gt;Tips&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The entropy concept was first introduced for discrete distributions (called Shannon entropy),
    which is defined as
    &lt;div class="math"&gt;$$H(X) = E[log(\frac{1}{f(x)})]$$&lt;/div&gt;
    where &lt;span class="math"&gt;\(X\)&lt;/span&gt; stands for a discrete random variable (distribution)
    and &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt; is the probability density function of &lt;span class="math"&gt;\(X\)&lt;/span&gt;.
    Shannon Entropy is non-negative.
    It is zero if and only if the discrete distribution is degenerate 
    (all mass concentrate on one point).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Shannon entropy is proven to be the lower bound of bits per symbol
    (&lt;span class="math"&gt;\(log_2(x)\)&lt;/span&gt; is used instead of &lt;span class="math"&gt;\(log(x)\)&lt;/span&gt;)
    to transfer identifiable information from a source to a destination 
    through a communication channel without data loss. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Shanno entropy is equivalent to entropy in thermodynamics (an area of physics).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Entropy is a good metric to measure the magnitude of "information" in features/variables in machine learning.
    It can be used to filter out non-useful features/variables.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The entropy concept can be extended to continuous distributions.
    However, 
    the entropy of a continuous distribution can be negative.
    As a matter of fact,
    the entropy of a continuous distribution has a range of &lt;span class="math"&gt;\((-\infty, \infty)\)&lt;/span&gt;.
    Taking the exponential distribution with the density function &lt;span class="math"&gt;\(\frac{1}{\mu}e^{-\frac{x}{\mu}}\)&lt;/span&gt; as example,
    its entropy is &lt;span class="math"&gt;\(log(\mu)+1\)&lt;/span&gt; which goes to &lt;span class="math"&gt;\(\infty\)&lt;/span&gt; as &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; goes to &lt;span class="math"&gt;\(\infty\)&lt;/span&gt; 
    and it goes to &lt;span class="math"&gt;\(-\infty\)&lt;/span&gt; as &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; goes to 0.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For the reason in bullet point 4,
    entropy is not a good measure for continuous distributions
    Cross-entropy and K-L divergence are more commonly used for both discrete and continuous distributions.
    The cross-entropy of a distribution q with respect to p is defined as 
    &lt;div class="math"&gt;$$H(p, q) = E_p[-log(q)]$$&lt;/div&gt;
    And the K-L divergence (also called relative entropy) is defined as
    &lt;div class="math"&gt;$$D_{KL}(p, q) = E_p[log(\frac{1}{q}) - log(\frac{1}{p})] = H(p, q) - H(p)$$&lt;/div&gt;
    Notice that the K-L divergence is always non-negative.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In a multi-class classification problem,
    the following are equivalent.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;minimizing the cross-entropy &lt;/li&gt;
&lt;li&gt;minimizing the K-L divergence&lt;/li&gt;
&lt;li&gt;maximizing the log likelihood of the corresponding multi-nomial distribution&lt;/li&gt;
&lt;li&gt;minimizing the negative log likelihood (NLL) of the corresponding multi-nomial distribution&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The above conclusion suggests that the cross-entropy loss, K-L loss and the NLL loss are equivalent.
However, 
be aware that PyTorch defines cross-entropy loss to be different from the NLL loss.
The cross-entropy loss in PyTorch is defined on the raw output of a neural network layer
while the NLL loss is defined on the output of a log softmax layer.
This means that in PyTorch the cross-entropy loss is equivalent to log_softmax + nll_loss.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="misc"&gt;Misc&lt;/h2&gt;
&lt;p&gt;Fisher information explanation&lt;/p&gt;
&lt;p&gt;likelihood based tests: LRT, wald, score &lt;/p&gt;
&lt;p&gt;expected fisher, &lt;/p&gt;
&lt;p&gt;observed fisher (sum, log, law of large number)&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/"&gt;A Gentle Introduction to Cross-Entropy for Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js','mhchem.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI"></category><category term="statistics"></category><category term="AI"></category><category term="entropy"></category><category term="data science"></category><category term="machine learning"></category><category term="deep learning"></category><category term="Shannon entropy"></category><category term="cross entropy"></category><category term="K-L divergence"></category></entry><entry><title>Handling Categorical Variables in Machine Learning</title><link href="https://misc.legendu.net/blog/handling-categorical-variables-in-machine-learning/" rel="alternate"></link><published>2019-12-24T10:44:18-08:00</published><updated>2022-03-26T22:21:23-07:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2019-12-24:/blog/handling-categorical-variables-in-machine-learning/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Categorical variables are very common in a machine learning project.
On a high level,
there are two ways to handle a categorical variable.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Drop a categorical variable 
    if a categorical variable …&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Categorical variables are very common in a machine learning project.
On a high level,
there are two ways to handle a categorical variable.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Drop a categorical variable 
    if a categorical variable won't help the model 
    and especially when the categorical variable has a large cardinality.
    User id is such an example
    when you build a user-level model.
    Of course,
    you can use feature hashing to reduce the dimension/cardinality of a categorical variable,
    and let the training process decides 
    whether the categorical variable should be included into the model or not.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Encode a categorical variable. 
    Below are some popular ways of encoding a categorical variable.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;- One-Hot Encoding
- Label Encoding
- Target Encoding
- Feature Hashing
- Weight of Evidence
- Light G-Boost Encoding
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Please refer to 
&lt;a href="https://towardsdatascience.com/know-about-categorical-encoding-even-new-ones-c266227b9cbd"&gt;Know about Categorical Encoding, even New Ones!&lt;/a&gt;
and
&lt;a href="https://medium.com/swlh/dealing-with-categorical-variables-in-machine-learning-4401b949b093"&gt;Dealing with Categorical Variables in Machine Learning&lt;/a&gt;
for more detailed discussions.
Notice that LightGBM has it's own way (Light G-Boost Encoding)
of handling categorical variables.
Please refer to
&lt;a href="http://www.legendu.net/misc/blog/handle-categorical-variables-in-lightgbm"&gt;Handle Categorical Variables in LightGBM&lt;/a&gt;
for more discussions
.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://towardsdatascience.com/know-about-categorical-encoding-even-new-ones-c266227b9cbd"&gt;Know about Categorical Encoding, even New Ones!&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://medium.com/swlh/dealing-with-categorical-variables-in-machine-learning-4401b949b093"&gt;Dealing with Categorical Variables in Machine Learning&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</content><category term="AI"></category><category term="AI"></category><category term="machine learning"></category><category term="data science"></category><category term="categorical variables"></category><category term="encoding"></category><category term="One-hot"></category></entry><entry><title>Tips on LightGBM</title><link href="https://misc.legendu.net/blog/tips-on-lightgbm/" rel="alternate"></link><published>2019-12-03T01:26:29-08:00</published><updated>2022-03-26T22:21:23-07:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2019-12-03:/blog/tips-on-lightgbm/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;It is strongly suggested that you load data into a pandas DataFrame
    and handle categorical variables
    by specifying a &lt;code&gt;dtype&lt;/code&gt; of &lt;code&gt;"category"&lt;/code&gt; for those categorical variables.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;df.cat_var&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;df.cat_var.astype …&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;It is strongly suggested that you load data into a pandas DataFrame
    and handle categorical variables
    by specifying a &lt;code&gt;dtype&lt;/code&gt; of &lt;code&gt;"category"&lt;/code&gt; for those categorical variables.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;df.cat_var&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;df.cat_var.astype&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;category&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This is the easiest way to handle categorical variables in LightGBM.
For more details,
please refer to 
&lt;a href="http://www.legendu.net/misc/blog/handle-categorical-variables-in-lightgbm"&gt;Handle Categorical Variables in LightGBM&lt;/a&gt;
.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The sklearn wrapper of LightGBM lag behind the development of sklearn. 
    Be aware of the latest supported version of sklearn 
    when you use sklearn wrapper of LightGBM.
    It is suggested that you use the original API of LightGBM to avoid version issues.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It is suggested that you always specify an validation dataset 
    when you train a model using the function &lt;code&gt;train&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;LightGBM supports distributed training on multiple machines (without Spark).&lt;/p&gt;
&lt;p&gt;https://github.com/microsoft/LightGBM/tree/master/examples/parallel_learning&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="hyper-parameter-tuning"&gt;Hyper Parameter Tuning&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://www.legendu.net/misc/blog/tips-on-optuna"&gt;Optuna&lt;/a&gt;
is a good framework for tuning hyper parameters.&lt;/p&gt;
&lt;p&gt;https://sites.google.com/view/lauraepp/parameters&lt;/p&gt;
&lt;p&gt;https://lightgbm.readthedocs.io/en/latest/Parameters.html&lt;/p&gt;
&lt;h2 id="gpu"&gt;GPU&lt;/h2&gt;
&lt;p&gt;https://lightgbm.readthedocs.io/en/latest/GPU-Tutorial.html&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://www.legendu.net/misc/blog/handle-categorical-variables-in-lightgbm"&gt;Handle Categorical Variables in LightGBM&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://lightgbm.readthedocs.io/en/latest/Parameters.html#parameters"&gt;Parameters&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://lightgbm.readthedocs.io/en/latest/Parameters.html#metric-parameters"&gt;Metric Parameters&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://lightgbm.readthedocs.io/en/latest/Experiments.html"&gt;LightGMB Benchmark&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.Dataset.html#lightgbm-dataset&lt;/p&gt;
&lt;p&gt;https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/sklearn.html&lt;/p&gt;
&lt;p&gt;https://www.kaggle.com/nicapotato/multi-class-lgbm-cv-and-seed-diversification&lt;/p&gt;
&lt;p&gt;https://sefiks.com/2018/10/13/a-gentle-introduction-to-lightgbm-for-applied-machine-learning/&lt;/p&gt;
&lt;p&gt;https://github.com/microsoft/LightGBM/blob/master/examples/python-guide/sklearn_example.py&lt;/p&gt;
&lt;p&gt;https://github.com/microsoft/LightGBM/tree/master/examples/python-guide&lt;/p&gt;
&lt;p&gt;https://www.kaggle.com/tapioca/multiclass-lightgbm&lt;/p&gt;
&lt;p&gt;https://lightgbm.readthedocs.io/en/latest/Features.html&lt;/p&gt;
&lt;p&gt;https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html&lt;/p&gt;
&lt;p&gt;https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.train.html&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="LightGBM"></category></entry><entry><title>Training Deep Neural Networks</title><link href="https://misc.legendu.net/blog/training-deep-neural-networks/" rel="alternate"></link><published>2020-01-21T13:44:37-08:00</published><updated>2021-10-08T12:57:31-07:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-01-21:/blog/training-deep-neural-networks/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="rules-of-thumb-for-training"&gt;Rules of Thumb for Training&lt;/h2&gt;
&lt;p&gt;https://arxiv.org/pdf/1206.5533.pdf**&lt;/p&gt;
&lt;p&gt;https://towardsdatascience.com/17-rules-of-thumb-for-building-a-neural-network-93356f9930af&lt;/p&gt;
&lt;p&gt;https://hackernoon.com/rules-of-thumb-for-deep-learning-5a3b6d4b0138&lt;/p&gt;
&lt;p&gt;https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw&lt;/p&gt;
&lt;p&gt;Batch size affects both …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="rules-of-thumb-for-training"&gt;Rules of Thumb for Training&lt;/h2&gt;
&lt;p&gt;https://arxiv.org/pdf/1206.5533.pdf**&lt;/p&gt;
&lt;p&gt;https://towardsdatascience.com/17-rules-of-thumb-for-building-a-neural-network-93356f9930af&lt;/p&gt;
&lt;p&gt;https://hackernoon.com/rules-of-thumb-for-deep-learning-5a3b6d4b0138&lt;/p&gt;
&lt;p&gt;https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw&lt;/p&gt;
&lt;p&gt;Batch size affects both the training/test speed and accuracy. 
A large batch size can speed up training but might reduce the accuracy. 
Generally speaking, 
try 64, 128, 256.
A too large batch size might also causes CPU/CUDA memory issues. 
For testing, 
you can use as large a batch size as possible as long as it does not run into memory issues. &lt;/p&gt;
&lt;h2 id="initialize-weights"&gt;Initialize Weights&lt;/h2&gt;
&lt;p&gt;The general rule for setting the weights in a neural network is to set them to be close to zero without being too small.&lt;/p&gt;
&lt;p&gt;Good practice is to start your weights in the range of [-y, y] where y=1/sqrt(n)
(n is the number of inputs to a given neuron).&lt;/p&gt;
&lt;p&gt;normal distribution to initialize the weights
The normal distribution should have a mean of 0 and a standard deviation of y=1/sqrt(n), where n is the number of inputs to NN&lt;/p&gt;
&lt;p&gt;https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch
How to initialize weights in PyTorch?&lt;/p&gt;
&lt;h2 id="nan-values"&gt;NAN Values&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Check whether there are NAN values in your raw data. 
    I encountered a tricky situation 
    where my training data (tensors) was prepared using pandas DataFrame
    and NAN values were generated due to missuse of the method &lt;code&gt;DataFrame.iterrows&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href="https://discuss.pytorch.org/t/nan-loss-coming-after-some-time/11568"&gt;Nan Loss coming after some time&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://discuss.pytorch.org/t/getting-nan-after-first-iteration-with-custom-loss/25929"&gt;Getting Nan after first iteration with custom loss&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="computer-vision-specific-tips"&gt;Computer Vision Specific Tips&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;explore your data&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;check image dimensions. 
    It is often a good idea to resize all images to a unify size before training.
    If some original images are really large (say, 4k images),
    it might be beneficial to downscaling images. 
    This has 2 potential benefits.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;dramatically increase training speed&lt;/li&gt;
&lt;li&gt;avoid generating "noisy" data caused by random crop data augumentation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="terminology"&gt;Terminology&lt;/h2&gt;
&lt;p&gt;1 Epoch = 1 Forward pass + 1 Backward pass for ALL training samples.
Batch Size = Number of training samples in 1 Forward/1 Backward pass. (With increase in Batch size, required memory space increases.)
Number of iterations = Number of passes i.e. 1 Pass = 1 Forward pass + 1 Backward pass (Forward pass and Backward pass are not counted differently.)&lt;/p&gt;
&lt;p&gt;In neural networks generally, an epoch is a single pass through the full training set. 
You don't just run through the training set once, 
it can take thousands of epochs for your backpropagation algorithm to converge on a combination of weights with an acceptable level of accuracy. 
Remember gradient descent only changes the weights by a small amount in the direction of improvement, 
so backpropagation can't get there by running through the training examples just once.&lt;/p&gt;
&lt;p&gt;https://mp.weixin.qq.com/s?__biz=MzUxNjcxMjQxNg==&amp;amp;mid=2247488694&amp;amp;idx=1&amp;amp;sn=167b8b9897165d3dcb285ffd12ff7aef&amp;amp;scene=21#wechat_redirect&lt;/p&gt;
&lt;p&gt;https://mp.weixin.qq.com/s/aW9yF15lPQWIrQTPu8ki2A&lt;/p&gt;
&lt;p&gt;https://jeffmacaluso.github.io/post/DeepLearningRulesOfThumb/&lt;/p&gt;
&lt;p&gt;https://pcc.cs.byu.edu/2017/10/02/practical-advice-for-building-deep-neural-networks/&lt;/p&gt;
&lt;p&gt;http://theorangeduck.com/page/neural-network-not-working&lt;/p&gt;
&lt;p&gt;https://zhuanlan.zhihu.com/p/59918821&lt;/p&gt;
&lt;p&gt;https://ml-cheatsheet.readthedocs.io/en/latest/index.html&lt;/p&gt;
&lt;p&gt;https://karpathy.github.io/2019/04/25/recipe/&lt;/p&gt;
&lt;p&gt;When the minibatch size is multiplied by k, multiply the learning rate by k.&lt;/p&gt;
&lt;h2 id="regularization"&gt;Regularization&lt;/h2&gt;
&lt;p&gt;DNNs are prone to overfitting because of the added layers of abstraction, 
which allow them to model rare dependencies in the training data. 
Regularization methods such as Ivakhnenko's unit pruning[29] 
or weight decay ( {\displaystyle \ell &lt;em 2&gt;{2}} \ell &lt;/em&gt;-regularization) 
or sparsity ( {\displaystyle \ell &lt;em 1&gt;{1}} \ell &lt;/em&gt;-regularization) can be applied during training to combat overfitting.[111] 
Alternatively dropout regularization randomly omits units from the hidden layers during training. 
This helps to exclude rare dependencies.[112] 
Finally, 
data can be augmented via methods such as cropping and rotating 
such that smaller training sets can be increased in size to reduce the chances of overfitting.[113]&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="machine learning"></category><category term="data science"></category><category term="deep learning"></category><category term="deep neural network"></category><category term="DNN"></category></entry><entry><title>Data for Computer Vision Research</title><link href="https://misc.legendu.net/blog/data-for-computer-vision-research/" rel="alternate"></link><published>2020-03-06T12:45:31-08:00</published><updated>2021-10-08T12:50:34-07:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-03-06:/blog/data-for-computer-vision-research/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="mnist"&gt;&lt;a href="http://yann.lecun.com/exdb/mnist/"&gt;MNIST&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research&lt;/p&gt;
&lt;p&gt;https://www.nist.gov/&lt;/p&gt;
&lt;h2 id="imagenet"&gt;&lt;a href="http://www.image-net.org/"&gt;ImageNet&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://gist.github.com/dclong/8537ea044cb5b44a33582927ddfc4c73"&gt;Mapping Between Class and Ids&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="raccoon-dataset-for-training-raccoon-detection"&gt;&lt;a href="https://github.com/dclong/raccoon_dataset"&gt;Raccoon Dataset for Training Raccoon Detection&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="coco-common-object-in-context"&gt;&lt;a href="https://cocodataset.org/#home"&gt;COCO - Common Object in Context&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;VOC-2012 &lt;/p&gt;
&lt;p&gt;&lt;a href="https://image-net.org/challenges/LSVRC/"&gt;ImageNet Large …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="mnist"&gt;&lt;a href="http://yann.lecun.com/exdb/mnist/"&gt;MNIST&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research&lt;/p&gt;
&lt;p&gt;https://www.nist.gov/&lt;/p&gt;
&lt;h2 id="imagenet"&gt;&lt;a href="http://www.image-net.org/"&gt;ImageNet&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://gist.github.com/dclong/8537ea044cb5b44a33582927ddfc4c73"&gt;Mapping Between Class and Ids&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="raccoon-dataset-for-training-raccoon-detection"&gt;&lt;a href="https://github.com/dclong/raccoon_dataset"&gt;Raccoon Dataset for Training Raccoon Detection&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="coco-common-object-in-context"&gt;&lt;a href="https://cocodataset.org/#home"&gt;COCO - Common Object in Context&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;VOC-2012 &lt;/p&gt;
&lt;p&gt;&lt;a href="https://image-net.org/challenges/LSVRC/"&gt;ImageNet Large Scale Visual Recognition Challenge (ILSVRC)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.cvlibs.net/datasets/kitti/index.php"&gt;The KITTI Vision Benchmark Suite&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.cis.upenn.edu/~jshi/ped_html/"&gt;Penn-Fudan Database for Pedestrian Detection and Segmentation&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.legendu.net/misc/blog/data-sources/"&gt;Data Sources&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://discuss.pytorch.org/t/imagenet-classes/4923"&gt;Imagenet classes&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="deep learning"></category><category term="Computer Vision"></category><category term="CV"></category><category term="data"></category></entry><entry><title>Learning to Rank</title><link href="https://misc.legendu.net/blog/learning-to-rank/" rel="alternate"></link><published>2019-12-10T14:27:43-08:00</published><updated>2021-10-08T12:13:57-07:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2019-12-10:/blog/learning-to-rank/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;https://www.kaggle.com/c/home-credit-default-risk/discussion/61613&lt;/p&gt;
&lt;p&gt;https://studylib.net/doc/18339870/yetirank--everybody-lies&lt;/p&gt;
&lt;p&gt;http://proceedings.mlr.press/v14/gulin11a/gulin11a.pdf&lt;/p&gt;
&lt;table style="width:100%"&gt;
  &lt;tr&gt;
    &lt;th&gt; Model &lt;/th&gt;
    &lt;th&gt; Architecture &lt;/th&gt;
    &lt;th&gt; Ranking Category &lt;/th&gt;
    &lt;th&gt; SOTA &lt;/th&gt;
    &lt;th&gt; Comments &lt;/th&gt;
    &lt;th&gt; Paper &lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt; RankNet &lt;/td&gt;
    &lt;td&gt; NN …&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;https://www.kaggle.com/c/home-credit-default-risk/discussion/61613&lt;/p&gt;
&lt;p&gt;https://studylib.net/doc/18339870/yetirank--everybody-lies&lt;/p&gt;
&lt;p&gt;http://proceedings.mlr.press/v14/gulin11a/gulin11a.pdf&lt;/p&gt;
&lt;table style="width:100%"&gt;
  &lt;tr&gt;
    &lt;th&gt; Model &lt;/th&gt;
    &lt;th&gt; Architecture &lt;/th&gt;
    &lt;th&gt; Ranking Category &lt;/th&gt;
    &lt;th&gt; SOTA &lt;/th&gt;
    &lt;th&gt; Comments &lt;/th&gt;
    &lt;th&gt; Paper &lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt; RankNet &lt;/td&gt;
    &lt;td&gt; NN &lt;/td&gt;
    &lt;th&gt; Pairwise &lt;/th&gt;
    &lt;td&gt; &lt;/td&gt;
    &lt;td&gt; ? &lt;/td&gt;
    &lt;td&gt; ? &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt; LambdaRank &lt;/td&gt;
    &lt;td&gt; NN &lt;/td&gt;
    &lt;th&gt; Pairwise &lt;/th&gt;
    &lt;td&gt; &lt;/td&gt;
    &lt;td&gt; ? &lt;/td&gt;
    &lt;td&gt; ? &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt; LambdaMART &lt;/td&gt;
    &lt;td&gt; boosted decision trees &lt;/td&gt;
    &lt;th&gt; Listwise &lt;/th&gt;
    &lt;td&gt; 2010 &lt;/td&gt;
    &lt;td&gt; ? &lt;/td&gt;
    &lt;td&gt; ? &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;a href="https://practicaldatascience.co.uk/machine-learning/a-quick-guide-to-learning-to-rank-models"&gt;A quick guide to Learning to Rank models&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="tutorials"&gt;Tutorials&lt;/h2&gt;
&lt;p&gt;https://github.com/sophwats/learning-to-rank&lt;/p&gt;
&lt;h2 id="neural-network-based-approaches"&gt;Neural Network Based Approaches&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2008.13368.pdf"&gt;PT-RANKING: A BENCHMARKING PLATFORM FOR NEURAL LEARNING-TO-RANK&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://dial.uclouvain.be/memoire/ucl/fr/object/thesis:4596/datastream/PDF_01/view"&gt;Learning to Rank with Deep Neural Networks&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://openreview.net/pdf?id=BJgxzlSFvr"&gt;AN ATTENTION-BASED DEEP NET FOR LEARNING TO RANK&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.researchgate.net/publication/322489214_Ranking_with_Deep_Neural_Networks"&gt;Ranking with Deep Neural Networks&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://icml.cc/2015/wp-content/uploads/2015/06/icml_ranking.pdf"&gt;Learning to Rank using Gradient Descent&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/1e34e05e5e4bf2d12f41eb9ff29ac3da9fdb4de3.pdf"&gt;The LambdaLoss Framework for Ranking Metric Optimization&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://ecmlpkdd2019.org/downloads/paper/400.pdf"&gt;Pairwise Learning to Rank by Neural Networks Revisited: Reconstruction, Theoretical Analysis and Practical Performance&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://ris.utwente.nl/ws/portalfiles/portal/6420086/ipm2015-preprint.pdf"&gt;A cross-benchmark comparison of 87 learning to rank methods&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://dl.acm.org/doi/abs/10.1145/3404835.3462904"&gt;Fast Attention-based Learning-To-Rank Model for Structured Map Search&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://aclanthology.org/C16-1163.pdf"&gt;Neural Attention for Learning to Rank Questions in Community Question Answering&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/1702.06106.pdf#:~:text=For%20learning%20to%20rank%2C%20neural,search%20results%20as%20the%20input.&lt;/p&gt;
&lt;h2 id="learning-to-rank-in-lightgbm"&gt;Learning to Rank in LightGBM&lt;/h2&gt;
&lt;p&gt;https://mlexplained.com/2019/05/27/learning-to-rank-explained-with-code/&lt;/p&gt;
&lt;p&gt;https://github.com/microsoft/LightGBM/tree/master/examples/xendcg&lt;/p&gt;
&lt;p&gt;https://github.com/microsoft/LightGBM/tree/master/examples/lambdarank&lt;/p&gt;
&lt;h2 id="learning-to-rank-in-xgboost"&gt;Learning to Rank in XGBoost&lt;/h2&gt;
&lt;p&gt;Use the objective &lt;code&gt;rank:pairwise&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;https://www.jianshu.com/p/9caef967ec0a&lt;/p&gt;
&lt;p&gt;https://tech.olx.com/ranking-ads-with-machine-learning-ee03d7734bf4&lt;/p&gt;
&lt;h2 id="benchmark"&gt;Benchmark&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://www.bigdatalab.ac.cn/benchmark/bm/Domain?domain=Learning%20to%20Rank"&gt;Domain: Learning to Rank&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.legendu.net/misc/blog/data-for-learning-to-rank"&gt;Data for Learning to Rank&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf"&gt;From RankNet to LambdaRank to LambdaMART: An Overview&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.quora.com/What-is-the-intuitive-explanation-of-Learning-to-Rank-and-algorithms-like-RankNet-LambdaRank-and-LambdaMART-In-what-types-of-data-variables-can-these-techniques-be-used-What-are-their-strengths-and-limitations"&gt;What is the intuitive explanation of Learning to Rank and algorithms like RankNet, LambdaRank and LambdaMART? In what types of data/variables can these techniques be used? What are their strengths and limitations?
&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;https://mlexplained.com/2019/05/27/learning-to-rank-explained-with-code/&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;https://medium.com/@nikhilbd/intuitive-explanation-of-learning-to-rank-and-ranknet-lambdarank-and-lambdamart-fe1e17fac418&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/Learning_to_rank&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;http://times.cs.uiuc.edu/course/598f14/l2r.pdf&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="learning to rank"></category><category term="machine learned ranking"></category><category term="MLR"></category></entry><entry><title>Effect of Duplicating Observations in Linear Models</title><link href="https://misc.legendu.net/blog/effect-of-duplicating-observations-in-linear-models/" rel="alternate"></link><published>2013-08-15T21:45:26-07:00</published><updated>2021-09-25T13:21:30-07:00</updated><author><name>Ben Chuanlong Du</name></author><id>tag:misc.legendu.net,2013-08-15:/blog/effect-of-duplicating-observations-in-linear-models/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;coefficients don't change but variance become smaller.
use formula to show it ...&lt;/p&gt;
&lt;h2 id="complete-duplication-of-all-data-points"&gt;Complete Duplication of All Data Points&lt;/h2&gt;
&lt;h2 id="complete-duplication-of-some-data-points"&gt;Complete Duplication of Some Data Points&lt;/h2&gt;
&lt;h2 id="duplication-with-noise"&gt;Duplication with Noise&lt;/h2&gt;
&lt;p&gt;common in computer vision …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;coefficients don't change but variance become smaller.
use formula to show it ...&lt;/p&gt;
&lt;h2 id="complete-duplication-of-all-data-points"&gt;Complete Duplication of All Data Points&lt;/h2&gt;
&lt;h2 id="complete-duplication-of-some-data-points"&gt;Complete Duplication of Some Data Points&lt;/h2&gt;
&lt;h2 id="duplication-with-noise"&gt;Duplication with Noise&lt;/h2&gt;
&lt;p&gt;common in computer vision &lt;/p&gt;</content><category term="AI"></category><category term="duplicate"></category><category term="statistics"></category><category term="modeling"></category><category term="linear model"></category><category term="observation"></category></entry><entry><title>Tips on Scikit-Learn</title><link href="https://misc.legendu.net/blog/python-sklearn-tips/" rel="alternate"></link><published>2019-12-01T10:06:27-08:00</published><updated>2021-09-16T21:12:30-07:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2019-12-01:/blog/python-sklearn-tips/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Cross validation in scikit-learn supports pipeline in addition to vanilla models.
    Please refer to 
    &lt;a href="https://chrisalbon.com/machine_learning/model_evaluation/cross_validation_pipeline/"&gt;Cross Validation Pipeline&lt;/a&gt;
    for more details.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://scikit-learn.org/stable/modules/preprocessing_targets.html#label-encoding"&gt;Label encoding&lt;/a&gt;
    is an easy way to convert a categorical response …&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Cross validation in scikit-learn supports pipeline in addition to vanilla models.
    Please refer to 
    &lt;a href="https://chrisalbon.com/machine_learning/model_evaluation/cross_validation_pipeline/"&gt;Cross Validation Pipeline&lt;/a&gt;
    for more details.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://scikit-learn.org/stable/modules/preprocessing_targets.html#label-encoding"&gt;Label encoding&lt;/a&gt;
    is an easy way to convert a categorical response/target variable to a numeric one and back to the raw value space.
    For transform of response/target varible in regression,
    please refer to &lt;a href="https://scikit-learn.org/stable/modules/compose.html#transforming-target-in-regression"&gt;Transforming Target in Tegression&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/skorch-dev/skorch"&gt;Skorch&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;https://scikit-learn.org/stable/modules/compose.html#transforming-target-in-regression&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.legendu.net/misc/blog/python-ai-split-dataset"&gt;Split a Dataset into Train and Test Datasets in Python&lt;/a&gt; &lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="Scikit-learn"></category><category term="sklearn"></category><category term="pipeline"></category></entry><entry><title>Common Issues in PyTorch</title><link href="https://misc.legendu.net/blog/common-issues-in-pytorch/" rel="alternate"></link><published>2020-03-03T11:41:04-08:00</published><updated>2021-09-16T20:58:50-07:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-03-03:/blog/common-issues-in-pytorch/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="gpu-related-issues-and-solutions"&gt;&lt;a href="http://www.legendu.net/misc/blog/GPU-related-issues-and-solutions"&gt;GPU Related Issues and Solutions&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="input-type-torchfloattensor-and-weight-type-torchcudafloattensor-should-be-the-same"&gt;Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same&lt;/h2&gt;
&lt;p&gt;This means that the input data and the model are on different …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="gpu-related-issues-and-solutions"&gt;&lt;a href="http://www.legendu.net/misc/blog/GPU-related-issues-and-solutions"&gt;GPU Related Issues and Solutions&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="input-type-torchfloattensor-and-weight-type-torchcudafloattensor-should-be-the-same"&gt;Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same&lt;/h2&gt;
&lt;p&gt;This means that the input data and the model are on different devices (CPU and CUDA). 
Moving them to the same device resolves the issue.
Please refer to 
&lt;a href="http://www.legendu.net/misc/blog/common-issues-in-pytorch/"&gt;Move a Tensor to a Specific Device in PyTorch&lt;/a&gt;
on how to move a Tensor to a specific device.&lt;/p&gt;
&lt;p&gt;https://discuss.pytorch.org/t/input-type-torch-floattensor-and-weight-type-torch-cuda-floattensor-should-be-the-same/48633&lt;/p&gt;
&lt;h2 id="error-expected-more-than-1-value-per-channel-when-training"&gt;&lt;a href="https://discuss.pytorch.org/t/error-expected-more-than-1-value-per-channel-when-training/26274"&gt;Error: Expected more than 1 value per channel when training&lt;/a&gt;&lt;/h2&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="deep learning"></category><category term="PyTorch"></category><category term="issue"></category><category term="device"></category></entry><entry><title>LightGBM on GPU</title><link href="https://misc.legendu.net/blog/lightgbm-on-gpu/" rel="alternate"></link><published>2020-02-04T20:49:35-08:00</published><updated>2021-09-16T09:54:29-07:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-02-04:/blog/lightgbm-on-gpu/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;https://pypi.org/project/lightgbm/#build-gpu-version&lt;/p&gt;
&lt;p&gt;https://github.com/microsoft/LightGBM/blob/master/docs/Installation-Guide.rst#build-gpu-version&lt;/p&gt;
&lt;p&gt;https://www.kaggle.com/vinhnguyen/gpu-acceleration-for-lightgbm&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/microsoft/LightGBM/blob/master/docker/gpu/dockerfile.gpu"&gt;Microsoft's Example Dockerfile for GPU version of LightGBM …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;https://pypi.org/project/lightgbm/#build-gpu-version&lt;/p&gt;
&lt;p&gt;https://github.com/microsoft/LightGBM/blob/master/docs/Installation-Guide.rst#build-gpu-version&lt;/p&gt;
&lt;p&gt;https://www.kaggle.com/vinhnguyen/gpu-acceleration-for-lightgbm&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/microsoft/LightGBM/blob/master/docker/gpu/dockerfile.gpu"&gt;Microsoft's Example Dockerfile for GPU version of LightGBM&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://lightgbm.readthedocs.io/en/latest/GPU-Tutorial.html"&gt;LightGBM GPU Tutorial&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://harangdev.github.io/tips/1/"&gt;How I set Windows GPU Environment for tensorflow, lightgbm, xgboost, catboost, etc…&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;install the below libraries (non GPU) before you install/compile LightGBM&lt;/p&gt;
&lt;p&gt;:::bash
apt-get install cmake libboost-dev
pip3 install joblib numpy scipy scikit-learn&lt;/p&gt;
&lt;h2 id="gpu-vs-cpu-performance"&gt;GPU vs CPU Performance&lt;/h2&gt;
&lt;p&gt;It seems to that GPU have 3 ~ 10 times speed up generally speaking.&lt;/p&gt;
&lt;p&gt;https://github.com/Microsoft/LightGBM/blob/master/docs/GPU-Performance.rst&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="GPU"></category><category term="LightGBM"></category></entry><entry><title>Tips on TensorFlow</title><link href="https://misc.legendu.net/blog/tips-on-tensorflow/" rel="alternate"></link><published>2019-12-27T10:57:44-08:00</published><updated>2021-09-16T09:54:29-07:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2019-12-27:/blog/tips-on-tensorflow/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="installation"&gt;Installation&lt;/h2&gt;
&lt;p&gt;The article
&lt;a href="https://towardsdatascience.com/stop-installing-tensorflow-using-pip-for-performance-sake-5854f9d9eb0c"&gt;Stop Installing Tensorflow using pip for performance sake!&lt;/a&gt;
suggest installing TensorFlow using conda instead of pip
as the version installed by conda leverages Intel Math Kernel Library 
and …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="installation"&gt;Installation&lt;/h2&gt;
&lt;p&gt;The article
&lt;a href="https://towardsdatascience.com/stop-installing-tensorflow-using-pip-for-performance-sake-5854f9d9eb0c"&gt;Stop Installing Tensorflow using pip for performance sake!&lt;/a&gt;
suggest installing TensorFlow using conda instead of pip
as the version installed by conda leverages Intel Math Kernel Library 
and is about 8 times faster &lt;strong&gt;on CPU&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;https://www.anaconda.com/tensorflow-in-anaconda/&lt;/p&gt;
&lt;p&gt;cuDNN is required by TensorFlow&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;pip&lt;span class="w"&gt; &lt;/span&gt;install&lt;span class="w"&gt; &lt;/span&gt;tensorflow
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;https://www.tensorflow.org/install/gpu&lt;/p&gt;
&lt;p&gt;https://www.tensorflow.org/install/docker&lt;/p&gt;
&lt;p&gt;https://www.tensorflow.org/install/pip&lt;/p&gt;
&lt;h2 id="docker-images"&gt;Docker Images&lt;/h2&gt;
&lt;p&gt;https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/dockerfiles/dockerfiles&lt;/p&gt;
&lt;p&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dockerfiles/dockerfiles/gpu.Dockerfile
uses pip (instead of conda) to install Python packages.&lt;/p&gt;
&lt;h2 id="general-tips"&gt;General Tips&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;With TensorFlow 2.0, 
    you should use &lt;code&gt;tf.keras&lt;/code&gt; instead of the separate Keras package.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;https://www.pyimagesearch.com/2019/10/21/keras-vs-tf-keras-whats-the-difference-in-tensorflow-2-0/&lt;/p&gt;
&lt;h2 id="tutorials"&gt;Tutorials&lt;/h2&gt;
&lt;p&gt;https://www.tensorflow.org/guide/keras/&lt;/p&gt;
&lt;p&gt;https://www.tensorflow.org/tutorials/keras/classification
https://www.tensorflow.org/tutorials/quickstart/beginner
https://www.tensorflow.org/guide/keras/functional
https://www.tensorflow.org/guide/keras/train_and_evaluate
https://www.tensorflow.org/guide/keras/custom_layers_and_models
https://www.tensorflow.org/guide/keras/masking_and_padding&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=UYRBHFAvLSs&amp;amp;feature=youtu.be"&gt;Inside TensorFlow: tf.Keras (part 1)&lt;/a&gt;
&lt;a href="https://www.youtube.com/watch?v=uhzGTijaw8A&amp;amp;feature=youtu.be"&gt;Inside TensorFlow: tf.Keras (part 2)&lt;/a&gt;
&lt;a href="https://www.youtube.com/watch?v=6g4O5UOH304"&gt;TensorFlow 2.0 Full Tutorial - Python Neural Networks for Beginners&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="gpu-runs-out-of-memory"&gt;GPU Runs Out of Memory&lt;/h2&gt;
&lt;p&gt;https://stackoverflow.com/questions/36927607/how-can-i-solve-ran-out-of-gpu-memory-in-tensorflow&lt;/p&gt;
&lt;p&gt;https://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory&lt;/p&gt;
&lt;p&gt;https://superuser.com/questions/980216/what-happens-when-the-gpu-memory-is-not-enough&lt;/p&gt;
&lt;h2 id="deep-learning-libraries-based-on-tensorflow"&gt;Deep Learning Libraries Based on TensorFlow&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/google/trax"&gt;trax&lt;/a&gt;&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="TensorFlow"></category><category term="GPU"></category></entry><entry><title>Use PyTorch on GPU</title><link href="https://misc.legendu.net/blog/use-pytorch-on-gpu/" rel="alternate"></link><published>2020-01-21T11:51:30-08:00</published><updated>2021-09-16T09:54:29-07:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-01-21:/blog/use-pytorch-on-gpu/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="pytorch-on-gpu"&gt;PyTorch on GPU&lt;/h2&gt;
&lt;p&gt;https://pytorch.org/docs/master/notes/cuda.html&lt;/p&gt;
&lt;p&gt;You can use the command &lt;code&gt;torch.cuda.is_available()&lt;/code&gt;
to check whether GPU is available for PyTorch.
Details of GPUs can …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="pytorch-on-gpu"&gt;PyTorch on GPU&lt;/h2&gt;
&lt;p&gt;https://pytorch.org/docs/master/notes/cuda.html&lt;/p&gt;
&lt;p&gt;You can use the command &lt;code&gt;torch.cuda.is_available()&lt;/code&gt;
to check whether GPU is available for PyTorch.
Details of GPUs can be obtained using the following code.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;In&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;:&lt;span class="w"&gt; &lt;/span&gt;import&lt;span class="w"&gt; &lt;/span&gt;torch

In&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;:&lt;span class="w"&gt; &lt;/span&gt;torch.cuda.current_device&lt;span class="o"&gt;()&lt;/span&gt;
Out&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;:&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;

In&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;:&lt;span class="w"&gt; &lt;/span&gt;torch.cuda.device&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
Out&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;:&lt;span class="w"&gt; &lt;/span&gt;&amp;lt;torch.cuda.device&lt;span class="w"&gt; &lt;/span&gt;at&lt;span class="w"&gt; &lt;/span&gt;0x7efce0b03be0&amp;gt;

In&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;:&lt;span class="w"&gt; &lt;/span&gt;torch.cuda.device_count&lt;span class="o"&gt;()&lt;/span&gt;
Out&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;:&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;

In&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;5&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;:&lt;span class="w"&gt; &lt;/span&gt;torch.cuda.get_device_name&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
Out&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;5&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;:&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;GeForce GTX 950M&amp;#39;&lt;/span&gt;

In&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;6&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;:&lt;span class="w"&gt; &lt;/span&gt;torch.cuda.is_available&lt;span class="o"&gt;()&lt;/span&gt;
Out&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;6&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;:&lt;span class="w"&gt; &lt;/span&gt;True
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You can force synchronous computation by setting environment variable CUDA_LAUNCH_BLOCKING=1. 
This can be handy when an error occurs on the GPU. 
(With asynchronous execution, such an error isn’t reported until after the operation is actually executed, 
so the stack trace does not show where it was requested.)&lt;/p&gt;
&lt;h2 id="make-sure-that-pytorch-is-using-gpu"&gt;Make Sure that PyTorch is Using GPU&lt;/h2&gt;
&lt;p&gt;https://discuss.pytorch.org/t/solved-make-sure-that-pytorch-using-gpu-to-compute/4870&lt;/p&gt;
&lt;h2 id="gpu-vs-cpu-performance"&gt;GPU vs CPU Performance&lt;/h2&gt;
&lt;p&gt;My personal experience sees a speed up of 3 - 30 using a single GeForce GTX 1080 GPU vs CPU. 
Generally speaking,
the more complicated your neural network is, 
the more speed up you get. 
Also be careful about IO bound when you train simple neural networks in Docker (which is what most people do)
as Docker seems to have issues with a multiprocessing (&lt;code&gt;num_workers &amp;gt; 0&lt;/code&gt;) DataLoader. &lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="deep learning"></category><category term="PyTorch"></category><category term="GPU"></category></entry><entry><title>Tips on Torchvision</title><link href="https://misc.legendu.net/blog/tips-on-torchvision/" rel="alternate"></link><published>2020-01-07T09:50:47-08:00</published><updated>2021-09-02T12:29:21-07:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-01-07:/blog/tips-on-torchvision/</id><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;https://pytorch.org/docs/stable/torchvision/index.html&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="torchvision"></category><category term="pytorch"></category></entry><entry><title>Machine Learning Libraries, Computing Frames and Programming Languages</title><link href="https://misc.legendu.net/blog/machine-learning-libraries-computing-frames-programming-languages/" rel="alternate"></link><published>2019-12-10T09:32:40-08:00</published><updated>2021-06-10T09:32:40-07:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2019-12-10:/blog/machine-learning-libraries-computing-frames-programming-languages/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;GPU is more accisible for average individual people.
    GPU is still the main tool for deep learning right now.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Python Distributed Computing Frameworks (Ray, Modin, etc.)
    servers as a mid solution …&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;GPU is more accisible for average individual people.
    GPU is still the main tool for deep learning right now.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Python Distributed Computing Frameworks (Ray, Modin, etc.)
    servers as a mid solution between GPU and Spark. 
    It can handle more data than GPU but less then Spark.
    Ray, Modin, etc is easier to use and maintain than Spark.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Even though there are many libraries making it possible to run deep learning on Spark,
    I still don't it is the right choice unless you have really large data 
    that cannot be handled by other frameworks.
    There are rarely such situations.
    Real big data mostly occur in the ETL and preprocessing stage 
    rather than in the model training stage.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Python and Rust are good choices. 
    C is not productive. 
    C++ is too complicated.
    JVM-based languages are first-class citizens for production.
    Rust seems to have a bright future. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;As the development of Kubernetes, 
    there will be distributed computing frameworks that does not limit you into a specific languages. 
    Once that is a common situation,
    people will start shifting away from JVM languages and seek for better performance and easier to use solutions.
    Rust is a good language choice for performance 
    while Python is a good choice for glue-language that is easy to use.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="machine-learning-frameworks"&gt;Machine Learning Frameworks&lt;/h2&gt;
&lt;p&gt;scikit-learn&lt;/p&gt;
&lt;p&gt;LightGBM / XGBoost&lt;/p&gt;
&lt;p&gt;PyTorch&lt;/p&gt;
&lt;p&gt;TensorFlow 2&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/apache/incubator-mxnet"&gt;Apache MXNet&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Caffe2 is often used for productionizing models trained in PyTorch
and it is part of the PyTorch project now.&lt;/p&gt;
&lt;p&gt;Notice that H2O-3 (less popularity and lower quality compared to the above libraries),
&lt;a href="https://mrnothing.github.io/AI-Blocks/index.html"&gt;AI-Blocks&lt;/a&gt;,
and &lt;a href="https://developer.nvidia.com/digits"&gt;Nvidia DIGIGS&lt;/a&gt;
provides user-friendly UI for training models.&lt;/p&gt;
&lt;h2 id="computing-frameworks"&gt;Computing Frameworks&lt;/h2&gt;
&lt;p&gt;Multi-threading &amp;amp; Multi-Processing are not discussed here 
since they are relatively simple for scientific computing.&lt;/p&gt;
&lt;h3 id="gpu"&gt;GPU&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/microsoft/DeepSpeed"&gt;ZeRO + DeepSpeed&lt;/a&gt;
is a deep learning optimization library 
that makes distributed training on GPU clusters easy, efficient, and effective.&lt;/p&gt;
&lt;p&gt;Apache Ray&lt;/p&gt;
&lt;h3 id="python-distributed-computing-frameworks-ray-celery-dask-modin-etc"&gt;Python Distributed Computing Frameworks (Ray, Celery, Dask, Modin, etc.)&lt;/h3&gt;
&lt;h3 id="spark"&gt;Spark&lt;/h3&gt;
&lt;h3 id="tpu"&gt;TPU&lt;/h3&gt;
&lt;h2 id="model-serving"&gt;Model Serving&lt;/h2&gt;
&lt;h3 id="httpsgithubcomcortexlabscortex"&gt;https://github.com/cortexlabs/cortex&lt;/h3&gt;
&lt;p&gt;Multi-framework machine learning model serving infrastructure.&lt;/p&gt;
&lt;h3 id="ray-serve"&gt;&lt;a href="https://github.com/ray-project/ray"&gt;Ray Serve&lt;/a&gt;&lt;/h3&gt;
&lt;h3 id="tfx"&gt;&lt;a href="https://www.tensorflow.org/tfx"&gt;TFX&lt;/a&gt;&lt;/h3&gt;
&lt;h3 id="torch-serve"&gt;&lt;a href="https://github.com/pytorch/pytorch"&gt;Torch Serve&lt;/a&gt;&lt;/h3&gt;
&lt;h2 id="programming-languages"&gt;Programming Languages&lt;/h2&gt;
&lt;p&gt;Python&lt;/p&gt;
&lt;p&gt;Rust&lt;/p&gt;
&lt;p&gt;JVM (Java, Scala, Kotlin)&lt;/p&gt;
&lt;p&gt;C/C++&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;(Machine Learning and Deep Learning frameworks and libraries for large-scale data mining: a survey)[https://link.springer.com/article/10.1007/s10462-018-09679-z]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/caffe2/AICamera"&gt;caffe2/AICamera&lt;/a&gt; 
    is a demonstration of using Caffe2 inside an Android application.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://pathmind.com/wiki/comparison-frameworks-dl4j-tensorflow-pytorch"&gt;Comparison of AI Frameworks&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://towardsdatascience.com/accelerating-deep-learning-using-distributed-sgd-an-overview-e66c4aee1a0c"&gt;Accelerating Deep Learning Using Distributed SGD — An Overview&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://spcl.inf.ethz.ch/Publications/.pdf/distdl-preprint.pdf"&gt;Demystifying Parallel and Distributed Deep Learning: An In-Depth Concurrency Analysis&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4465"&gt;Scalable Distributed DL Training: Batching Communication and Computation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/pdf/1903.11314.pdf"&gt;Scalable Deep Learning on Distributed Infrastructures: Challenges, Techniques and Tools&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/pdf/1810.11787.pdf"&gt;A Hitchhiker’s Guide On Distributed Training of Deep Neural Networks&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://medium.com/intel-student-ambassadors/distributed-training-of-deep-learning-models-with-pytorch-1123fa538848"&gt;Distributed training of Deep Learning models with PyTorch&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/bharathgs/Awesome-Distributed-Deep-Learning"&gt;Awesome Distributed Deep Learning&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://medium.com/@Petuum/intro-to-distributed-deep-learning-systems-a2e45c6b8e7"&gt;Intro to Distributed Deep Learning Systems&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/pdf/1706.02677.pdf"&gt;Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour&lt;/a&gt;&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="machine learning"></category><category term="data science"></category><category term="computing frameworks"></category><category term="programming languages"></category><category term="GPU"></category><category term="Python"></category><category term="Rust"></category></entry><entry><title>Data for NLP Research</title><link href="https://misc.legendu.net/blog/data-for-nlp-research/" rel="alternate"></link><published>2020-03-06T12:46:35-08:00</published><updated>2021-05-06T12:46:35-07:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-03-06:/blog/data-for-nlp-research/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="the-multi-genre-nli-corpus-multinli"&gt;&lt;a href="https://www.nyu.edu/projects/bowman/multinli/"&gt;The Multi-Genre NLI Corpus (MultiNLI)&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="general-language-understanding-evaluation-glue"&gt;&lt;a href="https://gluebenchmark.com/"&gt;General Language Understanding Evaluation (GLUE)&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="the-stanford-question-answering-dataset"&gt;&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/"&gt;The Stanford Question Answering Dataset&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="swag-situations-with-adversarial-generations"&gt;&lt;a href="https://rowanzellers.com/swag/"&gt;SWAG (Situations With Adversarial Generations)&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="reading-comprehension-dataset-race"&gt;&lt;a href="http://www.qizhexie.com/data/RACE_leaderboard.html"&gt;Reading Comprehension Dataset (RACE)&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="heuristic-analysis-for-nli-systems-data-set-hans"&gt;&lt;a href="https://github.com/tommccoy1/hans"&gt;Heuristic Analysis for NLI Systems Data set …&lt;/a&gt;&lt;/h2&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="the-multi-genre-nli-corpus-multinli"&gt;&lt;a href="https://www.nyu.edu/projects/bowman/multinli/"&gt;The Multi-Genre NLI Corpus (MultiNLI)&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="general-language-understanding-evaluation-glue"&gt;&lt;a href="https://gluebenchmark.com/"&gt;General Language Understanding Evaluation (GLUE)&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="the-stanford-question-answering-dataset"&gt;&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/"&gt;The Stanford Question Answering Dataset&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="swag-situations-with-adversarial-generations"&gt;&lt;a href="https://rowanzellers.com/swag/"&gt;SWAG (Situations With Adversarial Generations)&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="reading-comprehension-dataset-race"&gt;&lt;a href="http://www.qizhexie.com/data/RACE_leaderboard.html"&gt;Reading Comprehension Dataset (RACE)&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="heuristic-analysis-for-nli-systems-data-set-hans"&gt;&lt;a href="https://github.com/tommccoy1/hans"&gt;Heuristic Analysis for NLI Systems Data set (HANS)&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="the-cross-lingual-nli-corpus-xnli"&gt;&lt;a href="https://www.nyu.edu/projects/bowman/xnli/"&gt;The Cross-Lingual NLI Corpus (XNLI)&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="standford-nlp-sentiment-treebank"&gt;&lt;a href="https://nlp.stanford.edu/sentiment/treebank.html"&gt;Standford NLP - Sentiment Treebank&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="wordnet"&gt;&lt;a href="https://wordnet.princeton.edu/"&gt;WordNet&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="cola-the-corpus-of-linguistic-acceptability"&gt;&lt;a href="https://nyu-mll.github.io/CoLA/"&gt;CoLA: The Corpus of Linguistic Acceptability&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://www.legendu.net/misc/blog/data-sources/"&gt;Data Sources&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/pdf/1606.05250.pdf"&gt;SQuAD: 100,000+ Questions for Machine Comprehension of Text&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.groundai.com/project/swag-a-large-scale-adversarial-dataset-for-grounded-commonsense-inference/1"&gt;Swag: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference&lt;/a&gt;&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="deep learning"></category><category term="machine learning"></category><category term="NLP"></category><category term="data"></category></entry><entry><title>SLIDE: Sub-Linear Deep Learning Engine</title><link href="https://misc.legendu.net/blog/slide-Sub-LInear-Deep-learning-engine/" rel="alternate"></link><published>2020-03-11T12:01:22-07:00</published><updated>2021-04-11T12:01:22-07:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-03-11:/blog/slide-Sub-LInear-Deep-learning-engine/</id><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.cs.rice.edu/~as143/Papers/SLIDE_MLSys.pdf"&gt;Slide : In Defense of Smart Algorithms over Hardware Acceleration for Large-scale Deep Learning Systems&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://spectrum.ieee.org/tech-talk/computing/hardware/algorithms-and-hardware-for-deep-learning"&gt;Hash Your Way To a Better Neural Network&lt;/a&gt;&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="deep learning"></category><category term="machine learning"></category><category term="data science"></category><category term="CPU"></category><category term="GPU"></category></entry><entry><title>AI Learning</title><link href="https://misc.legendu.net/blog/ai-learning/" rel="alternate"></link><published>2013-12-08T23:07:57-08:00</published><updated>2021-01-08T23:07:57-08:00</updated><author><name>Ben Du</name></author><id>tag:misc.legendu.net,2013-12-08:/blog/ai-learning/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Machine Learning Algorithms" src="https://jixta.files.wordpress.com/2015/11/machinelearningalgorithms.png"&gt;
The picture comes from &lt;a href="https://jixta.wordpress.com/2015/07/17/machine-learning-algorithms-mindmap/"&gt;Machine Learning Algorithms Mindmap&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="feature-engineering"&gt;Feature Engineering&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://www.legendu.net/misc/blog/handling-categorical-variables-in-machine-learning/"&gt;Handling Categorical Variables in Machine Learning&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="regularization-in-machine-learning-models"&gt;&lt;a href="http://www.legendu.net/misc/blog/regularization-in-machine-learning-models/"&gt;Regularization in Machine Learning Models&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="ensemble"&gt;&lt;a href="http://www.legendu.net/misc/blog/ai-ensemble/"&gt;Ensemble&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="frameworks"&gt;Frameworks&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://www.legendu.net/misc/blog/libraries-for-gradient-boosting/"&gt;Libraries for Gradient Boosting&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="big-data-spark-friendly-frameworks"&gt;Big-data (Spark) Friendly Frameworks …&lt;/h3&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Machine Learning Algorithms" src="https://jixta.files.wordpress.com/2015/11/machinelearningalgorithms.png"&gt;
The picture comes from &lt;a href="https://jixta.wordpress.com/2015/07/17/machine-learning-algorithms-mindmap/"&gt;Machine Learning Algorithms Mindmap&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="feature-engineering"&gt;Feature Engineering&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://www.legendu.net/misc/blog/handling-categorical-variables-in-machine-learning/"&gt;Handling Categorical Variables in Machine Learning&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="regularization-in-machine-learning-models"&gt;&lt;a href="http://www.legendu.net/misc/blog/regularization-in-machine-learning-models/"&gt;Regularization in Machine Learning Models&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="ensemble"&gt;&lt;a href="http://www.legendu.net/misc/blog/ai-ensemble/"&gt;Ensemble&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="frameworks"&gt;Frameworks&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://www.legendu.net/misc/blog/libraries-for-gradient-boosting/"&gt;Libraries for Gradient Boosting&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="big-data-spark-friendly-frameworks"&gt;Big-data (Spark) Friendly Frameworks&lt;/h3&gt;
&lt;p&gt;https://mmlspark.blob.core.windows.net/website/index.html&lt;/p&gt;
&lt;h2 id="automl"&gt;&lt;a href="http://www.legendu.net/misc/blog/automl-tips/"&gt;AutoML&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="questions"&gt;Questions&lt;/h2&gt;
&lt;h3 id="random-forest"&gt;Random Forest&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Is discrete variables easier to handle than continous variables (in random forest)?
    Is there any advantage of discretize variables?
    The eseential question is how is categorical varialbes handled in RF?
    Does RF use category variables directly or does it have to convert it to numerical somehow?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Random forest has a way to impute missing values.
    What if I treat missing values in categorical predictors and a new class?
    It sounds like a good ...&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="imputation"&gt;Imputation&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;mean, median, etc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;SVD imputation using low dimension to approximate high dimension data&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="tips-on-kaggle"&gt;&lt;a href="http://www.legendu.net/misc/blog/tips-on-kaggle/"&gt;Tips on Kaggle&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="machine-learning-resources"&gt;&lt;a href="http://www.legendu.net/misc/blog/machine-learning-resources/"&gt;Machine Learning Resources&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="ai-tools"&gt;AI Tools&lt;/h2&gt;
&lt;p&gt;https://openai.com/blog/dall-e/&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;https://github.com/academic/awesome-datascience&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://startupsventurecapital.com/essential-cheat-sheets-for-machine-learning-and-deep-learning-researchers-efb6a8ebd2e5"&gt;Essential Cheat Sheets for Machine Learning and Deep Learning Engineers&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;https://rushter.com/dsreader/&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</content><category term="AI"></category><category term="AI"></category><category term="machine learning"></category><category term="statistics"></category><category term="data science"></category></entry><entry><title>Tips on Feature Engineering for Machine Learning</title><link href="https://misc.legendu.net/blog/tips-on-feature-engineering-for-machine-learning/" rel="alternate"></link><published>2020-01-30T12:43:37-08:00</published><updated>2020-12-30T12:43:37-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-01-30:/blog/tips-on-feature-engineering-for-machine-learning/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;https://towardsdatascience.com/automatic-feature-engineering-using-deep-learning-and-bayesian-inference-application-to-computer-7b2bb8dc7351&lt;/p&gt;
&lt;p&gt;Feature selection
Feature extraction
Adding features through domain expertise&lt;/p&gt;
&lt;p&gt;FeatureTools a Python library for feature engineering
Deep neural network can extract features too&lt;/p&gt;
&lt;p&gt;whether feature engineering is …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;https://towardsdatascience.com/automatic-feature-engineering-using-deep-learning-and-bayesian-inference-application-to-computer-7b2bb8dc7351&lt;/p&gt;
&lt;p&gt;Feature selection
Feature extraction
Adding features through domain expertise&lt;/p&gt;
&lt;p&gt;FeatureTools a Python library for feature engineering
Deep neural network can extract features too&lt;/p&gt;
&lt;p&gt;whether feature engineering is still need ...&lt;/p&gt;
&lt;p&gt;Mostly yes, but there maybe some exceptions to this.&lt;/p&gt;
&lt;p&gt;One exception I can think of is a scenario when your training dataset is not sufficient to cover all variety that will be involved during run time. In this case, if you are able to design features that are able to model the problem well, then these features might work better than the features provided by deep learning. Because for deep learning data is everything; features are learnt only based on the available data. In contrast, in feature engineering, you can transfer your own understanding of the problem to the model through feature engineering. Assuming that you have a good understanding of the problem and you can model this well enough in the features that you design, then you can reach more generalizable models in the end.&lt;/p&gt;
&lt;p&gt;Other than such exceptional scenarios, we can expect deep learning to work better than and to replace feature engineering.&lt;/p&gt;
&lt;h2 id="categorical-variables"&gt;Categorical Variables&lt;/h2&gt;
&lt;p&gt;http://www.legendu.net/misc/blog/handling-categorical-variables-in-machine-learning/&lt;/p&gt;
&lt;h2 id="feature-hashing"&gt;Feature Hashing&lt;/h2&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/Feature_hashing&lt;/p&gt;
&lt;p&gt;https://medium.com/value-stream-design/introducing-one-of-the-best-hacks-in-machine-learning-the-hashing-trick-bf6a9c8af18f&lt;/p&gt;
&lt;p&gt;https://booking.ai/dont-be-tricked-by-the-hashing-trick-192a6aae3087&lt;/p&gt;
&lt;h2 id="useful-libraries"&gt;Useful Libraries&lt;/h2&gt;
&lt;h3 id="featuretools"&gt;&lt;a href="https://github.com/alteryx/featuretools"&gt;FeatureTools&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/alteryx/featuretools"&gt;FeatureTools&lt;/a&gt;
is an open source Python library for automated feature engineering.&lt;/p&gt;
&lt;h3 id="compose"&gt;&lt;a href="https://github.com/alteryx/compose"&gt;compose&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/alteryx/compose"&gt;compose&lt;/a&gt;
is a machine learning tool for automated prediction engineering. 
It allows you to easily structure prediction problems and generate labels for supervised learning.&lt;/p&gt;
&lt;h3 id="tsfresh"&gt;&lt;a href="https://github.com/blue-yonder/tsfresh"&gt;tsfresh&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/blue-yonder/tsfresh"&gt;tsfresh&lt;/a&gt;
is a tool for automatic extraction of relevant features from time series.&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;https://www.kdnuggets.com/2018/08/automated-feature-engineering-will-change-machine-learning.html&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="machine learning"></category><category term="data science"></category><category term="feature engineering"></category><category term="feature hashing"></category></entry><entry><title>Hyper Parameter Tuning and Automatical Machine Learning</title><link href="https://misc.legendu.net/blog/ai-hyper-parameter-auto-ml/" rel="alternate"></link><published>2018-09-04T12:29:38-07:00</published><updated>2020-12-04T12:29:38-08:00</updated><author><name>Ben Chuanlong Du</name></author><id>tag:misc.legendu.net,2018-09-04:/blog/ai-hyper-parameter-auto-ml/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="methodology"&gt;Methodology&lt;/h2&gt;
&lt;p&gt;hyper-parameter tuning, 
grid search
bayesian optimization 
evolutionary algorithms
genetic programming
cross validation
k-fold 
&lt;a href="https://openreview.net/pdf?id=r1Ue8Hcxg"&gt;Neural Architecture Search with Reinforcement Learning&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="libraries"&gt;Libraries&lt;/h2&gt;
&lt;h3 id="optuna"&gt;&lt;a href="https://github.com/optuna/optuna"&gt;Optuna&lt;/a&gt;&lt;/h3&gt;
&lt;h3 id="auto-sklearn"&gt;auto-sklearn&lt;/h3&gt;
&lt;h3 id="ludwig"&gt;&lt;a href="https://github.com/uber/ludwig"&gt;Ludwig&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Ludwig is a toolbox that allows to …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="methodology"&gt;Methodology&lt;/h2&gt;
&lt;p&gt;hyper-parameter tuning, 
grid search
bayesian optimization 
evolutionary algorithms
genetic programming
cross validation
k-fold 
&lt;a href="https://openreview.net/pdf?id=r1Ue8Hcxg"&gt;Neural Architecture Search with Reinforcement Learning&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="libraries"&gt;Libraries&lt;/h2&gt;
&lt;h3 id="optuna"&gt;&lt;a href="https://github.com/optuna/optuna"&gt;Optuna&lt;/a&gt;&lt;/h3&gt;
&lt;h3 id="auto-sklearn"&gt;auto-sklearn&lt;/h3&gt;
&lt;h3 id="ludwig"&gt;&lt;a href="https://github.com/uber/ludwig"&gt;Ludwig&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Ludwig is a toolbox that allows to train and evaluate deep learning models without the need to write code.&lt;/p&gt;
&lt;h3 id="turicreate"&gt;&lt;a href="https://github.com/apple/turicreate"&gt;turicreate&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Turi Create simplifies the development of custom machine learning models. 
You don't have to be a machine learning expert 
to add recommendations, object detection, image classification, image similarity or activity classification to your app.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Easy-to-use: Focus on tasks instead of algorithms&lt;/li&gt;
&lt;li&gt;Visual: Built-in, streaming visualizations to explore your data&lt;/li&gt;
&lt;li&gt;Flexible: Supports text, images, audio, video and sensor data&lt;/li&gt;
&lt;li&gt;Fast and Scalable: Work with large datasets on a single machine&lt;/li&gt;
&lt;li&gt;Ready To Deploy: Export models to Core ML for use in iOS, macOS, watchOS, and tvOS apps&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="pycaret"&gt;&lt;a href="https://github.com/pycaret/pycaret"&gt;PyCaret&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;PyCaret is an open source &lt;code&gt;low-code&lt;/code&gt; machine learning library in Python 
that aims to reduce the hypothesis to insights cycle time in a ML experiment. 
It enables data scientists to perform end-to-end experiments quickly and efficiently. 
In comparison with the other open source machine learning libraries, 
PyCaret is an alternate low-code library 
that can be used to perform complex machine learning tasks with only few lines of code. 
PyCaret is essentially a Python wrapper 
around several machine learning libraries and frameworks 
such as &lt;code&gt;scikit-learn&lt;/code&gt;, &lt;code&gt;XGBoost&lt;/code&gt;, &lt;code&gt;Microsoft LightGBM&lt;/code&gt;, &lt;code&gt;spaCy&lt;/code&gt; and many more. &lt;/p&gt;
&lt;h3 id="autogluon"&gt;&lt;a href="https://github.com/awslabs/autogluon"&gt;autogluon&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;AutoGluon automates machine learning tasks enabling you 
to easily achieve strong predictive performance in your applications. 
With just a few lines of code, 
you can train and deploy high-accuracy deep learning models on tabular, image, and text data.&lt;/p&gt;
&lt;h3 id="apache-ray-tune"&gt;Apache Ray Tune&lt;/h3&gt;
&lt;h3 id="h2o-automl"&gt;H2O AutoML&lt;/h3&gt;
&lt;p&gt;Python: H2OAutoML(...)&lt;/p&gt;
&lt;h3 id="driverless-ai"&gt;Driverless AI&lt;/h3&gt;
&lt;h3 id="tpot-looks-like-a-good-one"&gt;tpot looks like a good one&lt;/h3&gt;
&lt;h2 id="platformsframework"&gt;Platforms/Framework&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://cloud.google.com/automl/"&gt;Google Cloud AutoML&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="shared-resources-of-models"&gt;Shared Resources of Models&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.tensorflow.org/hub"&gt;TensorFlow Hub&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://cloud.google.com/ai-hub/"&gt;Google AI Hub&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://dagshub.com/"&gt;DAGsHub&lt;/a&gt;
DAGsHub is a web platform for data version control and collaboration for data scientists and machine learning engineers.
It is like GitHub for data science and machine learning.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/google/ml-metadata"&gt;ml-metadata&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/mlflow/mlflow"&gt;mlflow&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kaggle&lt;/p&gt;
&lt;p&gt;transformers&lt;/p&gt;
&lt;h2 id="experiment-tracking"&gt;Experiment Tracking&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/neptune-ai/neptune-client"&gt;neptune-client&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;wandb, fitlog, runx&lt;/p&gt;
&lt;h2 id="examples"&gt;Examples&lt;/h2&gt;
&lt;p&gt;https://github.com/h2oai/driverlessai-recipes&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;https://arxiv.org/pdf/1908.00709v1.pdf&lt;/p&gt;
&lt;p&gt;https://towardsdatascience.com/an-example-of-hyperparameter-optimization-on-xgboost-lightgbm-and-catboost-using-hyperopt-12bc41a271e&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="machine learning"></category><category term="framework"></category><category term="AutoML"></category><category term="mlflow"></category><category term="Lugwid"></category><category term="Optuna"></category><category term="turicreate"></category><category term="PyCaret"></category></entry><entry><title>Tips on Reinforcement Learning</title><link href="https://misc.legendu.net/blog/tips-on-reinforcement-learning/" rel="alternate"></link><published>2020-01-28T11:02:29-08:00</published><updated>2020-11-28T11:02:29-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-01-28:/blog/tips-on-reinforcement-learning/</id><summary type="html">&lt;p&gt;**
Things on this page are fragmentary and immature notes/thoughts of the author.
Please read with your own judgement!&lt;/p&gt;
&lt;p&gt;&lt;a href="https://medium.com/@SmartLabAI/reinforcement-learning-algorithms-an-intuitive-overview-904e2dff5bbc"&gt;Reinforcement Learning algorithms — an intuitive overview&lt;/a&gt;
has a good overview of different approaches to RL.
**&lt;/p&gt;
&lt;h2 id="courses-tutorials"&gt;Courses &amp;amp; Tutorials&lt;/h2&gt;
&lt;p&gt;http://rail.eecs.berkeley.edu/deeprlcourse/&lt;/p&gt;
&lt;p&gt;https://towardsdatascience.com/reinforcement-learning-q-learning-with-decision-trees-ecb1215d9131&lt;/p&gt;
&lt;h2 id="frameworks"&gt;Frameworks&lt;/h2&gt;
&lt;p&gt;The article 
&lt;a href="https://winderresearch.com/a-comparison-of-reinforcement-learning-frameworks-dopamine-rllib-keras-rl-coach-trfl-tensorforce-coach-and-more/"&gt;A …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;**
Things on this page are fragmentary and immature notes/thoughts of the author.
Please read with your own judgement!&lt;/p&gt;
&lt;p&gt;&lt;a href="https://medium.com/@SmartLabAI/reinforcement-learning-algorithms-an-intuitive-overview-904e2dff5bbc"&gt;Reinforcement Learning algorithms — an intuitive overview&lt;/a&gt;
has a good overview of different approaches to RL.
**&lt;/p&gt;
&lt;h2 id="courses-tutorials"&gt;Courses &amp;amp; Tutorials&lt;/h2&gt;
&lt;p&gt;http://rail.eecs.berkeley.edu/deeprlcourse/&lt;/p&gt;
&lt;p&gt;https://towardsdatascience.com/reinforcement-learning-q-learning-with-decision-trees-ecb1215d9131&lt;/p&gt;
&lt;h2 id="frameworks"&gt;Frameworks&lt;/h2&gt;
&lt;p&gt;The article 
&lt;a href="https://winderresearch.com/a-comparison-of-reinforcement-learning-frameworks-dopamine-rllib-keras-rl-coach-trfl-tensorforce-coach-and-more/"&gt;A Comparison of Reinforcement Learning Frameworks: Dopamine, RLLib, Keras-RL, Coach, TRFL, Tensorforce, Coach and more&lt;/a&gt;
has a comprehensive comparison of diferent reinforcement learning frameworks. 
Based on the author's opinion,
&lt;a href="https://github.com/openai/gym"&gt;OpenAI Gym&lt;/a&gt;
and 
Apache Ray 
&lt;a href="https://github.com/ray-project/ray/tree/master/rllib"&gt;RLLib&lt;/a&gt;
are the best 2 libraries.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://medium.com/data-from-the-trenches/choosing-a-deep-reinforcement-learning-library-890fb0307092"&gt;On Choosing a Deep Reinforcement Learning Library&lt;/a&gt;
recommends Stable Baselines and TF-Agents.&lt;/p&gt;
&lt;p&gt;https://github.com/kngwyu/rogue-gym&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;https://winderresearch.com/a-comparison-of-reinforcement-learning-frameworks-dopamine-rllib-keras-rl-coach-trfl-tensorforce-coach-and-more/&lt;/p&gt;
&lt;p&gt;https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-part-ii-trpo-ppo-87f2c5919bb9&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/1707.06347.pdf&lt;/p&gt;
&lt;p&gt;https://medium.com/@jonathan_hui/rl-proximal-policy-optimization-ppo-explained-77f014ec3f12&lt;/p&gt;
&lt;p&gt;https://openai.com/blog/openai-baselines-ppo/&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="RL"></category><category term="reinforcement learning"></category><category term="RLLib"></category></entry><entry><title>Tips on NLP</title><link href="https://misc.legendu.net/blog/tips-on-nlp/" rel="alternate"></link><published>2020-01-28T10:59:14-08:00</published><updated>2020-11-28T10:59:14-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-01-28:/blog/tips-on-nlp/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;https://blog.floydhub.com/
is a great place for deep learning blogging.&lt;/p&gt;
&lt;h2 id="overview-of-nlp"&gt;Overview of NLP&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://medium.com/dair-ai/deep-learning-for-nlp-an-overview-of-recent-trends-d0d8f40a776d"&gt;Deep Learning for NLP: An Overview of Recent Trends&lt;/a&gt;
Chapter 8 of the book (Performance of …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;https://blog.floydhub.com/
is a great place for deep learning blogging.&lt;/p&gt;
&lt;h2 id="overview-of-nlp"&gt;Overview of NLP&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://medium.com/dair-ai/deep-learning-for-nlp-an-overview-of-recent-trends-d0d8f40a776d"&gt;Deep Learning for NLP: An Overview of Recent Trends&lt;/a&gt;
Chapter 8 of the book (Performance of Different Models on Different NLP Tasks) also summarizes the state-of-the-art methods 
fore each sub area of NLP. &lt;/p&gt;
&lt;p&gt;&lt;a href="https://blog.floydhub.com/ten-trends-in-deep-learning-nlp/"&gt;Ten trends in Deep learning NLP&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/&lt;/p&gt;
&lt;p&gt;https://github.com/Oxer11/NLP-task-review&lt;/p&gt;
&lt;p&gt;https://nlpoverview.com/&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/1703.03906.pdf&lt;/p&gt;
&lt;p&gt;The transformer architecture, which was first published at the end of 2017, 
addresses this by creating a way to allow parallel inputs. 
Each word can have a separate embedding and be process simultaneously 
which greatly improves training times which facilitates training on much larger datasets.&lt;/p&gt;
&lt;p&gt;Google's BERT and OpenAI's GPT-2 models are based on Transformer.&lt;/p&gt;
&lt;p&gt;transformer-XL&lt;/p&gt;
&lt;h2 id="semantics-vs-syntactic"&gt;Semantics vs Syntactic&lt;/h2&gt;
&lt;p&gt;https://medium.com/huggingface/learning-meaning-in-natural-language-processing-the-semantics-mega-thread-9c0332dfe28e&lt;/p&gt;
&lt;h2 id="coreferences"&gt;Coreferences&lt;/h2&gt;
&lt;p&gt;https://medium.com/huggingface/state-of-the-art-neural-coreference-resolution-for-chatbots-3302365dcf30&lt;/p&gt;
&lt;p&gt;https://huggingface.co/coref/&lt;/p&gt;
&lt;p&gt;https://github.com/huggingface/neuralcoref&lt;/p&gt;
&lt;h2 id="machine-translation"&gt;Machine Translation&lt;/h2&gt;
&lt;p&gt;Transformer&lt;/p&gt;
&lt;p&gt;In machine translation, 
self-attention also contributes to impressive results. 
For example, 
recently a model, named Transformer, 
was introduced in a paper with a rather bold title “Attention Is All You Need” [Vaswani, 2017]. 
As you can guess, 
this model relies only on self-attention without the use of RNNs. 
As a result, 
it is highly parallelizable and requires less time to train, while establishing state-of-the-art results on WMT2014.&lt;/p&gt;
&lt;p&gt;Large embeddings with 2048 dimensions
achieved the best results, but only by a small
margin. Even small embeddings with 128 dimensions seem to have sufficient capacity to
capture most of the necessary semantic information.
• LSTM Cells consistently outperformed GRU
Cells.
• Bidirectional encoders with 2 to 4 layers performed best. Deeper encoders were significantly more unstable to train, but show potential if they can be optimized well.
• Deep 4-layer decoders slightly outperformed
shallower decoders. Residual connections
were necessary to train decoders with 8 layers and dense residual connections offer additional robustness.
• Parameterized additive attention yielded the
overall best results.
A well-tuned beam search with length
penalty is crucial. Beam widths of 5 to 10
together with a length penalty of 1.0 seemed
to work well.&lt;/p&gt;
&lt;h2 id="seq2seq"&gt;seq2seq&lt;/h2&gt;
&lt;p&gt;https://github.com/IBM/pytorch-seq2seq&lt;/p&gt;
&lt;h2 id="attention"&gt;Attention&lt;/h2&gt;
&lt;p&gt;The article &lt;a href="https://medium.com/@joealato/attention-in-nlp-734c6fa9d983"&gt;Attention in NLP&lt;/a&gt;
has a very detailed summary of development and applications of Attention in NLP.&lt;/p&gt;
&lt;h2 id="libraries"&gt;Libraries&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/explosion/spaCy"&gt;SpaCy&lt;/a&gt;
is an industrial-strength Natural Language Processing (NLP) library.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/huggingface/transformers"&gt;transformers&lt;/a&gt;
(formerly known as pytorch-transformers and pytorch-pretrained-bert) 
provides state-of-the-art general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet, CTRL...) 
for Natural Language Understanding (NLU) and Natural Language Generation (NLG) 
with over 32+ pretrained models in 100+ languages and deep interoperability between TensorFlow 2.0 and PyTorch.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/huggingface/tokenizers/tree/master/bindings/python"&gt;huggingface/tokenizers&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/OpenNMT/OpenNMT-py"&gt;OpenNMT-py&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;https://github.com/jadore801120/attention-is-all-you-need-pytorch&lt;/p&gt;
&lt;p&gt;https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py&lt;/p&gt;
&lt;p&gt;https://github.com/google/seq2seq&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/RaRe-Technologies/gensim"&gt;gensim&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/stanfordnlp/GloVe"&gt;GloVe&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="data"&gt;Data&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://nyu-mll.github.io/CoLA/"&gt;CoLA: The Corpus of Linguistic Acceptability&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;https://medium.com/towards-artificial-intelligence/cross-lingual-language-model-56a65dba9358&lt;/p&gt;
&lt;p&gt;https://github.com/huggingface/transformers&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="machine learning"></category><category term="data science"></category><category term="NLP"></category><category term="nature language processing"></category></entry><entry><title>Machine Learning Resources</title><link href="https://misc.legendu.net/blog/machine-learning-resources/" rel="alternate"></link><published>2019-02-15T12:02:54-08:00</published><updated>2020-11-15T12:02:54-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2019-02-15:/blog/machine-learning-resources/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="platforms"&gt;Platforms&lt;/h2&gt;
&lt;p&gt;https://azure.microsoft.com/en-us/services/virtual-machines/data-science-virtual-machines/&lt;/p&gt;
&lt;p&gt;Google TPU&lt;/p&gt;
&lt;p&gt;Amazon&lt;/p&gt;
&lt;h2 id="misc"&gt;Misc&lt;/h2&gt;
&lt;p&gt;https://github.com/bulutyazilim/awesome-datascience&lt;/p&gt;
&lt;h2 id="data-miningmachine-learning"&gt;Data Mining/Machine Learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.holehouse.org/mlclass/"&gt;Stanford Machine Learning&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="http://work.caltech.edu/library/index.html"&gt;Machine Learning Video Library&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="http://work.caltech.edu/telecourse.html"&gt;Learning from …&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="platforms"&gt;Platforms&lt;/h2&gt;
&lt;p&gt;https://azure.microsoft.com/en-us/services/virtual-machines/data-science-virtual-machines/&lt;/p&gt;
&lt;p&gt;Google TPU&lt;/p&gt;
&lt;p&gt;Amazon&lt;/p&gt;
&lt;h2 id="misc"&gt;Misc&lt;/h2&gt;
&lt;p&gt;https://github.com/bulutyazilim/awesome-datascience&lt;/p&gt;
&lt;h2 id="data-miningmachine-learning"&gt;Data Mining/Machine Learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.holehouse.org/mlclass/"&gt;Stanford Machine Learning&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="http://work.caltech.edu/library/index.html"&gt;Machine Learning Video Library&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="http://work.caltech.edu/telecourse.html"&gt;Learning from Data&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="http://videolectures.net/single_jain_bigdata/"&gt;Big Data Clustering, Anil Jain&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.youtube.com/watch?v=nL55ixBbcMU"&gt;Big Data Clustering, Anil Jain&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.stanford.edu/~hastie/Papers/ESLII.pdf"&gt;The Elements of Statistical Learning&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="http://faculty.chass.ncsu.edu/garson/PA765/statnote.htm"&gt;StatNotes&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.ats.ucla.edu/stat/stata/whatstat/default.htm"&gt;UCLA&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.ats.ucla.edu/stat/"&gt;Statistical Computing&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.statsoft.com/textbook/elementary-statistics-concepts/"&gt;Elementary Statistical Concept&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.rdatamining.com/"&gt;RDataMining&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=qRJ3GKMOFrE"&gt;Lecture 5 | Machine Learning (Stanford) | Naive Bayesian Classification with an Example of Spam Email Prediction&lt;/a&gt;  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=qyyJKd-zXRE"&gt;Lecture 6 | Machine Learning (Stanford) | Naive Baysian, Neuron Network, SVM&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=s8B4A5ubw6c"&gt;Lecture 7 | Machine Learning (Stanford) | SVM, optimal margin classifier, primary/dual optimization, KKT, Kenel&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=bUv9bfMPMb4"&gt;Lecture 8 | Machine Learning (Stanford) | SVM, Kenel&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=tojaGtMPo5U"&gt;Lecture 9 | Machine Learning (Stanford) | Learning Theories&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=0kWZoyNRxTY"&gt;Lecture 10 | Machine Learning (Stanford) | Learning Theories, Variable Selection&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=sQ8T9b-uGVE"&gt;Lecture 11 | Machine Learning (Stanford) | Tips for Machine Learning&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=ZZGTuAkF-Hw"&gt;Lecture 12 | Machine Learning (Stanford) | Unsupervised Learning&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=LBtuYU-HfUg"&gt;Lecture 13 | Machine Learning (Stanford) | EM algorithm&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=ey2PE5xi9-A"&gt;Lecture 14 | Machine Learning (Stanford) | Factor Analysis, PCA&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=QGd06MTRMHs"&gt;Lecture 15 | Machine Learning (Stanford) | PCA, ICA&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=RtxI449ZjSc"&gt;Lecture 16 | Machine Learning (Stanford) | Reinforcement Learning&lt;/a&gt;  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=UFH5ibWnA7g"&gt;Lecture 19 | Machine Learning (Stanford) | Advice for Machine Learning&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=WOOTNBxbi8c"&gt;Decision Tree (1)&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=U2A-g6-Prrs"&gt;Decision Tree (2)&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=Yvn3--rIdZg"&gt;Ensemble/Aggregation (1) Basics&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=Yvn3--rIdZg"&gt;Ensemble/Aggregation (2) Bagging&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=Yvn3--rIdZg"&gt;Ensemble/Aggregation (3) Gradient Boosting&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=Yvn3--rIdZg"&gt;Ensemble/Aggregation (1)&lt;/a&gt;  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=IO7F1-PlKNM"&gt;Random Forests Theory and Applications for Variable Selection - Video 1 of 5&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=cQrvTYVN0ko"&gt;Random Forests Theory and Applications for Variable Selection - Video 2 of 5&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;https://github.com/academic/awesome-datascience&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://startupsventurecapital.com/essential-cheat-sheets-for-machine-learning-and-deep-learning-researchers-efb6a8ebd2e5"&gt;Essential Cheat Sheets for Machine Learning and Deep Learning Engineers&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;https://rushter.com/dsreader/&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</content><category term="AI"></category><category term="AI"></category><category term="machine learning"></category><category term="data science"></category><category term="resources"></category></entry><entry><title>Data Science Version Control and Continuous Delivery</title><link href="https://misc.legendu.net/blog/data-science-version-control-and-continuous-delivery/" rel="alternate"></link><published>2020-01-07T08:52:52-08:00</published><updated>2020-10-07T08:52:52-07:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-01-07:/blog/data-science-version-control-and-continuous-delivery/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="data-version-control"&gt;Data Version Control&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://dvc.org/"&gt;dvc&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://dagshub.com/"&gt;DAGsHub&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;dvc + DAGsHub sounds like a good lightweighted way for data version control.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.dolthub.com/"&gt;dolthub&lt;/a&gt; is another good way for data version control.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/pachyderm/pachyderm"&gt;Pachyderm&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Data Versioning, Data Pipelines …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="data-version-control"&gt;Data Version Control&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://dvc.org/"&gt;dvc&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://dagshub.com/"&gt;DAGsHub&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;dvc + DAGsHub sounds like a good lightweighted way for data version control.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.dolthub.com/"&gt;dolthub&lt;/a&gt; is another good way for data version control.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/pachyderm/pachyderm"&gt;Pachyderm&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Data Versioning, Data Pipelines, and Data Lineage&lt;/p&gt;
&lt;p&gt;&lt;a href="https://medium.com/thelaunchpad/retracing-your-steps-in-machine-learning-ml-versioning-74d19a66bd08"&gt;Retracing Your Steps in Machine Learning&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="model-life-cycle-tracking"&gt;Model Life Cycle Tracking&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://mlflow.org/docs/latest/index.html"&gt;mlflow&lt;/a&gt;
mlflow tracks every detail about a model (including training, servering, etc.)
but it seems to be a little bit complicated to use.&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://martinfowler.com/articles/cd4ml.html"&gt;Continuous Delivery for Machine Learning&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;https://towardsdatascience.com/version-control-ml-model-4adb2db5f87c&lt;/p&gt;
&lt;p&gt;&lt;a href="https://algorithmia.com/blog/how-to-version-control-your-production-machine-learning-models"&gt;How to version control your production machine learning models&lt;/a&gt;&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="machine learning"></category><category term="data science"></category><category term="continuous delivery"></category><category term="data science version control"></category></entry><entry><title>Nature Language Processing Using NLTK</title><link href="https://misc.legendu.net/blog/nltk-tips/" rel="alternate"></link><published>2014-11-22T14:30:18-08:00</published><updated>2020-05-22T14:30:18-07:00</updated><author><name>Ben Chuanlong Du</name></author><id>tag:misc.legendu.net,2014-11-22:/blog/nltk-tips/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;nltk.util.ngrams
nltk.bigrams
nltk.PorterStemmer&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;nltk.util&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;ngrams&lt;/span&gt;
&lt;span class="n"&gt;sentence&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;this is a foo bar sentences and i want to ngramize it&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;
&lt;span class="n"&gt;sixgrams&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ngrams&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentence&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split …&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;nltk.util.ngrams
nltk.bigrams
nltk.PorterStemmer&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;nltk.util&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;ngrams&lt;/span&gt;
&lt;span class="n"&gt;sentence&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;this is a foo bar sentences and i want to ngramize it&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;
&lt;span class="n"&gt;sixgrams&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ngrams&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentence&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;grams&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;sixgrams&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt; &lt;span class="n"&gt;grams&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content><category term="AI"></category><category term="machine learning"></category><category term="data mining"></category><category term="text mining"></category><category term="data science"></category><category term="NLP"></category></entry><entry><title>Time Series Analysis</title><link href="https://misc.legendu.net/blog/time-series-tips/" rel="alternate"></link><published>2015-01-22T13:26:10-08:00</published><updated>2020-05-22T13:26:10-07:00</updated><author><name>Ben Chuanlong Du</name></author><id>tag:misc.legendu.net,2015-01-22:/blog/time-series-tips/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In statistics, 
a unit root test tests whether a time series variable is non-stationary using an autoregressive model. 
A well-known test that is valid in large samples is the augmented Dickey …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In statistics, 
a unit root test tests whether a time series variable is non-stationary using an autoregressive model. 
A well-known test that is valid in large samples is the augmented Dickey–Fuller test. 
The optimal finite sample tests for a unit root in autoregressive models were developed by Denis Sargan and Alok Bhargava. 
Another test is the Phillips–Perron test. 
These tests use the existence of a unit root as the null hypothesis.&lt;/p&gt;
&lt;p&gt;In statistics and econometrics, an augmented Dickey–Fuller test (ADF) is a test for a unit root in a time series sample. 
It is an augmented version of the Dickey–Fuller test for a larger and more complicated set of time series models. 
The augmented Dickey–Fuller (ADF) statistic, used in the test, is a negative number. 
The more negative it is, the stronger the rejection of the hypothesis that there is a unit root at some level of confidence.[1]&lt;/p&gt;</content><category term="AI"></category><category term="statistics"></category><category term="Time Series Analysis"></category><category term="unit root test"></category><category term="data science"></category><category term="model"></category><category term="AI"></category></entry><entry><title>Keywords Extracting from Text</title><link href="https://misc.legendu.net/blog/keywords%20extracting%20from%20text/" rel="alternate"></link><published>2014-11-22T13:08:04-08:00</published><updated>2020-05-22T13:08:04-07:00</updated><author><name>Ben Chuanlong Du</name></author><id>tag:misc.legendu.net,2014-11-22:/blog/keywords extracting from text/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="word-stemming"&gt;Word Stemming&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;existing stemming method such as NLTK.PorterStem, etc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;didn't -&amp;gt; did not, there's -&amp;gt; there is, etc.
    Mr. -&amp;gt; Mister
    Mrs. -&amp;gt; ...
    Ms. -&amp;gt; ...&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="other-things"&gt;Other things&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;it seems that it is hard to get …&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="word-stemming"&gt;Word Stemming&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;existing stemming method such as NLTK.PorterStem, etc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;didn't -&amp;gt; did not, there's -&amp;gt; there is, etc.
    Mr. -&amp;gt; Mister
    Mrs. -&amp;gt; ...
    Ms. -&amp;gt; ...&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="other-things"&gt;Other things&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;it seems that it is hard to get useful information using 1-gram&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;URLs in text are often important and is relatively easy to extract. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;After handing URLs, you can replace "/" and "." with spaces to avoid confusing them with real long words.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;long words often contain useful information, 
    however, you have to be careful about  words of the form "and/or", etc.
    And do not confuse it with URLs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the idea of keeping upper/lower quantile (e.g., 5%) of long words, 2-grams, etc. is a very good idea&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</content><category term="AI"></category><category term="machine learning"></category><category term="text mining"></category><category term="data mining"></category><category term="data science"></category><category term="NLP"></category><category term="deep learning"></category></entry><entry><title>Experiment Design</title><link href="https://misc.legendu.net/blog/linear-models/" rel="alternate"></link><published>2012-11-22T12:24:54-08:00</published><updated>2020-05-22T12:24:54-07:00</updated><author><name>Ben Chuanlong Du</name></author><id>tag:misc.legendu.net,2012-11-22:/blog/linear-models/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Complete Randomized Design (CRD)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Randomized Complete Block Design (CBD)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;same RNE as CRD&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Latin Square Design (LSD)&lt;ul&gt;
&lt;li&gt;same RNE as CRD&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Balanced Incomplete Block Design&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;all treatments cannot fit in any …&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Complete Randomized Design (CRD)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Randomized Complete Block Design (CBD)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;same RNE as CRD&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Latin Square Design (LSD)&lt;ul&gt;
&lt;li&gt;same RNE as CRD&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Balanced Incomplete Block Design&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;all treatments cannot fit in any one block&lt;/li&gt;
&lt;li&gt;each treatment appears the same number of times in the design&lt;/li&gt;
&lt;li&gt;each pair of treatment appears together in the same number of blocks&lt;/li&gt;
&lt;li&gt;BIBD does not exist for every t (number of treatments), b (number of block) and k (the number of units in each block)&lt;/li&gt;
&lt;li&gt;inter-block comparison, intra-block information is usually sacrificed (confounded with unknown block effect)&lt;/li&gt;
&lt;li&gt;when block effect is random, intra-block information can be recovered. There is also a neat trick for BIBD when block effect is random: do inference using block totals (not necessarily better than inter-block information only)
assumptions:&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Split-plot Design (SPD)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;common in animal science, agriculture, engineering, chemistry, etc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;can analyze separately for whole-plot and split-plot (sub-plot) parts&lt;/li&gt;
&lt;li&gt;there are random effects in SPD&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;factorial analysis:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;appearance of higher interactions requires appearance of lower interactions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;EFFECT HIERARCHY PRINCIPLE – If an interaction
involving a given set of factors is included in the model, all
main effects and interactions involving subsets of these factors
should also be included.
• EFFECT HEREDITY PRINCIPLE – If an interaction
involving a given set of factors is included in the model, at least
one effect of the next smallest order involving a subset of these
factors should also be included.&lt;/p&gt;
&lt;p&gt;some times purposely confound a factor with another (usually these factors are not of interests to us) can be smart&lt;/p&gt;
&lt;p&gt;random and fixed effect, fixed is easy most of time,&lt;/p&gt;
&lt;p&gt;sometimes if random is too hard (usually seen in generalized linear models), can treat as fixed&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;independent&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;distribution assumption&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;equal variance/dispersion &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;residual plot&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Primary concern about model is lack of fit&lt;/p&gt;
&lt;p&gt;Primary concern about variance is equality&lt;/p&gt;
&lt;p&gt;Levene Test and BF test &lt;/p&gt;
&lt;p&gt;Simultaneous confidence intervals&lt;/p&gt;
&lt;p&gt;Scheffe (all)&lt;/p&gt;
&lt;p&gt;Tukey (pairwise)&lt;/p&gt;
&lt;p&gt;Dunnett (multiple vs one)   &lt;/p&gt;
&lt;p&gt;Estimation of Variance Components&lt;/p&gt;
&lt;p&gt;REML vs ML&lt;/p&gt;
&lt;p&gt;recursive estimating ... suppose we know variance we can estimate coefficients, and then estimate variances ... repeat ...&lt;/p&gt;
&lt;p&gt;non-linear ... based on approximation ...&lt;/p&gt;</content><category term="AI"></category><category term="model"></category><category term="statistics"></category><category term="regression"></category><category term="AI"></category><category term="experiement design"></category></entry><entry><title>Sampling Methods</title><link href="https://misc.legendu.net/blog/sampling-methods/" rel="alternate"></link><published>2013-04-22T12:23:20-07:00</published><updated>2020-05-22T12:23:20-07:00</updated><author><name>Ben Chuanlong Du</name></author><id>tag:misc.legendu.net,2013-04-22:/blog/sampling-methods/</id><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="probability-sampling"&gt;Probability Sampling&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Random Sampling&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Systematic Sampling&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stratified Sampling&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="non-probability-sampling"&gt;Non-probability Sampling&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Convenience Sampling&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Judgement Sampling&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Quota Sampling &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Snowball Sampling &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;bias&lt;/p&gt;</content><category term="AI"></category><category term="statistics"></category><category term="sampling"></category><category term="survey"></category><category term="AI"></category></entry><entry><title>Model Fitting in ANOVA Analysis</title><link href="https://misc.legendu.net/blog/model-fitting/" rel="alternate"></link><published>2012-11-22T12:21:02-08:00</published><updated>2020-05-22T12:21:02-07:00</updated><author><name>Ben Chuanlong Du</name></author><id>tag:misc.legendu.net,2012-11-22:/blog/model-fitting/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="contrast"&gt;Contrast&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Usually it does not matter what contrast(s) you use for factors in
    linear model problems[^20], so you can choose appropriate
    contrast(s) so that your problem is most …&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="contrast"&gt;Contrast&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Usually it does not matter what contrast(s) you use for factors in
    linear model problems[^20], so you can choose appropriate
    contrast(s) so that your problem is most simplified. In R, you can
    use &lt;code&gt;contr.helmert&lt;/code&gt;, &lt;code&gt;contr.helmert&lt;/code&gt;, &lt;code&gt;contr.poly&lt;/code&gt;, &lt;code&gt;contr.sum&lt;/code&gt;,
    &lt;code&gt;contr.treatment&lt;/code&gt; and &lt;code&gt;contr.SAS&lt;/code&gt; to create different kinds of
    contrasts. Among all these functions, the last three are popular. By
    default, R set the first level (whichever has the smallest order
    among all the levels in R) of a factor to be 0. If you want to set
    another level to be 0, you can use &lt;code&gt;contr.treatment&lt;/code&gt; together with
    &lt;code&gt;contrasts&lt;/code&gt;. For example, suppose you have a factor &lt;code&gt;brand&lt;/code&gt; in R:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&amp;gt; brand
[1] Dell   Dell   Lenovo Lenovo Mac    Mac
Levels: Dell Lenovo Mac

&amp;gt; contrasts(brand)
       Lenovo Mac
Dell        0   0
Lenovo      1   0
Mac         0   1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You are interested in comparing &lt;code&gt;Dell&lt;/code&gt; and &lt;code&gt;Lenovo&lt;/code&gt; to &lt;code&gt;Mac&lt;/code&gt;, i.e.
Mac is the control treatment (level) here. It's more convenient to
do tests if you set &lt;code&gt;Mac&lt;/code&gt; as the control treatment. To do so, you
can use the following command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&amp;gt; contrasts(brand) = contr.treatment(3,3)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now you can check the contrast again to see how R will constructs
design matrix based factor &lt;code&gt;brand&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&amp;gt; contrasts(brand)
       1 2
Dell   1 0
Lenovo 0 1
Mac    0 0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In anova, more often we data in hand and want to create the design
    matrix from the data according to different constraints. We can use
    function &lt;code&gt;lm&lt;/code&gt; and &lt;code&gt;model.matrix&lt;/code&gt; to achieve it. In order to obtain
    different kind of design matrix, we can change the options before we
    do the analysis of variance. To do this we can use function
    &lt;code&gt;options&lt;/code&gt;. The parameter contrast in this function can be
    "contr.treatment", "contr.sum" and "contr.SAS" and so on.
    However, &lt;code&gt;model.matrix&lt;/code&gt; only work for fixed effect factors. &lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="formulas"&gt;Formulas&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;统计中有很多不同的线性和非线性模型， 将它们写成R能识别的表达式是利用R来求解这些模型的第一步也是非常重要的一步， 这就决定了在R中书写
    Formula是非常重要的。 R中大部分处理模型的函数的Formula遵从通用的规则，
    但是有些Package里面的处理模型的函数有其自己的特殊规则， 比如说函数lme()和lmer()。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;函数lme()使用&lt;span class="math"&gt;\(\mid\)&lt;/span&gt;来分隔低层因此和Group因子， 用/表示因子的嵌套结构。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;函数lmer()使用小括号来表示Random Effect， 亦即小括号内的表达式部分表示随机效应， 不在小括号里面的部分表示固定效应。
    符号&lt;span class="math"&gt;\(\mid\)&lt;/span&gt;用来分隔低层因子和Group因子， &lt;span class="math"&gt;\(\mid\)&lt;/span&gt;之前的事低层因子，其后的是Group因子。
    如果没有低层因子（也可以说低层因子只有一个水平）则低层因子可以用1表示。 这个在Split模型和Repeat Measure
    模型中很有用。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="model-fitting"&gt;Model Fitting&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;There're many different functions in R used to fit different models.
    Besides, there're some many useful functions to extract useful
    information from the fitted model, such as &lt;code&gt;vcov&lt;/code&gt;,
    &lt;code&gt;fitted&lt;/code&gt;,&lt;code&gt;residuals&lt;/code&gt;,&lt;code&gt;effects&lt;/code&gt; and &lt;code&gt;coef&lt;/code&gt;. Also there's a function
    &lt;code&gt;aov&lt;/code&gt; which can be used to compare different models.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在ANOVA里面，线性模型的选择取决很多不同的因素。&lt;/p&gt;
&lt;p&gt;(i) 分析试验的设计方案，找出潜在的因子（包括我们实际感兴趣的因子， 以及block factors 等）&lt;/p&gt;
&lt;p&gt;(ii) 决定要不要加入因子的交互效应。这个步骤遵循几个原则：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;一般来说，我们不考虑block factor和感兴趣的factor的交互效应。这个不是绝对的，
    比如说split-plot design里面...(check
    it)。这个还是取决于试验的设计方案。在统计里面block里面的 EU性质是相近的，也就是观测值是正相关的。block
    factor 和一般因子的交互效应还是可以看作是新的sub-block.
    至于这个sub-block应不应该加入模型，可以看实际情况中sub-block 里面的EUs是不是正相关的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;选择的统计模型必须能够估计Error（否则无法检验模型），这就决定
    了当观测值不是很多（比如说一个Treatment只有一个观测值时），
    不可能所有的interaction都加入模型中。根据DOE里面的...准则（check
    it)我们应该将低级interaction保留在模型中,
    而将高级interaction舍弃使得eroor可以被估计。当然这实属不得已而为之。
    如果你有足够的观测值，当然可以把高级interaction加入模型中。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;通常来说我倾向于首先选择一个尽量大的模型，但这不表示我们认为这个模型是
    正确的。这个大模型只是为了方便进一步选择更好的模型。如果基于这个大模型，
    你认为某个interaction是不显著的，你可以将其从模型移除。当然你还是遵循
    前面提到的..准则，也就是说如果你的模型包含的高级Interaction，那么它的所有
    子interaction都应该包含在模型里面。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以上不过是纸上谈兵，实际操作中我们必须去fit很多模型。那么如何fit模型呢？我们知道在SAS 里面MIX procedure是fit
mixed linear models 的利器。在R里面，我们以用&lt;code&gt;nlme&lt;/code&gt;里面的&lt;code&gt;lme&lt;/code&gt;或者&lt;code&gt;lme4&lt;/code&gt;里面的&lt;code&gt;lmer&lt;/code&gt;来fit
mixed linear models.
这两个函数的模型声明语法类似，但是&lt;code&gt;lmer&lt;/code&gt;的模型声明语法更灵活简单一些。&lt;code&gt;lmer&lt;/code&gt;的不足之
处是fit完模型后不方便作统计检验。&lt;code&gt;anova&lt;/code&gt; 作用在&lt;code&gt;mer&lt;/code&gt;(&lt;code&gt;lmer&lt;/code&gt;的输出对象)的结果输出不完整，比如说F
test的自由度，以及Pvalues等都没有输出。这些信息只能根据自已对模型的了解来决定。而&lt;code&gt;anova&lt;/code&gt;
作用在&lt;code&gt;lme&lt;/code&gt;(&lt;code&gt;lme&lt;/code&gt;的输出对象）的输出结果很完整。当然这个函数在检测contrast的时候都不如SAS里面的 MIXED
procedure 方便。在SAS的MIXED
procedure里面，你只需要加入需要的contrast语句即可，但是在R厘米使用&lt;code&gt;lme&lt;/code&gt;和[] &lt;code&gt;lmer&lt;/code&gt;,
你都必须自己去计算contrast，并作检验。综合来说&lt;code&gt;lme&lt;/code&gt;比&lt;code&gt;lmer&lt;/code&gt;更好使用。
&lt;code&gt;~Device+(1|SET/Device)&lt;/code&gt; &lt;code&gt;/&lt;/code&gt;表示嵌套结构，也就是说Device的random
效应是nested在SET里面的。当然你也可以认为是SET和Device的交互效应。 注意到你也可以fit
&lt;code&gt;~Device+(Device|SET)&lt;/code&gt;，但是这个模型假设每个SET下的device里面有一个不同的随机效应。
这等价于假设了一个更加复杂的convariance matrix。这个假设和一般的关于covariance matrix的假设不一样。
至于那个更好，取决于模型的比较结果以及你对实际问题的理解。 当然，通常我们不会采用这个复杂的covariance
matrix假设。（....比较SAS里面的结果和R的结果，尤其是自由度，为什么一下子就变成
205了，当然，从用复杂的假设后可能使用了approximation，也就是严格的理论结果不存在了..） 在&lt;code&gt;lmer&lt;/code&gt;里面，
random effect
部分可以写为&lt;code&gt;~(1|SET/Device)&lt;/code&gt;或者更明白的形式&lt;code&gt;~(1|SET)+(1|SET:Device)))&lt;/code&gt;。
&lt;code&gt;~Day+(Day|Subject)&lt;/code&gt;我们想以Day为自变量（注意这里Day不是factor)fit一个线性模型，
但是对于每个subject来说，这个线性模型的intercept和斜率都增加了一个随机效应（注意如果没有这个随机效应
的话，那么所有的subject里面的线性模型都是一样的）。这有点类似于按某个factor(比如说性别)fit不同的
线性模型，但是区别是一个是Fixed
effect(比如说性别只有两种可能性)，而subject却有无数个（试验里面具体的subject只能被认为是一个群体里面
随机选取的）。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Function &lt;code&gt;median&lt;/code&gt; returns a vector of length one of two depending on
    whether the length of vector is odd or even. If the return result is
    a vector of 2, it might cause some bug while we can hardly notice it
    if all other vectors, matrices and arrays involved in the
    calculation with this result have even number of elements. So we
    have to be very careful about this function. My suggestion is that
    never use this function, instead, we can use &lt;code&gt;quantile(x,prob=0.5)&lt;/code&gt;
    to find the median of vector &lt;code&gt;x&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Function &lt;code&gt;summary&lt;/code&gt; can give some statistics about the data. But not
    as specific as SAS does.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;rstudent&lt;/code&gt; extracts studentized residuals of models from objects
    returned by modeling functions. However, it only works for some type
    of objects, e.g. &lt;code&gt;lm&lt;/code&gt; and &lt;code&gt;glm&lt;/code&gt;. It doesn't work for &lt;code&gt;lme&lt;/code&gt; objects.
    For &lt;code&gt;lme&lt;/code&gt; objects, you can use &lt;code&gt;residuals&lt;/code&gt; to extract model
    residuals. Using option &lt;code&gt;type="normalized"&lt;/code&gt;, you can extract
    studentized residuals of &lt;code&gt;lme&lt;/code&gt; objects.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;IQR&lt;/code&gt; calculate the interquartile range of a given data.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js','mhchem.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI"></category><category term="model"></category><category term="R"></category><category term="SAS"></category><category term="formula"></category><category term="statistics"></category></entry><entry><title>Make Your Model Training Reproducible in PyTorch</title><link href="https://misc.legendu.net/blog/make-your-model-training-reproducible-in-pytorch/" rel="alternate"></link><published>2020-03-16T14:34:57-07:00</published><updated>2020-04-16T14:34:57-07:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-03-16:/blog/make-your-model-training-reproducible-in-pytorch/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The PyTorch doc 
&lt;a href="https://pytorch.org/docs/stable/notes/randomness.html"&gt;Reproducibility&lt;/a&gt;
has very detailed instructions on how to make your model training reproducible.
Basically,
you need the following code.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;manual_seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed …&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The PyTorch doc 
&lt;a href="https://pytorch.org/docs/stable/notes/randomness.html"&gt;Reproducibility&lt;/a&gt;
has very detailed instructions on how to make your model training reproducible.
Basically,
you need the following code.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;manual_seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Notice that &lt;code&gt;torch.manual_seed&lt;/code&gt; set seeds of RNGs on all devices (both CPU and GPUs).
There is no need to make additional calls of &lt;code&gt;torch.cuda.manual_seed&lt;/code&gt;
or &lt;code&gt;torch.manual_seed_all&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://pytorch.org/docs/stable/notes/randomness.html"&gt;REPRODUCIBILITY&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pytorch.org/docs/stable/torch.html#torch.manual_seed"&gt;torch.manual_seed&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://discuss.pytorch.org/t/what-is-manual-seed/5939"&gt;What is manual_seed?&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://discuss.pytorch.org/t/difference-between-torch-manual-seed-and-torch-cuda-manual-seed/13848"&gt;Difference between torch.manual_seed and torch.cuda.manual_seed&lt;/a&gt;&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machcine learning"></category><category term="deep learning"></category><category term="PyTorch"></category><category term="reproducible"></category><category term="random"></category><category term="see"></category><category term="RNG"></category></entry><entry><title>Distance and Similarity for Machine Learning</title><link href="https://misc.legendu.net/blog/distance-and-similarity-for-machine-learning/" rel="alternate"></link><published>2019-12-17T11:46:20-08:00</published><updated>2020-03-17T11:46:20-07:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2019-12-17:/blog/distance-and-similarity-for-machine-learning/</id><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="cosine-similarity"&gt;Cosine Similarity&lt;/h2&gt;
&lt;h2 id="jaccard-index-jaccard-similarity-coefficient"&gt;&lt;a href="https://en.wikipedia.org/wiki/Jaccard_index"&gt;Jaccard Index (Jaccard Similarity Coefficient)&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="eucleadian-distance"&gt;Eucleadian Distance&lt;/h2&gt;
&lt;h2 id="l1-distance"&gt;L1 Distance&lt;/h2&gt;
&lt;h2 id="chebyshev-distance"&gt;&lt;a href="https://en.wikipedia.org/wiki/Chebyshev_distance"&gt;Chebyshev Distance&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/Cosine_similarity&lt;/p&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/Jaccard_index&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="similarity"></category><category term="distance"></category></entry><entry><title>Use XGBoost With Spark</title><link href="https://misc.legendu.net/blog/use-xgboost-with-spark/" rel="alternate"></link><published>2019-12-17T11:37:33-08:00</published><updated>2020-03-17T11:37:33-07:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2019-12-17:/blog/use-xgboost-with-spark/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The split-by-leaf mode (&lt;code&gt;grow_policy="lossguide"&lt;/code&gt;) is not supported in distributed training,
which makes XGBoost4J on Spark much slower than LightGBM on Spark.&lt;/p&gt;
&lt;h2 id="xgboost-with-spark"&gt;XGBoost with Spark&lt;/h2&gt;
&lt;p&gt;https://towardsdatascience.com/build-xgboost-lightgbm-models-on-large-datasets-what-are-the-possible-solutions-bf882da2c27d&lt;/p&gt;
&lt;p&gt;https://xgboost …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The split-by-leaf mode (&lt;code&gt;grow_policy="lossguide"&lt;/code&gt;) is not supported in distributed training,
which makes XGBoost4J on Spark much slower than LightGBM on Spark.&lt;/p&gt;
&lt;h2 id="xgboost-with-spark"&gt;XGBoost with Spark&lt;/h2&gt;
&lt;p&gt;https://towardsdatascience.com/build-xgboost-lightgbm-models-on-large-datasets-what-are-the-possible-solutions-bf882da2c27d&lt;/p&gt;
&lt;p&gt;https://xgboost.readthedocs.io/en/latest/jvm/xgboost4j_spark_tutorial.html&lt;/p&gt;
&lt;p&gt;https://xgboost.ai/2016/10/26/a-full-integration-of-xgboost-and-spark.html&lt;/p&gt;
&lt;p&gt;https://databricks.com/session/building-a-unified-data-pipeline-with-apache-spark-and-xgboost&lt;/p&gt;
&lt;p&gt;https://medium.com/cloudzone/xgboost-distributed-training-and-predicting-with-apache-spark-1127cdfb31ae&lt;/p&gt;
&lt;p&gt;https://news.developer.nvidia.com/gpu-accelerated-spark-xgboost/&lt;/p&gt;
&lt;p&gt;https://towardsdatascience.com/pyspark-and-xgboost-integration-tested-on-the-kaggle-titanic-dataset-4e75a568bdb&lt;/p&gt;
&lt;p&gt;https://www.kdnuggets.com/2016/03/xgboost-implementing-winningest-kaggle-algorithm-spark-flink.html&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="XGBoost"></category><category term="Spark"></category></entry><entry><title>Tips on the Transformers Python Library for NLP</title><link href="https://misc.legendu.net/blog/tips-on-the-transformers-python-library-for-nlp/" rel="alternate"></link><published>2020-03-06T13:19:56-08:00</published><updated>2020-03-06T13:19:56-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-03-06:/blog/tips-on-the-transformers-python-library-for-nlp/</id><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;https://github.com/huggingface/transformers#quick-tour&lt;/p&gt;
&lt;p&gt;https://github.com/huggingface/transformers&lt;/p&gt;
&lt;p&gt;https://huggingface.co/transformers/&lt;/p&gt;
&lt;p&gt;&lt;a href="https://huggingface.co/transformers/model_doc/bert.html"&gt;BERT&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://huggingface.co/transformers/model_doc/gpt2.html"&gt;GPT 2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://huggingface.co/transformers/model_doc/xlnet.html"&gt;XLNet&lt;/a&gt;&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="deep learning"></category><category term="NLP"></category><category term="transformers"></category><category term="library"></category><category term="BERT"></category><category term="XLNet"></category><category term="GPT-2"></category></entry><entry><title>Tokenization in NLP</title><link href="https://misc.legendu.net/blog/tokenization-in-nlp/" rel="alternate"></link><published>2020-03-06T12:07:36-08:00</published><updated>2020-03-06T12:07:36-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-03-06:/blog/tokenization-in-nlp/</id><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="libraries"&gt;Libraries&lt;/h2&gt;
&lt;h3 id="sentencepiece"&gt;&lt;a href="https://github.com/google/sentencepiece"&gt;SentencePiece&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/google/sentencepiece"&gt;SentencePiece&lt;/a&gt;
is an unsupervised text tokenizer for Neural Network-based text generation.&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="deep learning"></category><category term="NLP"></category><category term="tokenization"></category><category term="SentencePiece"></category></entry><entry><title>Subword Algorithms for NLP</title><link href="https://misc.legendu.net/blog/subword-algorithms-for-nlp/" rel="alternate"></link><published>2020-03-06T11:54:18-08:00</published><updated>2020-03-06T11:54:18-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-03-06:/blog/subword-algorithms-for-nlp/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Classic word representation cannot handle unseen word or rare word well. 
Character embeddings is one of the solution to overcome out-of-vocabulary (OOV). 
However, 
it may be too fine-grained and miss some …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Classic word representation cannot handle unseen word or rare word well. 
Character embeddings is one of the solution to overcome out-of-vocabulary (OOV). 
However, 
it may be too fine-grained and miss some important information. 
Subword is in between word and character. 
It is not too fine-grained while able to handle unseen word and rare word.&lt;/p&gt;
&lt;h2 id="general-tips"&gt;General Tips&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Subword balances vocabulary size and footprint. 
    Extreme case is we can only use 26 token (i.e. character) to present all English word. 
    16k or 32k subwords are recommended vocabulary size to have a good result.
    Many Asian language word cannot be separated by space. 
    Therefore, 
    the initial vocabulary is larger than English a lot. 
    You may need to prepare over 10k initial word to kick start the word segmentation. 
    From Schuster and Nakajima research, 
    they propose to use 22k word and 11k word for Japanese and Korean respectively.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="byte-pair-encoding-bpe"&gt;Byte Pair Encoding (BPE)&lt;/h2&gt;
&lt;p&gt;Sennrich et al. (2016) proposed to use Byte Pair Encoding (BPE) to build subword dictionary. 
Radfor et al adopt BPE to construct subword vector to build GPT-2 in 2019.&lt;/p&gt;
&lt;h2 id="wordpiece"&gt;WordPiece&lt;/h2&gt;
&lt;p&gt;WordPiece is another word segmentation algorithm and it is similar with BPE. 
Schuster and Nakajima introduced WordPiece by solving Japanese and Korea voice problem in 2012. 
Basically, 
WordPiece is similar with BPE and the difference part is forming a new subword by likelihood but not the next highest frequency pair.&lt;/p&gt;
&lt;h2 id="unigram-language-model"&gt;Unigram Language Model&lt;/h2&gt;
&lt;p&gt;Kudo. introduced unigram language model as another algorithm for subword segmentation. One of the assumption is all subword occurrence are independently and subword sequence is produced by the product of subword occurrence probabilities. Both WordPiece and Unigram Language Model leverages languages model to build subword vocabulary.&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://medium.com/@makcedward/how-subword-helps-on-your-nlp-model-83dd1b836f46"&gt;3 subword algorithms help to improve your NLP model performance&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/google/sentencepiece"&gt;SentencePiece&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/pdf/1808.06226.pdf"&gt;SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing&lt;/a&gt;&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="deep learning"></category><category term="NLP"></category><category term="subword algorithm"></category><category term="BPE"></category><category term="WordPiece"></category><category term="Unigram Language Model"></category></entry><entry><title>Terminologies and Concepts in NLP</title><link href="https://misc.legendu.net/blog/terminologies-concepts-in-nlp/" rel="alternate"></link><published>2020-03-06T11:29:11-08:00</published><updated>2020-03-06T11:29:11-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-03-06:/blog/terminologies-concepts-in-nlp/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Word Embedding
Character Embedding
Subword Embeddling
Tokenization&lt;/p&gt;
&lt;p&gt;General Language Understanding Evaluation (GLUE)&lt;/p&gt;
&lt;p&gt;Natural Language Generation (NLG)
Natural Language Generation, as defined by Artificial Intelligence: Natural Language Processing Fundamentals, is the “process …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Word Embedding
Character Embedding
Subword Embeddling
Tokenization&lt;/p&gt;
&lt;p&gt;General Language Understanding Evaluation (GLUE)&lt;/p&gt;
&lt;p&gt;Natural Language Generation (NLG)
Natural Language Generation, as defined by Artificial Intelligence: Natural Language Processing Fundamentals, is the “process of producing meaningful phrases and sentences in the form of natural language.” In its essence, it automatically generates narratives that describe, summarize or explain input structured data in a human-like manner at the speed of thousands of pages per second.&lt;/p&gt;
&lt;p&gt;Multi-Genre Natural Language Inference (MultiNLI)&lt;/p&gt;
&lt;p&gt;Named Entity Recognition (NER) &lt;/p&gt;
&lt;p&gt;Heuristic Analysis for NLI Systems (HANS)&lt;/p&gt;
&lt;h2 id="data-related"&gt;Data Related&lt;/h2&gt;
&lt;p&gt;The Stanford Question Answering Dataset (SQuAD)&lt;/p&gt;
&lt;p&gt;Situations With Adversarial Generations (SWAG)&lt;/p&gt;
&lt;p&gt;Reading Comprehension Dataset (RACE) &lt;/p&gt;
&lt;p&gt;The Cross-Lingual NLI Corpus (XNLI)&lt;/p&gt;
&lt;h2 id="model-related"&gt;Model Related&lt;/h2&gt;
&lt;p&gt;Generative Pre-Training (GPT)&lt;/p&gt;
&lt;h3 id="cross-lingual-language-models-xlm"&gt;Cross-lingual Language Models (XLM)&lt;/h3&gt;
&lt;p&gt;Cross-lingual Natural Language Inference (XNLI)&lt;/p&gt;
&lt;h3 id="causal-language-modeling-clm"&gt;Causal Language Modeling (CLM)&lt;/h3&gt;
&lt;p&gt;CLM consists of a Transformer to learn text representation by providing a set of previous features. 
Given the previous hidden state to the current batch, the model predicts the next word.&lt;/p&gt;
&lt;h3 id="masked-language-modeling-mlm"&gt;Masked Language Modeling (MLM)&lt;/h3&gt;
&lt;p&gt;Lample and Connea follow Devlin et al. (2018) approach to pick 15% of subword randomly 
and replacing it by reserved word ([MASK]) 80% of the time, 
by a random word 10% of the time 
and remaining unchanged 10% of the time.
The differences between Devlin et al. (2018) are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using an arbitrary number of sentences but not pairs of sentences only&lt;/li&gt;
&lt;li&gt;Subsample high-frequency subword&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="translation-language-modeling-tlm"&gt;Translation Language Modeling (TLM)&lt;/h3&gt;
&lt;p&gt;CLM and MLM are designed for monolingual data while TLM targets on cross-lingual data. BERT use segment embeddings to represent different sentence in a single sequence of input while replace it by language embeddings to represent different language.
Subwords are randomly picked in both language data. Both language subword can be leveraged to predict any MASK word.&lt;/p&gt;
&lt;h3 id="out-of-vocabulary-oov"&gt;Out-of-Vocabulary (OOV)&lt;/h3&gt;
&lt;h3 id="double-dipping-in-machine-learning"&gt;&lt;a href="https://www.legendu.net/misc/blog/double-dipping-in-machine-learning"&gt;Double Dipping in Machine Learning&lt;/a&gt;&lt;/h3&gt;
&lt;h3 id="byte-pair-encoding-bpe-subword-algorithm"&gt;Byte Pair Encoding (BPE) Subword Algorithm&lt;/h3&gt;
&lt;h2 id="take-away"&gt;Take Away&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;BERT use segment embeddings (represent different sentence) while XLM use language embeddings (represent different language).&lt;/li&gt;
&lt;li&gt;CLM does not scale to a cross-lingual scenario.&lt;/li&gt;
&lt;li&gt;XLM may not fit for low resource language as if required parallel data (TML) to boost up the performance. Meanwhile, Multilingual Neural Language Models are designed to overcome this limitation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://medium.com/towards-artificial-intelligence/cross-lingual-language-model-56a65dba9358"&gt;Cross-lingual Language Model&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://medium.com/sciforce/a-comprehensive-guide-to-natural-language-generation-dd63a4b6e548"&gt;A Comprehensive Guide to Natural Language Generation&lt;/a&gt;&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="deep learning"></category><category term="NLP"></category></entry><entry><title>Difference Between forward and __call__ Methods of a Module in PyTorch</title><link href="https://misc.legendu.net/blog/difference-between-forward-and-__call__-methods-of-a-module-in-pytorch/" rel="alternate"></link><published>2020-03-05T16:05:30-08:00</published><updated>2020-03-05T16:05:30-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-03-05:/blog/difference-between-forward-and-__call__-methods-of-a-module-in-pytorch/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The &lt;code&gt;Module.__call__&lt;/code&gt; method register all hooks and call the method &lt;code&gt;Module.forward&lt;/code&gt;. 
    In short, 
    when you train the model you should use the method &lt;code&gt;forward&lt;/code&gt;,
    while when you test the …&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The &lt;code&gt;Module.__call__&lt;/code&gt; method register all hooks and call the method &lt;code&gt;Module.forward&lt;/code&gt;. 
    In short, 
    when you train the model you should use the method &lt;code&gt;forward&lt;/code&gt;,
    while when you test the model during training 
    or when you do prediction using a well trained model, 
    you should use the method &lt;code&gt;__call__&lt;/code&gt;. 
    &lt;code&gt;model.__call__(data)&lt;/code&gt; is equivalent to &lt;code&gt;model(data)&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://stackoverflow.com/questions/55338756/why-there-are-different-output-between-model-forwardinput-and-modelinput"&gt;Why there are different output between model.forward(input) and model(input)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://discuss.pytorch.org/t/is-model-forward-x-the-same-as-model-call-x/33460"&gt;Is model.forward(x) the same as model.&lt;strong&gt;call&lt;/strong&gt;(x)?&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://discuss.pytorch.org/t/why-can-you-call-model-without-specifying-forward-method/24762"&gt;Why can you call model without specifying forward method&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://discuss.pytorch.org/t/any-different-between-model-input-and-model-forward-input/3690"&gt;Any different between model(input) and model.forward(input)&lt;/a&gt;&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="deep learning"></category><category term="PyTorch"></category><category term="forward"></category><category term="__call__"></category></entry><entry><title>Representation of Machine Learning Models</title><link href="https://misc.legendu.net/blog/representation-of-machine-learning-models/" rel="alternate"></link><published>2020-01-05T15:32:24-08:00</published><updated>2020-03-05T15:32:24-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-01-05:/blog/representation-of-machine-learning-models/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="open-neural-network-exchange-onnx"&gt;&lt;a href="https://onnx.ai/"&gt;Open Neural Network Exchange (ONNX)&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;https://github.com/onnx&lt;/p&gt;
&lt;p&gt;https://github.com/onnx/onnxmltools&lt;/p&gt;
&lt;h2 id="predictive-model-markup-language-pmml"&gt;&lt;a href="http://dmg.org/pmml/v4-4/GeneralStructure.html"&gt;Predictive Model Markup Language (PMML)&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;https://github.com/jpmml/sklearn2pmml&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/autodeployai/pmml4s"&gt;PMML4S&lt;/a&gt;
is a PMML (Predictive Model Markup …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="open-neural-network-exchange-onnx"&gt;&lt;a href="https://onnx.ai/"&gt;Open Neural Network Exchange (ONNX)&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;https://github.com/onnx&lt;/p&gt;
&lt;p&gt;https://github.com/onnx/onnxmltools&lt;/p&gt;
&lt;h2 id="predictive-model-markup-language-pmml"&gt;&lt;a href="http://dmg.org/pmml/v4-4/GeneralStructure.html"&gt;Predictive Model Markup Language (PMML)&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;https://github.com/jpmml/sklearn2pmml&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/autodeployai/pmml4s"&gt;PMML4S&lt;/a&gt;
is a PMML (Predictive Model Markup Language) scoring library for Scala. It provides both Scala and Java Evaluator API for PMML.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/jpmml/jpmml-evaluator"&gt;JPMML-Evaluator&lt;/a&gt;
is a Java evaluator API for PMML.&lt;/p&gt;
&lt;h2 id="plain-old-java-object-pojo"&gt;[Plain Old Java Object (POJO)]&lt;/h2&gt;
&lt;p&gt;H2O supports exporting models into POJO (JAR) files.&lt;/p&gt;
&lt;h2 id="mleap"&gt;&lt;a href="https://mleap-docs.combust.ml/"&gt;MLeap&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="pickle"&gt;Pickle&lt;/h2&gt;
&lt;p&gt;Some Python libraries support serializaing a model into pickle format.&lt;/p&gt;
&lt;h2 id="save-and-load-models-in-pytorch"&gt;Save and Load Models in PyTorch&lt;/h2&gt;
&lt;p&gt;https://pytorch.org/docs/stable/notes/serialization.html#recommend-saving-models&lt;/p&gt;
&lt;h2 id="pytorch-onnx-tensorflow"&gt;PyTorch -&amp;gt; ONNX -&amp;gt; TensorFlow&lt;/h2&gt;
&lt;p&gt;https://github.com/onnx/onnx-tensorflow/issues/491&lt;/p&gt;
&lt;p&gt;https://towardsdatascience.com/converting-a-simple-deep-learning-model-from-pytorch-to-tensorflow-b6b353351f5d&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="serialization"></category><category term="model representation"></category></entry><entry><title>Voice Recognition</title><link href="https://misc.legendu.net/blog/voice-recognition/" rel="alternate"></link><published>2020-03-04T08:08:46-08:00</published><updated>2020-03-04T08:08:46-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-03-04:/blog/voice-recognition/</id><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://voice.mozilla.org/en"&gt;Voice Web&lt;/a&gt;&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="deep learning"></category><category term="machine learning"></category><category term="voice"></category><category term="recognition"></category><category term="speech"></category></entry><entry><title>Popular and Useful Modules and Functions in PyTorch</title><link href="https://misc.legendu.net/blog/popular-and-useful-modules-and-functions-in-pytorch/" rel="alternate"></link><published>2020-02-03T10:08:35-08:00</published><updated>2020-03-03T10:08:35-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-02-03:/blog/popular-and-useful-modules-and-functions-in-pytorch/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="functions"&gt;Functions&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://pytorch.org/docs/stable/torch.html#torch.load"&gt;torch.load&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pytorch.org/docs/master/torch.html#torch.from_numpy"&gt;torch.from_numpy&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pytorch.org/docs/master/torch.html#torch.tensor"&gt;torch.tensor&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pytorch.org/docs/master/torch.html#torch.as_tensor"&gt;torch.as_tensor&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pytorch.org/docs/stable/torch.html#torch.flatten"&gt;torch.flatten&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pytorch.org/docs/stable/torch.html#torch.numel"&gt;torch.numel&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split"&gt;torch.utils.data.random_split&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pytorch.org/docs/stable/nn.functional.html#relu"&gt;torch.nn.functional.relu&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pytorch.org/docs/stable/nn.functional.html#leaky-relu"&gt;torch.nn.functional.leaky_relu&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pytorch.org/docs/stable/nn.functional.html#gelu"&gt;torch.nn.functional.gelu&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pytorch.org/docs/stable/nn.functional.html#logsigmoid"&gt;torch.nn …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="functions"&gt;Functions&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://pytorch.org/docs/stable/torch.html#torch.load"&gt;torch.load&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pytorch.org/docs/master/torch.html#torch.from_numpy"&gt;torch.from_numpy&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pytorch.org/docs/master/torch.html#torch.tensor"&gt;torch.tensor&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pytorch.org/docs/master/torch.html#torch.as_tensor"&gt;torch.as_tensor&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pytorch.org/docs/stable/torch.html#torch.flatten"&gt;torch.flatten&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pytorch.org/docs/stable/torch.html#torch.numel"&gt;torch.numel&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split"&gt;torch.utils.data.random_split&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pytorch.org/docs/stable/nn.functional.html#relu"&gt;torch.nn.functional.relu&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pytorch.org/docs/stable/nn.functional.html#leaky-relu"&gt;torch.nn.functional.leaky_relu&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pytorch.org/docs/stable/nn.functional.html#gelu"&gt;torch.nn.functional.gelu&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pytorch.org/docs/stable/nn.functional.html#logsigmoid"&gt;torch.nn.functional.logsigmoid&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pytorch.org/docs/stable/nn.functional.html#softmin"&gt;torch.nn.functional.softmin&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pytorch.org/docs/stable/nn.functional.html#softmax"&gt;torch.nn.functional.softmax&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pytorch.org/docs/stable/nn.functional.html#log-softmax"&gt;torch.nn.functional.log_softmax&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pytorch.org/docs/stable/nn.functional.html#sigmoid"&gt;torch.nn.functional.sigmoid&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pytorch.org/docs/stable/nn.functional.html#batch-norm"&gt;torch.nn.functional.batch_norm&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pytorch.org/docs/stable/nn.functional.html#normalize"&gt;torch.nn.functional.normalize&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="modules"&gt;Modules&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/docs/stable/nn.html#sequential"&gt;torch.nn.Sequential&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/docs/stable/nn.html#conv2d"&gt;torch.nn.Conv2d&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/docs/stable/nn.html#dropout2d"&gt;torch.nn.Dropout2d&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/docs/stable/nn.html#linear"&gt;torch.nn.Linear&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="transforms"&gt;Transforms&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.ToTensor"&gt;torchvison.transforms.ToTensor&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Normalize"&gt;torchvision.transforms.Normalize&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Compose"&gt;torchvision.transforms.Compose&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="referneces"&gt;Referneces&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://stackoverflow.com/questions/53419474/using-dropout-in-pytorch-nn-dropout-vs-f-dropout"&gt;Using Dropout in Pytorch: nn.Dropout vs. F.dropout&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://discuss.pytorch.org/t/how-does-transforms-totensor-work-and-computation-of-mean-and-std-values/9085"&gt;How does transforms.ToTensor() work and computation of mean and std values&lt;/a&gt;&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="deep learning"></category><category term="PyTorch"></category></entry><entry><title>Number Precision in Deep Learning</title><link href="https://misc.legendu.net/blog/number-precision-in-deep-learning/" rel="alternate"></link><published>2020-03-02T21:35:55-08:00</published><updated>2020-03-02T21:35:55-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-03-02:/blog/number-precision-in-deep-learning/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;https://www.mathworks.com/company/newsletters/articles/what-is-int8-quantization-and-why-is-it-popular-for-deep-neural-networks.html&lt;/p&gt;
&lt;p&gt;https://engineering.fb.com/ai-research/floating-point-math/&lt;/p&gt;
&lt;p&gt;&lt;a href="https://research.fb.com/publications/rethinking-floating-point-for-deep-learning/"&gt;Rethinking floating point for deep learning&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://papers.nips.cc/paper/7994-training-deep-neural-networks-with-8-bit-floating-point-numbers.pdf"&gt;Training Deep Neural Networks with 8-bit Floating Point Numbers&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://heartbeat.fritz.ai/8-bit-quantization-and-tensorflow-lite-speeding-up-mobile-inference-with-low-precision-a882dfcafbbd"&gt;8-Bit …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;https://www.mathworks.com/company/newsletters/articles/what-is-int8-quantization-and-why-is-it-popular-for-deep-neural-networks.html&lt;/p&gt;
&lt;p&gt;https://engineering.fb.com/ai-research/floating-point-math/&lt;/p&gt;
&lt;p&gt;&lt;a href="https://research.fb.com/publications/rethinking-floating-point-for-deep-learning/"&gt;Rethinking floating point for deep learning&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://papers.nips.cc/paper/7994-training-deep-neural-networks-with-8-bit-floating-point-numbers.pdf"&gt;Training Deep Neural Networks with 8-bit Floating Point Numbers&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://heartbeat.fritz.ai/8-bit-quantization-and-tensorflow-lite-speeding-up-mobile-inference-with-low-precision-a882dfcafbbd"&gt;8-Bit Quantization and TensorFlow Lite: Speeding up mobile inference with low precision&lt;/a&gt;&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="deep learning"></category><category term="number precision"></category><category term="float"></category><category term="int8"></category></entry><entry><title>Tips on TVM</title><link href="https://misc.legendu.net/blog/tips-on-tvm/" rel="alternate"></link><published>2020-03-02T18:30:38-08:00</published><updated>2020-03-02T18:30:38-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-03-02:/blog/tips-on-tvm/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;
TVM is an open deep learning compiler stack for CPUs, GPUs, and specialized accelerators. 
It aims to close the gap between the productivity-focused deep learning frameworks, 
and the performance- or efficiency-oriented …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;
TVM is an open deep learning compiler stack for CPUs, GPUs, and specialized accelerators. 
It aims to close the gap between the productivity-focused deep learning frameworks, 
and the performance- or efficiency-oriented hardware backends. TVM provides the following main features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compilation of deep learning models in Keras, MXNet, PyTorch, Tensorflow, CoreML, DarkNet into minimum deployable modules on diverse hardware backends.&lt;/li&gt;
&lt;li&gt;Infrastructure to automatic generate and optimize tensor operators on more backend with better performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In short, 
TVM to deep learning is kind of like LLVM to programming languages.&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;https://github.com/apache/incubator-tvm&lt;/p&gt;
&lt;p&gt;https://discuss.tvm.ai/&lt;/p&gt;
&lt;p&gt;https://tvm.apache.org/blog&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="deep learning"></category><category term="TVM"></category><category term="tips"></category></entry><entry><title>Public Cloud Offering GPU Support</title><link href="https://misc.legendu.net/blog/public-cloud-offering-gpu-support/" rel="alternate"></link><published>2020-01-02T18:01:37-08:00</published><updated>2020-03-02T18:01:37-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-01-02:/blog/public-cloud-offering-gpu-support/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;https://lambdalabs.com/&lt;/p&gt;
&lt;p&gt;https://www.floydhub.com/&lt;/p&gt;
&lt;p&gt;https://colab.research.google.com/notebooks/intro.ipynb#recent=true&lt;/p&gt;
&lt;h2 id="amazon-aws"&gt;Amazon AWS&lt;/h2&gt;
&lt;p&gt;Below is a list of instances that have 1 GPU on Amazon …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;https://lambdalabs.com/&lt;/p&gt;
&lt;p&gt;https://www.floydhub.com/&lt;/p&gt;
&lt;p&gt;https://colab.research.google.com/notebooks/intro.ipynb#recent=true&lt;/p&gt;
&lt;h2 id="amazon-aws"&gt;Amazon AWS&lt;/h2&gt;
&lt;p&gt;Below is a list of instances that have 1 GPU on Amazon AWS.
g3s.xlarge and g4dn.xlarge (&lt;strong&gt;recommended&lt;/strong&gt;) are good ones for occasional use.
&lt;img alt="AWS EC2 Instances" src="https://user-images.githubusercontent.com/824507/73386836-607ad380-4284-11ea-862a-d04a19b98ee2.png"&gt;&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="GPU"></category><category term="machine learning"></category><category term="data science"></category><category term="programming"></category><category term="cloud"></category><category term="host"></category></entry><entry><title>Train PyTorch Distributedly Using Apache Ray</title><link href="https://misc.legendu.net/blog/train-pytorch-distributedly-using-apache-ray/" rel="alternate"></link><published>2020-02-01T11:36:44-08:00</published><updated>2020-03-01T11:36:44-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-02-01:/blog/train-pytorch-distributedly-using-apache-ray/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="training-a-model-implemented-in-pytorch"&gt;Training a Model Implemented in PyTorch&lt;/h2&gt;
&lt;p&gt;https://github.com/ray-project/ray/tree/master/python/ray/util/sgd/pytorch/examples&lt;/p&gt;
&lt;p&gt;&lt;a href="https://ray.readthedocs.io/en/latest/raysgd/raysgd_pytorch.html"&gt;Distributed PyTorch Using Apache Ray&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://ray.readthedocs.io/en/latest/raysgd/raysgd.html"&gt;RaySGD: Distributed Training Wrappers&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="hyperparameter-optimization-for-models-implemented-in-pytorch"&gt;Hyperparameter Optimization for …&lt;/h2&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="training-a-model-implemented-in-pytorch"&gt;Training a Model Implemented in PyTorch&lt;/h2&gt;
&lt;p&gt;https://github.com/ray-project/ray/tree/master/python/ray/util/sgd/pytorch/examples&lt;/p&gt;
&lt;p&gt;&lt;a href="https://ray.readthedocs.io/en/latest/raysgd/raysgd_pytorch.html"&gt;Distributed PyTorch Using Apache Ray&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://ray.readthedocs.io/en/latest/raysgd/raysgd.html"&gt;RaySGD: Distributed Training Wrappers&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="hyperparameter-optimization-for-models-implemented-in-pytorch"&gt;Hyperparameter Optimization for Models Implemented in PyTorch&lt;/h2&gt;
&lt;p&gt;https://ray.readthedocs.io/en/latest/tune-examples.html&lt;/p&gt;
&lt;p&gt;Is the following example running distributed or not?
Do I need to use tags to tell Ray to run it on multiple machines?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch.optim&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;optim&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;ray&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;tune&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;ray.tune.examples.mnist_pytorch&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;get_data_loaders&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ConvNet&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;train_mnist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;train_loader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_loader&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;get_data_loaders&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ConvNet&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;optim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SGD&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;lr&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_loader&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;acc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_loader&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;tune&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;track&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean_accuracy&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;acc&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="n"&gt;analysis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tune&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;train_mnist&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;lr&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tune&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grid_search&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;0.001&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;])})&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Best config: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;analysis&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_best_config&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;metric&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;mean_accuracy&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# Get a dataframe for analyzing trial results.&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;analysis&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dataframe&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;data parallelism vs model parallelism  &lt;/li&gt;
&lt;li&gt;use Ring Allreduce (RA) (instead of Parameter Server or Peer to Peer) 
    for synchronization among processes (CPU/GPU on the same node or different nodes)&lt;/li&gt;
&lt;li&gt;Distributed Optimization Algorithm&lt;ul&gt;
&lt;li&gt;synchronized SGD &lt;/li&gt;
&lt;li&gt;asynchronized SGD &lt;/li&gt;
&lt;li&gt;1-bit SGD&lt;/li&gt;
&lt;li&gt;The Hogwild algorithm&lt;/li&gt;
&lt;li&gt;Downpour SGD&lt;/li&gt;
&lt;li&gt;synchronized SGD + large minibatch to reduce update frequency of parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://stanford.edu/~rezab/classes/cme323/S16/projects_reports/hedge_usmani.pdf"&gt;Parallel and Distributed Deep Learning&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://cse.buffalo.edu/~demirbas/publications/DistMLplat.pdf"&gt;A Comparison of Distributed Machine Learning Platforms&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/pdf/1909.02061.pdf"&gt;Performance Analysis and Comparison of Distributed Machine Learning Systems&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://discuss.pytorch.org/t/multiprocessing-failed-with-torch-distributed-launch-module/33056"&gt;Multiprocessing failed with Torch.distributed.launch module&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;https://jdhao.github.io/2019/11/01/pytorch_distributed_training/&lt;/p&gt;
&lt;p&gt;&lt;a href="https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255"&gt;Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU &amp;amp; Distributed setups&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pytorch.org/docs/stable/distributed.html"&gt;DISTRIBUTED COMMUNICATION PACKAGE - TORCH.DISTRIBUTED&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pytorch.org/docs/master/nn.html#torch.nn.parallel.DistributedDataParallel"&gt;DistributedDataParallel&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://yangkky.github.io/2019/07/08/distributed-pytorch-tutorial.html"&gt;Distributed data parallel training in Pytorch&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://towardsdatascience.com/visual-intuition-on-ring-allreduce-for-distributed-deep-learning-d1f34b4911da"&gt;Visual intuition on ring-Allreduce for distributed Deep Learning&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://tech.preferred.jp/en/blog/technologies-behind-distributed-deep-learning-allreduce/"&gt;Technologies behind Distributed Deep Learning: AllReduce&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://seba1511.net/tutorials/intermediate/dist_tuto.html"&gt;Writing Distributed Applications with PyTorch&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;https://github.com/ray-project/ray/issues/3609&lt;/p&gt;
&lt;p&gt;https://github.com/ray-project/ray/issues/3520&lt;/p&gt;
&lt;p&gt;&lt;a href="https://towardsdatascience.com/accelerating-deep-learning-using-distributed-sgd-an-overview-e66c4aee1a0c"&gt;Accelerating Deep Learning Using Distributed SGD — An Overview&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://medium.com/intel-student-ambassadors/distributed-training-of-deep-learning-models-with-pytorch-1123fa538848"&gt;Distributed training of Deep Learning models with PyTorch&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4465"&gt;Scalable Distributed DL Training: Batching Communication and Computation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;https://github.com/dmmiller612/sparktorch&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/bharathgs/Awesome-Distributed-Deep-Learning"&gt;Awesome Distributed Deep Learning&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://medium.com/@Petuum/intro-to-distributed-deep-learning-systems-a2e45c6b8e7"&gt;Intro to Distributed Deep Learning Systems&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/pdf/1706.02677.pdf"&gt;Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour&lt;/a&gt;&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="deep learning"></category><category term="PyTorch"></category><category term="distributed"></category><category term="Apache Ray"></category></entry><entry><title>Tips on AutoGluon</title><link href="https://misc.legendu.net/blog/tips-on-autogluon/" rel="alternate"></link><published>2020-01-27T15:28:00-08:00</published><updated>2020-02-27T15:28:00-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-01-27:/blog/tips-on-autogluon/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;https://github.com/awslabs/autogluon&lt;/p&gt;
&lt;p&gt;https://autogluon.mxnet.io/&lt;/p&gt;
&lt;p&gt;AutoGluon automatically inferences the problem type. 
However, 
you are still able to specify the probelm type 
if AutoGluon fails to infer the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;https://github.com/awslabs/autogluon&lt;/p&gt;
&lt;p&gt;https://autogluon.mxnet.io/&lt;/p&gt;
&lt;p&gt;AutoGluon automatically inferences the problem type. 
However, 
you are still able to specify the probelm type 
if AutoGluon fails to infer the right problem type.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;AutoGluon auto saves all models into the specified output directory during training.
    And the save models can be load back using the &lt;code&gt;load&lt;/code&gt; method of the corresponding predictor.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GPU support in AutoGluon is for image/text but not Tabular data currently.
    For more details,
    please refer to
    &lt;a href="https://github.com/awslabs/autogluon/issues/262"&gt;issue 262&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="questions"&gt;Questions&lt;/h2&gt;
&lt;p&gt;Can I choose a model to save and choose a model to load?&lt;/p&gt;
&lt;h2 id="customization"&gt;Customization&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://autogluon.mxnet.io/tutorials/course/core.html"&gt;Search Space and Decorator&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://autogluon.mxnet.io/tutorials/course/algorithm.html"&gt;Search Algorithms&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://autogluon.mxnet.io/tutorials/course/object.html"&gt;Customize User Objects&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://autogluon.mxnet.io/tutorials/course/script.html"&gt;Customize Training Script&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="hyperparameter-tuning"&gt;Hyperparameter Tuning&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://autogluon.mxnet.io/tutorials/torch/hpo.html"&gt;Use AutoGluon for Hyperparameter Optimization for MNIST Training in PyTorch&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="neural-architecture-search"&gt;Neural Architecture Search&lt;/h2&gt;
&lt;p&gt;https://autogluon.mxnet.io/tutorials/nas/enas_mnist.html&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;https://futurumresearch.com/aws-releases-autogluon-an-innovative-open-source-tooling-for-automated-machine-learning/&lt;/p&gt;
&lt;p&gt;https://www.amazon.science/amazons-autogluon-helps-developers-get-up-and-running-with-state-of-the-art-deep-learning-models-with-just-a-few-lines-of-code&lt;/p&gt;
&lt;p&gt;https://towardsdatascience.com/autogluon-deep-learning-automl-5cdb4e2388ec&lt;/p&gt;
&lt;p&gt;https://venturebeat.com/2020/01/09/amazons-autogluon-produces-ai-models-with-as-little-as-three-lines-of-code/&lt;/p&gt;
&lt;p&gt;https://autogluon.mxnet.io/tutorials/course/distributed.html&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="AutoGluon"></category><category term="AutoML"></category><category term="deep learning"></category><category term="ENAS"></category><category term="ProxylessNAS"></category><category term="gluon"></category><category term="MXNet"></category></entry><entry><title>Resizing and Padding for Image Recognition</title><link href="https://misc.legendu.net/blog/resizing-and-padding-for-image-recognition/" rel="alternate"></link><published>2020-02-25T14:11:09-08:00</published><updated>2020-02-25T14:11:09-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-02-25:/blog/resizing-and-padding-for-image-recognition/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The best way to deal with different sized images 
    is to downscale them to match dimensions from the smallest image available.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://stackoverflow.com/questions/47697622/cnn-image-resizing-vs-padding-keeping-aspect-ratio-or-not/49882055#49882055"&gt;CNN - Image Resizing VS Padding (keeping aspect ratio or …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The best way to deal with different sized images 
    is to downscale them to match dimensions from the smallest image available.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://stackoverflow.com/questions/47697622/cnn-image-resizing-vs-padding-keeping-aspect-ratio-or-not/49882055#49882055"&gt;CNN - Image Resizing VS Padding (keeping aspect ratio or not?)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://towardsdatascience.com/boost-your-cnn-image-classifier-performance-with-progressive-resizing-in-keras-a7d96da06e20"&gt;Boost your CNN image classifier performance with progressive resizing in Keras&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://medium.com/neuronio/how-to-deal-with-image-resizing-in-deep-learning-e5177fad7d89"&gt;How to deal with image resizing in Deep Learning&lt;/a&gt;&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="machine learning"></category><category term="data science"></category><category term="deep learning"></category></entry><entry><title>Convolutional Neural Networks</title><link href="https://misc.legendu.net/blog/convolutional-neural-networks/" rel="alternate"></link><published>2020-01-24T22:24:05-08:00</published><updated>2020-02-24T22:24:05-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-01-24:/blog/convolutional-neural-networks/</id><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://cs231n.github.io/convolutional-networks/"&gt;CS231n: Convolutional Neural Networks for Visual Recognition&lt;/a&gt;
is a great introduction of CNN.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://cs231n.github.io/convolutional-networks/#convnet-architectures"&gt;ConvNet Architectures&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;http://vision.stanford.edu/teaching/cs231n/&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="CNN"></category><category term="convolutional neural network"></category></entry><entry><title>Handle Imbalanced Data in Machine Learning</title><link href="https://misc.legendu.net/blog/handle-imbalanced-data-in-machine-learning/" rel="alternate"></link><published>2020-01-18T10:40:49-08:00</published><updated>2020-02-18T10:40:49-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-01-18:/blog/handle-imbalanced-data-in-machine-learning/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/scikit-learn-contrib/imbalanced-learn"&gt;scikit-learn-contrib/imbalanced-learn&lt;/a&gt;
is a Python Package to Tackle the Curse of Imbalanced Datasets in Machine Learning.&lt;/p&gt;
&lt;h2 id="type-of-imbalanced-data"&gt;Type of Imbalanced Data&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Intrinsic (imbalance is a direct result of the nature of the …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/scikit-learn-contrib/imbalanced-learn"&gt;scikit-learn-contrib/imbalanced-learn&lt;/a&gt;
is a Python Package to Tackle the Curse of Imbalanced Datasets in Machine Learning.&lt;/p&gt;
&lt;h2 id="type-of-imbalanced-data"&gt;Type of Imbalanced Data&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Intrinsic (imbalance is a direct result of the nature of the dataspace)&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Extrinsic (due to time and/or storage, etc.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Between-class Imbalance&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Relative imbalance (OOM)&lt;/li&gt;
&lt;li&gt;Rare instances a.k.a. absolute rarity (pink blood patient)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Within-class Imbalance&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Data complexity (primary)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Overlapping&lt;/li&gt;
&lt;li&gt;Lack of representative data&lt;/li&gt;
&lt;li&gt;Small disjuncts&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Imbalanced&lt;/li&gt;
&lt;li&gt;Small sample size&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="impact-of-imbalanced-data-on-decision-tree"&gt;Impact of Imbalanced Data on Decision Tree&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Fewer and fewer observations of minority class examples
    resulting in fewer leaves describing minority concepts and successively weaker confidences estimates&lt;/li&gt;
&lt;li&gt;Concepts that have dependencies on different feature space conjunctions 
    can go unlearned by the sparseness introduced through partitionining&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="evaluation"&gt;Evaluation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;don't use accuracy (or error rate)&lt;/li&gt;
&lt;li&gt;use ROC, PR curve, F1 score, etc.&lt;/li&gt;
&lt;li&gt;don't get hard classifications&lt;/li&gt;
&lt;li&gt;get probability estimates&lt;/li&gt;
&lt;li&gt;don't use a 0.5 decision threshold blindly&lt;/li&gt;
&lt;li&gt;check performance curves&lt;/li&gt;
&lt;li&gt;test on data to operate on&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="ways-to-handle-imbalanced-data"&gt;Ways to Handle Imbalanced Data&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Do nothing&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Balance the training set
    Oversampling: tied data leading to overfitting
    Undersampling: miss important concepts 
    overall undersampling is preferred if there are enough data.
    However, oversampling might be better if you have very small data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Border based approach&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Sampling with Data Cleaning&lt;/li&gt;
&lt;li&gt;Adjust algorithms&lt;/li&gt;
&lt;li&gt;Cluster-based Sampling&lt;/li&gt;
&lt;li&gt;Sampling + Boosting&lt;/li&gt;
&lt;li&gt;New algorithms&lt;/li&gt;
&lt;li&gt;Anomaly detection&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="undersampling"&gt;Undersampling&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;EasyEnsemble (recommended)&lt;/li&gt;
&lt;li&gt;BalanceCascade&lt;/li&gt;
&lt;li&gt;KNN based (NearMiss-1, NearMiss-2, Near-Miss-3, Most Distant)&lt;/li&gt;
&lt;li&gt;One-sided selection (OSS)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="border-based-approaches"&gt;Border-based Approaches&lt;/h2&gt;
&lt;h3 id="tomek-links"&gt;Tomek Links&lt;/h3&gt;
&lt;p&gt;A pair of minimally distanced nearest neighbors of opposite classes. 
 Remove the majority instance of Tomek Links. 
  Makes the border more clear&lt;/p&gt;
&lt;h3 id="smote"&gt;SMOTE&lt;/h3&gt;
&lt;p&gt;Synthetic Minority Oversampling TEchique
 Synthesizing new minority class examples 
  break the tie introduced by simple oversampling and augment the original data
   shown a great success in various applications&lt;br&gt;
   Similar to mixup for deep learning&lt;/p&gt;
&lt;h3 id="variation-of-smote"&gt;Variation of SMOTE&lt;/h3&gt;
&lt;p&gt;Borderline-SMOTE
 ADASYN
  SMOTE + Undersampling
   SMOTE-NC (nominal continuous)
     SMOTE-N (nominal)&lt;/p&gt;
&lt;h3 id="sampling-data-cleaning"&gt;Sampling + Data Cleaning&lt;/h3&gt;
&lt;p&gt;OSS
  CNN + Tomek Links
   NCL based on ENN
    SMOTE + ENN 
     SMOTE + Tomek&lt;/p&gt;
&lt;h3 id="adjusting-algorithms"&gt;Adjusting Algorithms&lt;/h3&gt;
&lt;p&gt;Class weights
 Decision threshold
  Modify an algorithm to be more sensitive to rare classes&lt;/p&gt;
&lt;h2 id="box-drawings"&gt;Box Drawings&lt;/h2&gt;
&lt;p&gt;Construct boxes (axis-parallel hyper-rectangles) around minority class examples
 Concise, intelligible representation of the minority class
  Penalize the number of boxes
   Exact Boxes
    Mixed-integer programming 
     Exact but fairly expensive solution
      Fast Boxes 
       Faster clustering method to generate the initial boxes
        Refine the boxes
         Both perform well among a large set of test datasets&lt;/p&gt;
&lt;h2 id="anomaly-detection-isolation-forest"&gt;Anomaly Detection - Isolation Forest&lt;/h2&gt;
&lt;p&gt;identify anomalies in data (by learning random forests)
     measuring the average number of decision splits to isolate each point
      calculate each data points anomaly score (likelihood to belong to minority)&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;https://www.youtube.com/watch?v=YMPMZmlH5Bo&lt;/p&gt;
&lt;p&gt;http://storm.cis.fordham.edu/~gweiss/small_disjuncts.html&lt;/p&gt;
&lt;p&gt;https://www.svds.com/learning-imbalanced-classes/&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="imbalanced data"></category></entry><entry><title>Tips on LSTM</title><link href="https://misc.legendu.net/blog/tips-on-lstm/" rel="alternate"></link><published>2020-01-11T15:58:32-08:00</published><updated>2020-02-11T15:58:32-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-01-11:/blog/tips-on-lstm/</id><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;http://colah.github.io/posts/2015-08-Understanding-LSTMs/&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="LSTM"></category><category term="RNN"></category></entry><entry><title>POS Tagging in NLP</title><link href="https://misc.legendu.net/blog/pos-tagging-in-nlp/" rel="alternate"></link><published>2020-01-11T15:58:10-08:00</published><updated>2020-02-11T15:58:10-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-01-11:/blog/pos-tagging-in-nlp/</id><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;https://nlpoverview.com/#a-pos-tagging&lt;/p&gt;
&lt;p&gt;Andor et al. (2016) showed a transition-based approach that produces competitive result with a simple feed-forward neural network.&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine lerning"></category><category term="POS tagging"></category><category term="NLP"></category></entry><entry><title>The GPT-2 Model in NLP</title><link href="https://misc.legendu.net/blog/the-gpt-2-model-in-nlp/" rel="alternate"></link><published>2020-01-06T09:57:38-08:00</published><updated>2020-02-06T09:57:38-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-01-06:/blog/the-gpt-2-model-in-nlp/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/openai/gpt-2"&gt;Code for the paper "Language Models are Unsupervised Multitask Learners"&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The project 
&lt;a href="https://github.com/huggingface/transformers"&gt;huggingface/transfomer&lt;/a&gt;
has implementation of transfomer based modles 
(such as
&lt;a href="https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_gpt2.py"&gt;GPT-2&lt;/a&gt;
)
.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/huggingface/transformers/tree/master/examples#gpt-2gpt-and-causal-language-modeling"&gt;Fine Tune the GPT-2 Model&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;https://openai …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/openai/gpt-2"&gt;Code for the paper "Language Models are Unsupervised Multitask Learners"&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The project 
&lt;a href="https://github.com/huggingface/transformers"&gt;huggingface/transfomer&lt;/a&gt;
has implementation of transfomer based modles 
(such as
&lt;a href="https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_gpt2.py"&gt;GPT-2&lt;/a&gt;
)
.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/huggingface/transformers/tree/master/examples#gpt-2gpt-and-causal-language-modeling"&gt;Fine Tune the GPT-2 Model&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;https://openai.com/blog/better-language-models/&lt;/p&gt;
&lt;p&gt;https://medium.com/@asierarranz/i-have-created-a-website-to-query-the-gpt-2-openai-model-11dd30e1c8b0&lt;/p&gt;
&lt;p&gt;https://github.com/openai/gpt-2&lt;/p&gt;
&lt;p&gt;&lt;a href="https://talktotransformer.com/"&gt;TalkToTransformer&lt;/a&gt; is an online example of transformer-based sentence completion.&lt;/p&gt;
&lt;p&gt;https://www.kdnuggets.com/2019/03/openai-gpt-2-model-hype-controversy.html&lt;/p&gt;
&lt;p&gt;https://jaxenter.com/openai-releases-language-model-gpt-2-163881.html&lt;/p&gt;
&lt;p&gt;&lt;a href="http://gltr.io/dist/index.html"&gt;Giant Language model Test Room&lt;/a&gt;&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="GPT-2"></category><category term="NLP"></category><category term="transformer"></category></entry><entry><title>The Bert Model in NLP</title><link href="https://misc.legendu.net/blog/the-bert-model-in-nlp/" rel="alternate"></link><published>2020-01-05T16:28:22-08:00</published><updated>2020-02-05T16:28:22-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-01-05:/blog/the-bert-model-in-nlp/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="implementations"&gt;Implementations&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/huggingface/transformers"&gt;huggingface/transformers&lt;/a&gt;
has PyTorch implementation of transfomer based models 
(such as 
&lt;a href="https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py"&gt;BERT&lt;/a&gt;
and
&lt;a href="https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_distilbert.py"&gt;DistilBERT&lt;/a&gt;
).&lt;/p&gt;
&lt;p&gt;https://github.com/codertimo/BERT-pytorch&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/tensorflow/models/tree/master/official/nlp/bert"&gt;Official Implementation of BERT in TensorFlow&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="tutorials"&gt;Tutorials&lt;/h2&gt;
&lt;p&gt;https://mccormickml.com/2019 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="implementations"&gt;Implementations&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/huggingface/transformers"&gt;huggingface/transformers&lt;/a&gt;
has PyTorch implementation of transfomer based models 
(such as 
&lt;a href="https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py"&gt;BERT&lt;/a&gt;
and
&lt;a href="https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_distilbert.py"&gt;DistilBERT&lt;/a&gt;
).&lt;/p&gt;
&lt;p&gt;https://github.com/codertimo/BERT-pytorch&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/tensorflow/models/tree/master/official/nlp/bert"&gt;Official Implementation of BERT in TensorFlow&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="tutorials"&gt;Tutorials&lt;/h2&gt;
&lt;p&gt;https://mccormickml.com/2019/07/22/BERT-fine-tuning/&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;https://arxiv.org/pdf/1810.04805.pdf&lt;/p&gt;
&lt;p&gt;https://medium.com/huggingface/multi-label-text-classification-using-bert-the-mighty-transformer-69714fa3fb3d&lt;/p&gt;
&lt;p&gt;https://hanxiao.io/2019/01/02/Serving-Google-BERT-in-Production-using-Tensorflow-and-ZeroMQ/&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="machine learning"></category><category term="data science"></category><category term="BERT"></category><category term="Transformer"></category></entry><entry><title>AutoML on Tabular Data Using AutoGluon</title><link href="https://misc.legendu.net/blog/python-automl-autogluon/" rel="alternate"></link><published>2020-02-04T00:00:00-08:00</published><updated>2020-02-04T00:00:00-08:00</updated><author><name>Ben Du</name></author><id>tag:misc.legendu.net,2020-02-04:/blog/python-automl-autogluon/</id><summary type="html">&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;</summary><content type="html">&lt;style type="text/css"&gt;/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI escape sequences */
/* The color values are a mix of
   http://www.xcolors.net/dl/baskerville-ivorylight and
   http://www.xcolors.net/dl/euphrasia */
.ansi-black-fg {
  color: #3E424D;
}
.ansi-black-bg {
  background-color: #3E424D;
}
.ansi-black-intense-fg {
  color: #282C36;
}
.ansi-black-intense-bg {
  background-color: #282C36;
}
.ansi-red-fg {
  color: #E75C58;
}
.ansi-red-bg {
  background-color: #E75C58;
}
.ansi-red-intense-fg {
  color: #B22B31;
}
.ansi-red-intense-bg {
  background-color: #B22B31;
}
.ansi-green-fg {
  color: #00A250;
}
.ansi-green-bg {
  background-color: #00A250;
}
.ansi-green-intense-fg {
  color: #007427;
}
.ansi-green-intense-bg {
  background-color: #007427;
}
.ansi-yellow-fg {
  color: #DDB62B;
}
.ansi-yellow-bg {
  background-color: #DDB62B;
}
.ansi-yellow-intense-fg {
  color: #B27D12;
}
.ansi-yellow-intense-bg {
  background-color: #B27D12;
}
.ansi-blue-fg {
  color: #208FFB;
}
.ansi-blue-bg {
  background-color: #208FFB;
}
.ansi-blue-intense-fg {
  color: #0065CA;
}
.ansi-blue-intense-bg {
  background-color: #0065CA;
}
.ansi-magenta-fg {
  color: #D160C4;
}
.ansi-magenta-bg {
  background-color: #D160C4;
}
.ansi-magenta-intense-fg {
  color: #A03196;
}
.ansi-magenta-intense-bg {
  background-color: #A03196;
}
.ansi-cyan-fg {
  color: #60C6C8;
}
.ansi-cyan-bg {
  background-color: #60C6C8;
}
.ansi-cyan-intense-fg {
  color: #258F8F;
}
.ansi-cyan-intense-bg {
  background-color: #258F8F;
}
.ansi-white-fg {
  color: #C5C1B4;
}
.ansi-white-bg {
  background-color: #C5C1B4;
}
.ansi-white-intense-fg {
  color: #A1A6B2;
}
.ansi-white-intense-bg {
  background-color: #A1A6B2;
}
.ansi-default-inverse-fg {
  color: #FFFFFF;
}
.ansi-default-inverse-bg {
  background-color: #000000;
}
.ansi-bold {
  font-weight: bold;
}
.ansi-underline {
  text-decoration: underline;
}
/* The following styles are deprecated an will be removed in a future version */
.ansibold {
  font-weight: bold;
}
.ansi-inverse {
  outline: 0.5px dotted;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  position: relative;
  overflow: visible;
}
div.cell:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: transparent;
}
div.cell.jupyter-soft-selected {
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected,
div.cell.selected.jupyter-soft-selected {
  border-color: #ababab;
}
div.cell.selected:before,
div.cell.selected.jupyter-soft-selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #42A5F5;
}
@media print {
  div.cell.selected,
  div.cell.selected.jupyter-soft-selected {
    border-color: transparent;
  }
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
}
.edit_mode div.cell.selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #66BB6A;
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell &gt; div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area &gt; div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area &gt; div.highlight &gt; pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the &lt;head&gt; if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  /* Note that this should set vertical padding only, since CodeMirror assumes
       that horizontal padding will be set on CodeMirror pre */
  padding: 0.4em 0;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. This sets horizontal padding only,
    use .CodeMirror-lines for vertical */
  padding: 0 0.4em;
  border: 0;
  border-radius: 0;
}
.CodeMirror-cursor {
  border-left: 1.4px solid black;
}
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .CodeMirror-cursor {
    border-left: 2px solid black;
  }
}
@media screen and (min-width: 4320px) {
  .CodeMirror-cursor {
    border-left: 4px solid black;
  }
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev &lt;Maniac@SoftwareManiacs.Org&gt;
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area 
div.output_area 
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
div.output_area .mglyph &gt; img {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 1px 0 1px 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}



.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}






.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}
.rendered_html ul:not(.list-inline),
.rendered_html ol:not(.list-inline) {
  padding-left: 2em;
}








.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}





.rendered_html pre,




.rendered_html tr,
.rendered_html th,


.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
.rendered_html * + table {
  margin-top: 1em;
}

.rendered_html * + p {
  margin-top: 1em;
}

.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,

.rendered_html img.unconfined,


.rendered_html * + .alert {
  margin-top: 1em;
}
[dir="rtl"] 
div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell &gt; div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered 
.text_cell.rendered .rendered_html tr,
.text_cell.rendered .rendered_html th,
.text_cell.rendered 
.text_cell.unrendered .text_cell_render {
  display: none;
}
.text_cell .dropzone .input_area {
  border: 2px dashed #bababa;
  margin: -1px;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
&lt;/style&gt;
&lt;style type="text/css"&gt;pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
 .highlight pre  .hll { background-color: #ffffcc }
 .highlight pre  { background: #f8f8f8; }
 .highlight pre  .c { color: #3D7B7B; font-style: italic } /* Comment */
 .highlight pre  .err { border: 1px solid #FF0000 } /* Error */
 .highlight pre  .k { color: #008000; font-weight: bold } /* Keyword */
 .highlight pre  .o { color: #666666 } /* Operator */
 .highlight pre  .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */
 .highlight pre  .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */
 .highlight pre  .cp { color: #9C6500 } /* Comment.Preproc */
 .highlight pre  .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */
 .highlight pre  .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */
 .highlight pre  .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */
 .highlight pre  .gd { color: #A00000 } /* Generic.Deleted */
 .highlight pre  .ge { font-style: italic } /* Generic.Emph */
 .highlight pre  .gr { color: #E40000 } /* Generic.Error */
 .highlight pre  .gh { color: #000080; font-weight: bold } /* Generic.Heading */
 .highlight pre  .gi { color: #008400 } /* Generic.Inserted */
 .highlight pre  .go { color: #717171 } /* Generic.Output */
 .highlight pre  .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
 .highlight pre  .gs { font-weight: bold } /* Generic.Strong */
 .highlight pre  .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
 .highlight pre  .gt { color: #0044DD } /* Generic.Traceback */
 .highlight pre  .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
 .highlight pre  .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
 .highlight pre  .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
 .highlight pre  .kp { color: #008000 } /* Keyword.Pseudo */
 .highlight pre  .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
 .highlight pre  .kt { color: #B00040 } /* Keyword.Type */
 .highlight pre  .m { color: #666666 } /* Literal.Number */
 .highlight pre  .s { color: #BA2121 } /* Literal.String */
 .highlight pre  .na { color: #687822 } /* Name.Attribute */
 .highlight pre  .nb { color: #008000 } /* Name.Builtin */
 .highlight pre  .nc { color: #0000FF; font-weight: bold } /* Name.Class */
 .highlight pre  .no { color: #880000 } /* Name.Constant */
 .highlight pre  .nd { color: #AA22FF } /* Name.Decorator */
 .highlight pre  .ni { color: #717171; font-weight: bold } /* Name.Entity */
 .highlight pre  .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */
 .highlight pre  .nf { color: #0000FF } /* Name.Function */
 .highlight pre  .nl { color: #767600 } /* Name.Label */
 .highlight pre  .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
 .highlight pre  .nt { color: #008000; font-weight: bold } /* Name.Tag */
 .highlight pre  .nv { color: #19177C } /* Name.Variable */
 .highlight pre  .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
 .highlight pre  .w { color: #bbbbbb } /* Text.Whitespace */
 .highlight pre  .mb { color: #666666 } /* Literal.Number.Bin */
 .highlight pre  .mf { color: #666666 } /* Literal.Number.Float */
 .highlight pre  .mh { color: #666666 } /* Literal.Number.Hex */
 .highlight pre  .mi { color: #666666 } /* Literal.Number.Integer */
 .highlight pre  .mo { color: #666666 } /* Literal.Number.Oct */
 .highlight pre  .sa { color: #BA2121 } /* Literal.String.Affix */
 .highlight pre  .sb { color: #BA2121 } /* Literal.String.Backtick */
 .highlight pre  .sc { color: #BA2121 } /* Literal.String.Char */
 .highlight pre  .dl { color: #BA2121 } /* Literal.String.Delimiter */
 .highlight pre  .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
 .highlight pre  .s2 { color: #BA2121 } /* Literal.String.Double */
 .highlight pre  .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */
 .highlight pre  .sh { color: #BA2121 } /* Literal.String.Heredoc */
 .highlight pre  .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */
 .highlight pre  .sx { color: #008000 } /* Literal.String.Other */
 .highlight pre  .sr { color: #A45A77 } /* Literal.String.Regex */
 .highlight pre  .s1 { color: #BA2121 } /* Literal.String.Single */
 .highlight pre  .ss { color: #19177C } /* Literal.String.Symbol */
 .highlight pre  .bp { color: #008000 } /* Name.Builtin.Pseudo */
 .highlight pre  .fm { color: #0000FF } /* Name.Function.Magic */
 .highlight pre  .vc { color: #19177C } /* Name.Variable.Class */
 .highlight pre  .vg { color: #19177C } /* Name.Variable.Global */
 .highlight pre  .vi { color: #19177C } /* Name.Variable.Instance */
 .highlight pre  .vm { color: #19177C } /* Name.Variable.Magic */
 .highlight pre  .il { color: #666666 } /* Literal.Number.Integer.Long */&lt;/style&gt;&lt;body&gt;&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[1]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;autogluon&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;autogluon&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;ag&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;autogluon&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;TabularPrediction&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;task&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.model_selection&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Note: please download the dataset &lt;code&gt;legendu/avg_score_after_round3_features&lt;/code&gt; from Kaggle
before proceeding to the following.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[3]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_parquet&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;avg_score_after_round3_features/&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iloc&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;:],&lt;/span&gt; &lt;span class="n"&gt;test_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;119&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[4]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;

    &lt;div class="prompt output_prompt"&gt;Out[4]:&lt;/div&gt;




&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;(553608, 35)&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[5]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;

    &lt;div class="prompt output_prompt"&gt;Out[5]:&lt;/div&gt;




&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;(369072, 35)&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[6]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;task&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;test_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;task&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[&amp;nbsp;]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;task&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_directory&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;auto_gluon&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;avg_score_after_round3&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;

    &lt;div class="prompt"&gt;&lt;/div&gt;


&lt;div class="output_subarea output_stream output_stderr output_text"&gt;
&lt;pre&gt;Beginning AutoGluon training ...
AutoGluon will save models to auto_gluon/
Train Data Rows:    553608
Train Data Columns: 35
Preprocessing data ...
Here are the first 10 unique label values in your data:  [ 5.41576689  0.59051321  3.5933087   0.22835347  2.58887605  0.03626061
 15.30469829  4.38147371  4.57907499  9.85850969]
AutoGluon infers your prediction problem is: regression  (because dtype of label-column == float and many unique label-values observed)
If this is wrong, please specify `problem_type` argument in fit() instead (You may specify problem_type as one of: [&amp;#39;binary&amp;#39;, &amp;#39;multiclass&amp;#39;, &amp;#39;regression&amp;#39;])

Feature Generator processed 553608 data points with 34 features
Original Features:
	int features: 11
	float features: 23
Generated Features:
	int features: 0
All Features:
	int features: 11
	float features: 23
	Data preprocessing and feature engineering runtime = 1.72s ...
AutoGluon will gauge predictive performance using evaluation metric: root_mean_squared_error
To change this, specify the eval_metric argument of fit()
AutoGluon will early stop models using evaluation metric: root_mean_squared_error
/usr/lib/python3.7/imp.py:342: DeprecationWarning: Using or importing the ABCs from &amp;#39;collections&amp;#39; instead of from &amp;#39;collections.abc&amp;#39; is deprecated, and in 3.8 it will stop working
  return _load(spec)
Fitting model: RandomForestRegressorMSE ...
	-1.111	 = Validation root_mean_squared_error score
	153.25s	 = Training runtime
	0.55s	 = Validation runtime
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[13]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_summary&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;

    &lt;div class="prompt"&gt;&lt;/div&gt;


&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;*** Summary of fit() ***
Number of models trained: 9
Types of models trained: 
{&amp;#39;LGBModel&amp;#39;, &amp;#39;CatboostModel&amp;#39;, &amp;#39;KNNModel&amp;#39;, &amp;#39;WeightedEnsembleModel&amp;#39;, &amp;#39;TabularNeuralNetModel&amp;#39;, &amp;#39;RFModel&amp;#39;}
Validation performance of individual models: {&amp;#39;RandomForestRegressorMSE&amp;#39;: -1.0657452978721853, &amp;#39;ExtraTreesRegressorMSE&amp;#39;: -0.9910711261197889, &amp;#39;KNeighborsRegressorUnif&amp;#39;: -1.7777866717734974, &amp;#39;KNeighborsRegressorDist&amp;#39;: -1.5954465432398592, &amp;#39;LightGBMRegressor&amp;#39;: -1.06333423182554, &amp;#39;CatboostRegressor&amp;#39;: -1.0040615551029053, &amp;#39;NeuralNetRegressor&amp;#39;: -0.9160444564621223, &amp;#39;LightGBMRegressorCustom&amp;#39;: -1.038906098211013, &amp;#39;weighted_ensemble_k0_l1&amp;#39;: -0.8812506880621533}
Best model (based on validation performance): weighted_ensemble_k0_l1
Hyperparameter-tuning used: False
Bagging used: False 
Stack-ensembling used: False 
User-specified hyperparameters:
{&amp;#39;NN&amp;#39;: {&amp;#39;num_epochs&amp;#39;: 500}, &amp;#39;GBM&amp;#39;: {&amp;#39;num_boost_round&amp;#39;: 10000}, &amp;#39;CAT&amp;#39;: {&amp;#39;iterations&amp;#39;: 10000}, &amp;#39;RF&amp;#39;: {&amp;#39;n_estimators&amp;#39;: 300}, &amp;#39;XT&amp;#39;: {&amp;#39;n_estimators&amp;#39;: 300}, &amp;#39;KNN&amp;#39;: {}, &amp;#39;custom&amp;#39;: [&amp;#39;GBM&amp;#39;]}
Plot summary of models saved to file: SummaryOfModels.html
*** End of fit() summary ***
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;

    &lt;div class="prompt"&gt;&lt;/div&gt;


&lt;div class="output_subarea output_stream output_stderr output_text"&gt;
&lt;pre&gt;/usr/lib/python3.7/imp.py:342: DeprecationWarning: Using or importing the ABCs from &amp;#39;collections&amp;#39; instead of from &amp;#39;collections.abc&amp;#39; is deprecated, and in 3.8 it will stop working
  return _load(spec)
/usr/local/lib/python3.7/dist-packages/autogluon/utils/plots.py:133: UserWarning: AutoGluon summary plots cannot be created because bokeh is not installed. Please do: &amp;#34;pip install bokeh&amp;#34;
  warnings.warn(&amp;#39;AutoGluon summary plots cannot be created because bokeh is not installed. Please do: &amp;#34;pip install bokeh&amp;#34;&amp;#39;)
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;

    &lt;div class="prompt output_prompt"&gt;Out[13]:&lt;/div&gt;




&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;{&amp;#39;model_types&amp;#39;: {&amp;#39;RandomForestRegressorMSE&amp;#39;: &amp;#39;RFModel&amp;#39;,
  &amp;#39;ExtraTreesRegressorMSE&amp;#39;: &amp;#39;RFModel&amp;#39;,
  &amp;#39;KNeighborsRegressorUnif&amp;#39;: &amp;#39;KNNModel&amp;#39;,
  &amp;#39;KNeighborsRegressorDist&amp;#39;: &amp;#39;KNNModel&amp;#39;,
  &amp;#39;LightGBMRegressor&amp;#39;: &amp;#39;LGBModel&amp;#39;,
  &amp;#39;CatboostRegressor&amp;#39;: &amp;#39;CatboostModel&amp;#39;,
  &amp;#39;NeuralNetRegressor&amp;#39;: &amp;#39;TabularNeuralNetModel&amp;#39;,
  &amp;#39;LightGBMRegressorCustom&amp;#39;: &amp;#39;LGBModel&amp;#39;,
  &amp;#39;weighted_ensemble_k0_l1&amp;#39;: &amp;#39;WeightedEnsembleModel&amp;#39;},
 &amp;#39;model_performance&amp;#39;: {&amp;#39;RandomForestRegressorMSE&amp;#39;: -1.0657452978721853,
  &amp;#39;ExtraTreesRegressorMSE&amp;#39;: -0.9910711261197889,
  &amp;#39;KNeighborsRegressorUnif&amp;#39;: -1.7777866717734974,
  &amp;#39;KNeighborsRegressorDist&amp;#39;: -1.5954465432398592,
  &amp;#39;LightGBMRegressor&amp;#39;: -1.06333423182554,
  &amp;#39;CatboostRegressor&amp;#39;: -1.0040615551029053,
  &amp;#39;NeuralNetRegressor&amp;#39;: -0.9160444564621223,
  &amp;#39;LightGBMRegressorCustom&amp;#39;: -1.038906098211013,
  &amp;#39;weighted_ensemble_k0_l1&amp;#39;: -0.8812506880621533},
 &amp;#39;model_best&amp;#39;: &amp;#39;weighted_ensemble_k0_l1&amp;#39;,
 &amp;#39;model_paths&amp;#39;: {&amp;#39;RandomForestRegressorMSE&amp;#39;: &amp;#39;auto_gluon/models/RandomForestRegressorMSE/&amp;#39;,
  &amp;#39;ExtraTreesRegressorMSE&amp;#39;: &amp;#39;auto_gluon/models/ExtraTreesRegressorMSE/&amp;#39;,
  &amp;#39;KNeighborsRegressorUnif&amp;#39;: &amp;#39;auto_gluon/models/KNeighborsRegressorUnif/&amp;#39;,
  &amp;#39;KNeighborsRegressorDist&amp;#39;: &amp;#39;auto_gluon/models/KNeighborsRegressorDist/&amp;#39;,
  &amp;#39;LightGBMRegressor&amp;#39;: &amp;#39;auto_gluon/models/LightGBMRegressor/&amp;#39;,
  &amp;#39;CatboostRegressor&amp;#39;: &amp;#39;auto_gluon/models/CatboostRegressor/&amp;#39;,
  &amp;#39;NeuralNetRegressor&amp;#39;: &amp;#39;auto_gluon/models/NeuralNetRegressor/&amp;#39;,
  &amp;#39;LightGBMRegressorCustom&amp;#39;: &amp;#39;auto_gluon/models/LightGBMRegressorCustom/&amp;#39;,
  &amp;#39;weighted_ensemble_k0_l1&amp;#39;: &amp;#39;auto_gluon/models/weighted_ensemble_k0_l1/&amp;#39;},
 &amp;#39;model_fit_times&amp;#39;: {&amp;#39;RandomForestRegressorMSE&amp;#39;: 152.92721271514893,
  &amp;#39;ExtraTreesRegressorMSE&amp;#39;: 100.98368000984192,
  &amp;#39;KNeighborsRegressorUnif&amp;#39;: 19.484992265701294,
  &amp;#39;KNeighborsRegressorDist&amp;#39;: 19.212828636169434,
  &amp;#39;LightGBMRegressor&amp;#39;: 13.707395076751709,
  &amp;#39;CatboostRegressor&amp;#39;: 799.2617483139038,
  &amp;#39;NeuralNetRegressor&amp;#39;: 4635.101431131363,
  &amp;#39;LightGBMRegressorCustom&amp;#39;: 21.870562076568604,
  &amp;#39;weighted_ensemble_k0_l1&amp;#39;: 0.795548677444458},
 &amp;#39;model_pred_times&amp;#39;: {&amp;#39;RandomForestRegressorMSE&amp;#39;: 0.5544726848602295,
  &amp;#39;ExtraTreesRegressorMSE&amp;#39;: 0.738731861114502,
  &amp;#39;KNeighborsRegressorUnif&amp;#39;: 0.15466761589050293,
  &amp;#39;KNeighborsRegressorDist&amp;#39;: 0.13497662544250488,
  &amp;#39;LightGBMRegressor&amp;#39;: 0.02784132957458496,
  &amp;#39;CatboostRegressor&amp;#39;: 0.04940915107727051,
  &amp;#39;NeuralNetRegressor&amp;#39;: 4.679487466812134,
  &amp;#39;LightGBMRegressorCustom&amp;#39;: 0.03708219528198242,
  &amp;#39;weighted_ensemble_k0_l1&amp;#39;: 0.0013790130615234375},
 &amp;#39;num_bagging_folds&amp;#39;: 0,
 &amp;#39;stack_ensemble_levels&amp;#39;: 0,
 &amp;#39;feature_prune&amp;#39;: False,
 &amp;#39;hyperparameter_tune&amp;#39;: False,
 &amp;#39;hyperparameters_userspecified&amp;#39;: {&amp;#39;NN&amp;#39;: {&amp;#39;num_epochs&amp;#39;: 500},
  &amp;#39;GBM&amp;#39;: {&amp;#39;num_boost_round&amp;#39;: 10000},
  &amp;#39;CAT&amp;#39;: {&amp;#39;iterations&amp;#39;: 10000},
  &amp;#39;RF&amp;#39;: {&amp;#39;n_estimators&amp;#39;: 300},
  &amp;#39;XT&amp;#39;: {&amp;#39;n_estimators&amp;#39;: 300},
  &amp;#39;KNN&amp;#39;: {},
  &amp;#39;custom&amp;#39;: [&amp;#39;GBM&amp;#39;]},
 &amp;#39;model_hyperparams&amp;#39;: {&amp;#39;RandomForestRegressorMSE&amp;#39;: {&amp;#39;model_type&amp;#39;: &amp;#39;rf&amp;#39;,
   &amp;#39;n_estimators&amp;#39;: 300,
   &amp;#39;n_jobs&amp;#39;: -1,
   &amp;#39;criterion&amp;#39;: &amp;#39;mse&amp;#39;},
  &amp;#39;ExtraTreesRegressorMSE&amp;#39;: {&amp;#39;model_type&amp;#39;: &amp;#39;xt&amp;#39;,
   &amp;#39;n_estimators&amp;#39;: 300,
   &amp;#39;n_jobs&amp;#39;: -1,
   &amp;#39;criterion&amp;#39;: &amp;#39;mse&amp;#39;},
  &amp;#39;KNeighborsRegressorUnif&amp;#39;: {&amp;#39;weights&amp;#39;: &amp;#39;uniform&amp;#39;, &amp;#39;n_jobs&amp;#39;: -1},
  &amp;#39;KNeighborsRegressorDist&amp;#39;: {&amp;#39;weights&amp;#39;: &amp;#39;distance&amp;#39;, &amp;#39;n_jobs&amp;#39;: -1},
  &amp;#39;LightGBMRegressor&amp;#39;: {&amp;#39;num_boost_round&amp;#39;: 10000,
   &amp;#39;num_threads&amp;#39;: -1,
   &amp;#39;objective&amp;#39;: &amp;#39;regression&amp;#39;,
   &amp;#39;metric&amp;#39;: &amp;#39;regression&amp;#39;,
   &amp;#39;verbose&amp;#39;: -1,
   &amp;#39;boosting_type&amp;#39;: &amp;#39;gbdt&amp;#39;,
   &amp;#39;two_round&amp;#39;: True},
  &amp;#39;CatboostRegressor&amp;#39;: {&amp;#39;iterations&amp;#39;: 10000,
   &amp;#39;learning_rate&amp;#39;: 0.1,
   &amp;#39;random_seed&amp;#39;: 0,
   &amp;#39;eval_metric&amp;#39;: &amp;lt;autogluon.utils.tabular.ml.models.catboost.catboost_utils.RegressionCustomMetric at 0x7f3500c8eb00&amp;gt;},
  &amp;#39;NeuralNetRegressor&amp;#39;: {&amp;#39;num_epochs&amp;#39;: 500,
   &amp;#39;seed_value&amp;#39;: None,
   &amp;#39;proc.embed_min_categories&amp;#39;: 4,
   &amp;#39;proc.impute_strategy&amp;#39;: &amp;#39;median&amp;#39;,
   &amp;#39;proc.max_category_levels&amp;#39;: 100,
   &amp;#39;proc.skew_threshold&amp;#39;: 0.99,
   &amp;#39;network_type&amp;#39;: &amp;#39;widedeep&amp;#39;,
   &amp;#39;layers&amp;#39;: [256, 128],
   &amp;#39;numeric_embed_dim&amp;#39;: 420,
   &amp;#39;activation&amp;#39;: &amp;#39;relu&amp;#39;,
   &amp;#39;max_layer_width&amp;#39;: 2056,
   &amp;#39;embedding_size_factor&amp;#39;: 1.0,
   &amp;#39;embed_exponent&amp;#39;: 0.56,
   &amp;#39;max_embedding_dim&amp;#39;: 100,
   &amp;#39;y_range&amp;#39;: (0, 57.325798296928404),
   &amp;#39;y_range_extend&amp;#39;: 0.05,
   &amp;#39;use_batchnorm&amp;#39;: True,
   &amp;#39;dropout_prob&amp;#39;: 0.1,
   &amp;#39;batch_size&amp;#39;: 512,
   &amp;#39;loss_function&amp;#39;: L1Loss(batch_axis=0, w=None),
   &amp;#39;optimizer&amp;#39;: &amp;#39;adam&amp;#39;,
   &amp;#39;learning_rate&amp;#39;: 0.0003,
   &amp;#39;weight_decay&amp;#39;: 1e-06,
   &amp;#39;clip_gradient&amp;#39;: 100.0,
   &amp;#39;momentum&amp;#39;: 0.9,
   &amp;#39;epochs_wo_improve&amp;#39;: 20,
   &amp;#39;num_dataloading_workers&amp;#39;: 20,
   &amp;#39;ctx&amp;#39;: cpu(0)},
  &amp;#39;LightGBMRegressorCustom&amp;#39;: {&amp;#39;num_boost_round&amp;#39;: 10000,
   &amp;#39;num_threads&amp;#39;: -1,
   &amp;#39;objective&amp;#39;: &amp;#39;regression&amp;#39;,
   &amp;#39;metric&amp;#39;: &amp;#39;regression&amp;#39;,
   &amp;#39;verbose&amp;#39;: -1,
   &amp;#39;boosting_type&amp;#39;: &amp;#39;gbdt&amp;#39;,
   &amp;#39;two_round&amp;#39;: True,
   &amp;#39;learning_rate&amp;#39;: 0.03,
   &amp;#39;num_leaves&amp;#39;: 128,
   &amp;#39;feature_fraction&amp;#39;: 0.9,
   &amp;#39;min_data_in_leaf&amp;#39;: 5,
   &amp;#39;seed_value&amp;#39;: 0},
  &amp;#39;weighted_ensemble_k0_l1&amp;#39;: {&amp;#39;max_models&amp;#39;: 25, &amp;#39;max_models_per_type&amp;#39;: 5}}}&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[15]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;leaderboard&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;

    &lt;div class="prompt"&gt;&lt;/div&gt;


&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;                      model  score_val     fit_time  pred_time_val  stack_level
8   weighted_ensemble_k0_l1  -0.881251     0.795549       0.001379            1
6        NeuralNetRegressor  -0.916044  4635.101431       4.679487            0
1    ExtraTreesRegressorMSE  -0.991071   100.983680       0.738732            0
5         CatboostRegressor  -1.004062   799.261748       0.049409            0
7   LightGBMRegressorCustom  -1.038906    21.870562       0.037082            0
4         LightGBMRegressor  -1.063334    13.707395       0.027841            0
0  RandomForestRegressorMSE  -1.065745   152.927213       0.554473            0
3   KNeighborsRegressorDist  -1.595447    19.212829       0.134977            0
2   KNeighborsRegressorUnif  -1.777787    19.484992       0.154668            0
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;

    &lt;div class="prompt output_prompt"&gt;Out[15]:&lt;/div&gt;



&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;model&lt;/th&gt;
      &lt;th&gt;score_val&lt;/th&gt;
      &lt;th&gt;fit_time&lt;/th&gt;
      &lt;th&gt;pred_time_val&lt;/th&gt;
      &lt;th&gt;stack_level&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;weighted_ensemble_k0_l1&lt;/td&gt;
      &lt;td&gt;-0.881251&lt;/td&gt;
      &lt;td&gt;0.795549&lt;/td&gt;
      &lt;td&gt;0.001379&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;NeuralNetRegressor&lt;/td&gt;
      &lt;td&gt;-0.916044&lt;/td&gt;
      &lt;td&gt;4635.101431&lt;/td&gt;
      &lt;td&gt;4.679487&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;ExtraTreesRegressorMSE&lt;/td&gt;
      &lt;td&gt;-0.991071&lt;/td&gt;
      &lt;td&gt;100.983680&lt;/td&gt;
      &lt;td&gt;0.738732&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;CatboostRegressor&lt;/td&gt;
      &lt;td&gt;-1.004062&lt;/td&gt;
      &lt;td&gt;799.261748&lt;/td&gt;
      &lt;td&gt;0.049409&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;LightGBMRegressorCustom&lt;/td&gt;
      &lt;td&gt;-1.038906&lt;/td&gt;
      &lt;td&gt;21.870562&lt;/td&gt;
      &lt;td&gt;0.037082&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;LightGBMRegressor&lt;/td&gt;
      &lt;td&gt;-1.063334&lt;/td&gt;
      &lt;td&gt;13.707395&lt;/td&gt;
      &lt;td&gt;0.027841&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;RandomForestRegressorMSE&lt;/td&gt;
      &lt;td&gt;-1.065745&lt;/td&gt;
      &lt;td&gt;152.927213&lt;/td&gt;
      &lt;td&gt;0.554473&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;KNeighborsRegressorDist&lt;/td&gt;
      &lt;td&gt;-1.595447&lt;/td&gt;
      &lt;td&gt;19.212829&lt;/td&gt;
      &lt;td&gt;0.134977&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;KNeighborsRegressorUnif&lt;/td&gt;
      &lt;td&gt;-1.777787&lt;/td&gt;
      &lt;td&gt;19.484992&lt;/td&gt;
      &lt;td&gt;0.154668&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[16]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;dir&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;

    &lt;div class="prompt output_prompt"&gt;Out[16]:&lt;/div&gt;




&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;[&amp;#39;__abstractmethods__&amp;#39;,
 &amp;#39;__class__&amp;#39;,
 &amp;#39;__delattr__&amp;#39;,
 &amp;#39;__dict__&amp;#39;,
 &amp;#39;__dir__&amp;#39;,
 &amp;#39;__doc__&amp;#39;,
 &amp;#39;__eq__&amp;#39;,
 &amp;#39;__format__&amp;#39;,
 &amp;#39;__ge__&amp;#39;,
 &amp;#39;__getattribute__&amp;#39;,
 &amp;#39;__gt__&amp;#39;,
 &amp;#39;__hash__&amp;#39;,
 &amp;#39;__init__&amp;#39;,
 &amp;#39;__init_subclass__&amp;#39;,
 &amp;#39;__le__&amp;#39;,
 &amp;#39;__lt__&amp;#39;,
 &amp;#39;__module__&amp;#39;,
 &amp;#39;__ne__&amp;#39;,
 &amp;#39;__new__&amp;#39;,
 &amp;#39;__reduce__&amp;#39;,
 &amp;#39;__reduce_ex__&amp;#39;,
 &amp;#39;__repr__&amp;#39;,
 &amp;#39;__setattr__&amp;#39;,
 &amp;#39;__sizeof__&amp;#39;,
 &amp;#39;__slots__&amp;#39;,
 &amp;#39;__str__&amp;#39;,
 &amp;#39;__subclasshook__&amp;#39;,
 &amp;#39;__weakref__&amp;#39;,
 &amp;#39;_abc_impl&amp;#39;,
 &amp;#39;_createResults&amp;#39;,
 &amp;#39;_format_results&amp;#39;,
 &amp;#39;_learner&amp;#39;,
 &amp;#39;_save_model&amp;#39;,
 &amp;#39;_save_results&amp;#39;,
 &amp;#39;_summarize&amp;#39;,
 &amp;#39;_trainer&amp;#39;,
 &amp;#39;class_labels&amp;#39;,
 &amp;#39;eval_metric&amp;#39;,
 &amp;#39;evaluate&amp;#39;,
 &amp;#39;evaluate_predictions&amp;#39;,
 &amp;#39;feature_types&amp;#39;,
 &amp;#39;fit_summary&amp;#39;,
 &amp;#39;label_column&amp;#39;,
 &amp;#39;leaderboard&amp;#39;,
 &amp;#39;load&amp;#39;,
 &amp;#39;model_names&amp;#39;,
 &amp;#39;model_performance&amp;#39;,
 &amp;#39;output_directory&amp;#39;,
 &amp;#39;predict&amp;#39;,
 &amp;#39;predict_proba&amp;#39;,
 &amp;#39;problem_type&amp;#39;,
 &amp;#39;save&amp;#39;]&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Load-Saved-Models"&gt;Load Saved Models&lt;a class="anchor-link" href="#Load-Saved-Models"&gt;&amp;#182;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The trained models are automatically saved to disk 
and can be load back into memory.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[10]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;task&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;auto_gluon&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[11]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;leaderboard&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;

    &lt;div class="prompt"&gt;&lt;/div&gt;


&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;                      model  score_val     fit_time  pred_time_val  stack_level
8   weighted_ensemble_k0_l1  -0.881251     0.795549       0.001379            1
6        NeuralNetRegressor  -0.916044  4635.101431       4.679487            0
1    ExtraTreesRegressorMSE  -0.991071   100.983680       0.738732            0
5         CatboostRegressor  -1.004062   799.261748       0.049409            0
7   LightGBMRegressorCustom  -1.038906    21.870562       0.037082            0
4         LightGBMRegressor  -1.063334    13.707395       0.027841            0
0  RandomForestRegressorMSE  -1.065745   152.927213       0.554473            0
3   KNeighborsRegressorDist  -1.595447    19.212829       0.134977            0
2   KNeighborsRegressorUnif  -1.777787    19.484992       0.154668            0
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;

    &lt;div class="prompt output_prompt"&gt;Out[11]:&lt;/div&gt;



&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;model&lt;/th&gt;
      &lt;th&gt;score_val&lt;/th&gt;
      &lt;th&gt;fit_time&lt;/th&gt;
      &lt;th&gt;pred_time_val&lt;/th&gt;
      &lt;th&gt;stack_level&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;weighted_ensemble_k0_l1&lt;/td&gt;
      &lt;td&gt;-0.881251&lt;/td&gt;
      &lt;td&gt;0.795549&lt;/td&gt;
      &lt;td&gt;0.001379&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;NeuralNetRegressor&lt;/td&gt;
      &lt;td&gt;-0.916044&lt;/td&gt;
      &lt;td&gt;4635.101431&lt;/td&gt;
      &lt;td&gt;4.679487&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;ExtraTreesRegressorMSE&lt;/td&gt;
      &lt;td&gt;-0.991071&lt;/td&gt;
      &lt;td&gt;100.983680&lt;/td&gt;
      &lt;td&gt;0.738732&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;CatboostRegressor&lt;/td&gt;
      &lt;td&gt;-1.004062&lt;/td&gt;
      &lt;td&gt;799.261748&lt;/td&gt;
      &lt;td&gt;0.049409&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;LightGBMRegressorCustom&lt;/td&gt;
      &lt;td&gt;-1.038906&lt;/td&gt;
      &lt;td&gt;21.870562&lt;/td&gt;
      &lt;td&gt;0.037082&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;LightGBMRegressor&lt;/td&gt;
      &lt;td&gt;-1.063334&lt;/td&gt;
      &lt;td&gt;13.707395&lt;/td&gt;
      &lt;td&gt;0.027841&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;RandomForestRegressorMSE&lt;/td&gt;
      &lt;td&gt;-1.065745&lt;/td&gt;
      &lt;td&gt;152.927213&lt;/td&gt;
      &lt;td&gt;0.554473&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;KNeighborsRegressorDist&lt;/td&gt;
      &lt;td&gt;-1.595447&lt;/td&gt;
      &lt;td&gt;19.212829&lt;/td&gt;
      &lt;td&gt;0.134977&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;KNeighborsRegressorUnif&lt;/td&gt;
      &lt;td&gt;-1.777787&lt;/td&gt;
      &lt;td&gt;19.484992&lt;/td&gt;
      &lt;td&gt;0.154668&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Further-Research"&gt;Further Research&lt;a class="anchor-link" href="#Further-Research"&gt;&amp;#182;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;It is strange that ExtraTreesRegressorMSE and RandomForestRegressorMSE generate huge models. 
Check to see what happened.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[20]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt;du&lt;span class="w"&gt; &lt;/span&gt;-lhd&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;auto_gluon/models
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;

    &lt;div class="prompt"&gt;&lt;/div&gt;


&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;1.3M	auto_gluon/models/LightGBMRegressor
100K	auto_gluon/models/weighted_ensemble_k0_l1
20G	auto_gluon/models/ExtraTreesRegressorMSE
311M	auto_gluon/models/KNeighborsRegressorDist
311M	auto_gluon/models/KNeighborsRegressorUnif
13G	auto_gluon/models/RandomForestRegressorMSE
4.3M	auto_gluon/models/LightGBMRegressorCustom
3.9M	auto_gluon/models/NeuralNetRegressor
1.8M	auto_gluon/models/CatboostRegressor
32G	auto_gluon/models
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[&amp;nbsp;]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; 
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
 

&lt;/body&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['$','$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        " linebreaks: { automatic: true, width: '95% container' }, " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;
</content><category term="Ai"></category><category term="Ai"></category><category term="Python"></category><category term="data science"></category><category term="machine learning"></category><category term="AutoML"></category><category term="AutoGluon"></category></entry><entry><title>Tips on Recommendation Systems</title><link href="https://misc.legendu.net/blog/tips-on-recommendation-systems/" rel="alternate"></link><published>2020-01-27T12:00:51-08:00</published><updated>2020-01-27T12:00:51-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-01-27:/blog/tips-on-recommendation-systems/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;User-based Filtering (Memory-based Filtering)&lt;/p&gt;
&lt;p&gt;Item-based Filtering (Content-based Filtering)&lt;/p&gt;
&lt;p&gt;Non-negative Matrix Factorization&lt;/p&gt;
&lt;p&gt;Neural Matrix Factorization&lt;/p&gt;
&lt;p&gt;Variational Autoencoder&lt;/p&gt;
&lt;p&gt;Hybrid&lt;/p&gt;
&lt;p&gt;New methods like VAE, AE, or Deep Collaborative outperform classical methods like NMF on …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;User-based Filtering (Memory-based Filtering)&lt;/p&gt;
&lt;p&gt;Item-based Filtering (Content-based Filtering)&lt;/p&gt;
&lt;p&gt;Non-negative Matrix Factorization&lt;/p&gt;
&lt;p&gt;Neural Matrix Factorization&lt;/p&gt;
&lt;p&gt;Variational Autoencoder&lt;/p&gt;
&lt;p&gt;Hybrid&lt;/p&gt;
&lt;p&gt;New methods like VAE, AE, or Deep Collaborative outperform classical methods like NMF on the NDCG metric. Non-linear probabilistic models such as variational autoencoders enable us to go beyond the limited modeling capacity of linear factor models.&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;https://medium.com/snipfeed/how-variational-autoencoders-make-classical-recommender-systems-obsolete-4df8bae51546&lt;/p&gt;
&lt;p&gt;https://medium.com/snipfeed/how-to-implement-deep-generative-models-for-recommender-systems-29110be8971f&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/pdf/1802.05814.pdf"&gt;Variational Autoencoders for Collaborative Filtering&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/Matrix_factorization_(recommender_systems)&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="machine learning"></category><category term="data science"></category><category term="recommendation system"></category><category term="collaborative filtering"></category><category term="NMF"></category><category term="non-negative matrix factorization"></category></entry><entry><title>Activation Functions in Neural Network</title><link href="https://misc.legendu.net/blog/activation-functions-in-neural-network/" rel="alternate"></link><published>2019-12-17T14:20:03-08:00</published><updated>2020-01-17T14:20:03-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2019-12-17:/blog/activation-functions-in-neural-network/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="gelu"&gt;GELU&lt;/h2&gt;
&lt;p&gt;GELU is the best activation function currently (at least in NLP).&lt;/p&gt;
&lt;div class="math"&gt;$$ GELU(x) == x \Phi(x) $$&lt;/div&gt;
&lt;p&gt;,&lt;/p&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\Phi(x)\)&lt;/span&gt; is the cumulative distribution function of the standard normal distribution.&lt;/p&gt;
&lt;h2 id="relu"&gt;ReLU …&lt;/h2&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="gelu"&gt;GELU&lt;/h2&gt;
&lt;p&gt;GELU is the best activation function currently (at least in NLP).&lt;/p&gt;
&lt;div class="math"&gt;$$ GELU(x) == x \Phi(x) $$&lt;/div&gt;
&lt;p&gt;,&lt;/p&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\Phi(x)\)&lt;/span&gt; is the cumulative distribution function of the standard normal distribution.&lt;/p&gt;
&lt;h2 id="relu"&gt;ReLU&lt;/h2&gt;
&lt;h2 id="elu"&gt;ELU&lt;/h2&gt;
&lt;h2 id="swish"&gt;Swish&lt;/h2&gt;
&lt;div class="math"&gt;$$ f(x) = x \dot \sigma(x) $$&lt;/div&gt;
&lt;p&gt;,
where &lt;span class="math"&gt;\(\sigma(x)\)&lt;/span&gt; is the 
&lt;a href="https://en.wikipedia.org/wiki/Sigmoid_function"&gt;sigmoid function&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="sigmoid"&gt;Sigmoid&lt;/h2&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;https://mp.weixin.qq.com/s/LEPalstOc15CX6fuqMRJ8Q&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js','mhchem.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="activation function"></category><category term="ReLU"></category><category term="GELU"></category><category term="Switch"></category><category term="Sigmoid"></category></entry><entry><title>Tips on Transformer in NLP</title><link href="https://misc.legendu.net/blog/tips-on-transformer-in-nlp/" rel="alternate"></link><published>2020-01-13T14:17:48-08:00</published><updated>2020-01-13T14:17:48-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-01-13:/blog/tips-on-transformer-in-nlp/</id><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;http://nlp.seas.harvard.edu/2018/04/03/attention.html&lt;/p&gt;
&lt;p&gt;https://blog.floydhub.com/the-transformer-in-pytorch/&lt;/p&gt;
&lt;p&gt;http://jalammar.github.io/illustrated-transformer/&lt;/p&gt;
&lt;p&gt;https://towardsdatascience.com/transformers-141e32e69591&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="NLP"></category></entry><entry><title>Feature Scaling in Machine Learning</title><link href="https://misc.legendu.net/blog/feature-scaling-in-machine-learning/" rel="alternate"></link><published>2020-01-13T14:12:39-08:00</published><updated>2020-01-13T14:12:39-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-01-13:/blog/feature-scaling-in-machine-learning/</id><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/Feature_scaling&lt;/p&gt;
&lt;p&gt;https://www.jeremyjordan.me/batch-normalization/&lt;/p&gt;
&lt;p&gt;&lt;a href="https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/"&gt;How to use Data Scaling Improve Deep Learning Model Stability and Performance&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;https://medium.com/@urvashilluniya/why-data-normalization-is-necessary-for-machine-learning-models-681b65a05029&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="feature"></category><category term="normalize"></category><category term="scale"></category></entry><entry><title>Understand Attention in NLP</title><link href="https://misc.legendu.net/blog/understand-attention-in-nlp/" rel="alternate"></link><published>2020-01-08T15:27:58-08:00</published><updated>2020-01-08T15:27:58-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-01-08:/blog/understand-attention-in-nlp/</id><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/&lt;/p&gt;
&lt;p&gt;https://medium.com/@joealato/attention-in-nlp-734c6fa9d983&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="NLP"></category><category term="attention"></category></entry><entry><title>Tips on Word2Vec</title><link href="https://misc.legendu.net/blog/tips-on-word2vec/" rel="alternate"></link><published>2020-01-08T12:06:32-08:00</published><updated>2020-01-08T12:06:32-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-01-08:/blog/tips-on-word2vec/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="word2vec"&gt;Word2Vec&lt;/h2&gt;
&lt;p&gt;https://code.google.com/archive/p/word2vec/&lt;/p&gt;
&lt;h3 id="hierarchical-softmasx"&gt;Hierarchical Softmasx&lt;/h3&gt;
&lt;h3 id="negative-sampling"&gt;Negative Sampling&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://code.google.com/archive/p/word2vec/"&gt;Google Word2Vec&lt;/a&gt;
claims that 
hierarchical softmax is better for infrequent words 
while negative sampling is better for frequent words …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="word2vec"&gt;Word2Vec&lt;/h2&gt;
&lt;p&gt;https://code.google.com/archive/p/word2vec/&lt;/p&gt;
&lt;h3 id="hierarchical-softmasx"&gt;Hierarchical Softmasx&lt;/h3&gt;
&lt;h3 id="negative-sampling"&gt;Negative Sampling&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://code.google.com/archive/p/word2vec/"&gt;Google Word2Vec&lt;/a&gt;
claims that 
hierarchical softmax is better for infrequent words 
while negative sampling is better for frequent words 
and better with low dimensional vectors.&lt;/p&gt;
&lt;p&gt;http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/&lt;/p&gt;
&lt;p&gt;https://stackoverflow.com/questions/27860652/word2vec-negative-sampling-in-layman-term&lt;/p&gt;
&lt;p&gt;https://towardsdatascience.com/hierarchical-softmax-and-negative-sampling-short-notes-worth-telling-2672010dbe08&lt;/p&gt;
&lt;p&gt;https://www.quora.com/What-is-negative-sampling&lt;/p&gt;
&lt;p&gt;https://stats.stackexchange.com/questions/180076/why-is-hierarchical-softmax-better-for-infrequent-words-while-negative-sampling&lt;/p&gt;
&lt;h2 id="examples"&gt;Examples&lt;/h2&gt;
&lt;p&gt;https://blog.floydhub.com/automate-customer-support-part-one/&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="Word2Vec"></category><category term="word embedding"></category><category term="distributed representation"></category></entry><entry><title>Compresion of Deep Learning Models</title><link href="https://misc.legendu.net/blog/compresion-of-deep-learning-models/" rel="alternate"></link><published>2020-01-08T11:26:35-08:00</published><updated>2020-01-08T11:26:35-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-01-08:/blog/compresion-of-deep-learning-models/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding&lt;/p&gt;
&lt;p&gt;MobileNet&lt;/p&gt;
&lt;p&gt;一、网络修剪&lt;/p&gt;
&lt;p&gt;网络修剪，采用当网络权重非 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding&lt;/p&gt;
&lt;p&gt;MobileNet&lt;/p&gt;
&lt;p&gt;一、网络修剪&lt;/p&gt;
&lt;p&gt;网络修剪，采用当网络权重非常小的时候(小于某个设定的阈值)，把它置0，就像二值网络一般；然后屏蔽被设置为0的权重更新，继续进行训练；以此循环，每隔训练几轮过后，继续进行修剪。&lt;/p&gt;
&lt;p&gt;二、权重共享&lt;/p&gt;
&lt;p&gt;对于每一层的参数,我们进行k-means聚类,进行量化，对于归属于同一个聚类中心的权重，采用共享一个权重,进行重新训练。需要注意的是这个权重共享并不是层之间的权重共享，这是对于每一层的单独共享。&lt;/p&gt;
&lt;p&gt;三、增加L2权重&lt;/p&gt;
&lt;p&gt;增加L2权重可以让更多的权重，靠近0，这样每次修剪的比例大大增加。&lt;/p&gt;
&lt;p&gt;四、从结构上，简化网络计算&lt;/p&gt;
&lt;p&gt;这些需自己阅读比较多相关文献，才能设计出合理，速度更快的网络，比如引入fire module、NIN、除全连接层等一些设计思想，这边不进行具体详述。&lt;/p&gt;
&lt;p&gt;&lt;a href="https://blog.floydhub.com/knowledge-distillation/"&gt;Distilling knowledge from Neural Networks to build smaller and faster models&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;https://blog.floydhub.com/knowledge-distillation/&lt;/p&gt;
&lt;p&gt;https://mp.weixin.qq.com/s?__biz=MzU0NTAyNTQ1OQ==&amp;amp;mid=2247484793&amp;amp;idx=1&amp;amp;sn=d18b5f6a0b278d24ee5589dec5d72f9a&amp;amp;chksm=fb7279a5cc05f0b3edce2f2e87467a34dd4e3cda042312082bc5591e438a4dca31044762977d&amp;amp;scene=21#wechat_redirect&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="deep compression"></category><category term="compression"></category></entry><entry><title>Difference Between torch.nn.Module and torch.nn.functional</title><link href="https://misc.legendu.net/blog/difference-between-torch.nn.Module-and-torch.nn.functional/" rel="alternate"></link><published>2020-01-07T11:26:38-08:00</published><updated>2020-01-07T11:26:38-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-01-07:/blog/difference-between-torch.nn.Module-and-torch.nn.functional/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Modules in &lt;code&gt;torch.nn&lt;/code&gt; are internal implemented based on &lt;code&gt;torch.nn.functional&lt;/code&gt;.
Modules in &lt;code&gt;torch.nn&lt;/code&gt; are easier to use while &lt;code&gt;torch.nn.functional&lt;/code&gt; is more flexible.
It is recommended to …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Modules in &lt;code&gt;torch.nn&lt;/code&gt; are internal implemented based on &lt;code&gt;torch.nn.functional&lt;/code&gt;.
Modules in &lt;code&gt;torch.nn&lt;/code&gt; are easier to use while &lt;code&gt;torch.nn.functional&lt;/code&gt; is more flexible.
It is recommended to use &lt;code&gt;nn.Conv2d&lt;/code&gt; because it uses the &lt;code&gt;nn.Module&lt;/code&gt; abstraction 
and nicely ties into the torch.optim framework well.&lt;/p&gt;
&lt;p&gt;Based on my understanding,
it is always safe to use modules in torch.nn if available
and it is suggested that you do it this way.
However, 
for modules/layers that do not have parameters to optimize, 
you can also use the functional equivalence (which is the underlying implementation of the modules/layers).&lt;/p&gt;
&lt;p&gt;Similarly for loss functions.&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;https://discuss.pytorch.org/t/what-is-the-difference-between-torch-nn-and-torch-nn-functional/33597&lt;/p&gt;
&lt;p&gt;https://discuss.pytorch.org/t/how-to-choose-between-torch-nn-functional-and-torch-nn-module/2800&lt;/p&gt;
&lt;p&gt;https://discuss.pytorch.org/t/what-is-the-difference-between-torch-nn-and-torch-nn-functional/33597&lt;/p&gt;
&lt;p&gt;https://discuss.pytorch.org/t/beginner-should-relu-sigmoid-be-called-in-the-init-method/18689&lt;/p&gt;
&lt;p&gt;https://stackoverflow.com/questions/54662984/pytorch-why-loss-functions-are-implemented-both-in-nn-modules-loss-and-nn-funct&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="PyTorch"></category><category term="torch.nn.Module"></category><category term="torch.nn.functional"></category></entry><entry><title>Log Softmax vs Softmax</title><link href="https://misc.legendu.net/blog/log-softmax-vs-softmax/" rel="alternate"></link><published>2020-01-07T10:19:59-08:00</published><updated>2020-01-07T10:19:59-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-01-07:/blog/log-softmax-vs-softmax/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The difference betwen Log Softmax and Softmax should be understood together with the loss function.&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;https://discuss.pytorch.org/t/what-is-the-difference-between-log-softmax-and-softmax/11801&lt;/p&gt;
&lt;p&gt;https://discuss.pytorch.org/t/logsoftmax-vs-softmax/21386&lt;/p&gt;
&lt;p&gt;https …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The difference betwen Log Softmax and Softmax should be understood together with the loss function.&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;https://discuss.pytorch.org/t/what-is-the-difference-between-log-softmax-and-softmax/11801&lt;/p&gt;
&lt;p&gt;https://discuss.pytorch.org/t/logsoftmax-vs-softmax/21386&lt;/p&gt;
&lt;p&gt;https://forums.fast.ai/t/logsoftmax-vs-softmax/14469&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="Softmax"></category><category term="Log Softmax"></category></entry><entry><title>Models for Computer Vision</title><link href="https://misc.legendu.net/blog/models-for-computer-vision/" rel="alternate"></link><published>2019-12-07T09:50:39-08:00</published><updated>2020-01-07T09:50:39-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2019-12-07:/blog/models-for-computer-vision/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/pytorch/vision"&gt;torchvision&lt;/a&gt;
has implementation of popular deep learning models for computer vision.&lt;/p&gt;
&lt;h2 id="resnet-50"&gt;ResNet-50&lt;/h2&gt;
&lt;p&gt;ResNet-50 is a 50-layer Residual Neural Network. &lt;/p&gt;
&lt;h2 id="resnet-101"&gt;ResNet 101&lt;/h2&gt;
&lt;p&gt;ResNet-101 is a 101-layer Residual Neural Network. &lt;/p&gt;
&lt;h2 id="resnet-152"&gt;ResNet 152&lt;/h2&gt;
&lt;p&gt;ResNet-152 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/pytorch/vision"&gt;torchvision&lt;/a&gt;
has implementation of popular deep learning models for computer vision.&lt;/p&gt;
&lt;h2 id="resnet-50"&gt;ResNet-50&lt;/h2&gt;
&lt;p&gt;ResNet-50 is a 50-layer Residual Neural Network. &lt;/p&gt;
&lt;h2 id="resnet-101"&gt;ResNet 101&lt;/h2&gt;
&lt;p&gt;ResNet-101 is a 101-layer Residual Neural Network. &lt;/p&gt;
&lt;h2 id="resnet-152"&gt;ResNet 152&lt;/h2&gt;
&lt;p&gt;ResNet-152 is a 152-layer Residual Neural Network. &lt;/p&gt;
&lt;p&gt;&lt;a href="https://towardsdatascience.com/residual-network-implementing-resnet-a7da63c7b278"&gt;Residual Networks: Implementing ResNet in Pytorch&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="inceptionnets"&gt;InceptionNets&lt;/h2&gt;
&lt;h2 id="efficientnets"&gt;EfficientNets&lt;/h2&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;https://www.quora.com/What-is-the-deep-neural-network-known-as-%E2%80%9CResNet-50%E2%80%9D&lt;/p&gt;
&lt;p&gt;http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/1512.03385.pdf&lt;/p&gt;
&lt;p&gt;https://medium.com/@14prakash/understanding-and-implementing-architectures-of-resnet-and-resnext-for-state-of-the-art-image-cf51669e1624&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="computer vision"></category><category term="neural network"></category><category term="image classification"></category><category term="image processing"></category></entry><entry><title>Interpretation of Neural Networks</title><link href="https://misc.legendu.net/blog/interpretation-of-neural-networks/" rel="alternate"></link><published>2020-01-05T10:44:54-08:00</published><updated>2020-01-05T10:44:54-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-01-05:/blog/interpretation-of-neural-networks/</id><summary type="html">&lt;p&gt;**
Things on this page are fragmentary and immature notes/thoughts of the author.
Please read with your own judgement!&lt;/p&gt;
&lt;p&gt;**
Please refer to the Interpretations section of the wiki page
&lt;a href="https://en.wikipedia.org/wiki/Deep_learning"&gt;Deep Learning&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;https://github.com/slundberg/shap#methods-unified-by-shap&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/1802.03888.pdf&lt;/p&gt;
&lt;p&gt;https://github.com/slundberg/shap&lt;/p&gt;
&lt;h2 id="references"&gt;References …&lt;/h2&gt;</summary><content type="html">&lt;p&gt;**
Things on this page are fragmentary and immature notes/thoughts of the author.
Please read with your own judgement!&lt;/p&gt;
&lt;p&gt;**
Please refer to the Interpretations section of the wiki page
&lt;a href="https://en.wikipedia.org/wiki/Deep_learning"&gt;Deep Learning&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;https://github.com/slundberg/shap#methods-unified-by-shap&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/1802.03888.pdf&lt;/p&gt;
&lt;p&gt;https://github.com/slundberg/shap&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/Deep_learning&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="machine learning"></category><category term="data science"></category><category term="interpretation"></category></entry><entry><title>Distributed Training of Models on Spark</title><link href="https://misc.legendu.net/blog/distributed-training-of-models-on-spark/" rel="alternate"></link><published>2020-01-05T10:26:27-08:00</published><updated>2020-01-05T10:26:27-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-01-05:/blog/distributed-training-of-models-on-spark/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="xgboost"&gt;XGBoost&lt;/h2&gt;
&lt;p&gt;http://www.legendu.net/misc/blog/use-xgboost-with-spark/&lt;/p&gt;
&lt;h2 id="lightgbm"&gt;LightGBM&lt;/h2&gt;
&lt;p&gt;http://www.legendu.net/misc/blog/use-lightgbm-with-spark/&lt;/p&gt;
&lt;h2 id="bigdl"&gt;&lt;a href="https://github.com/intel-analytics/BigDL"&gt;BigDL&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="mmlspark"&gt;&lt;a href="https://github.com/Azure/mmlspark"&gt;MMLSpark&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="apache-ray"&gt;Apache Ray&lt;/h2&gt;
&lt;p&gt;You can run Apache Ray on top of Spark via 
&lt;a href="https://github.com/intel-analytics/analytics-zoo"&gt;analytics-zoo …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="xgboost"&gt;XGBoost&lt;/h2&gt;
&lt;p&gt;http://www.legendu.net/misc/blog/use-xgboost-with-spark/&lt;/p&gt;
&lt;h2 id="lightgbm"&gt;LightGBM&lt;/h2&gt;
&lt;p&gt;http://www.legendu.net/misc/blog/use-lightgbm-with-spark/&lt;/p&gt;
&lt;h2 id="bigdl"&gt;&lt;a href="https://github.com/intel-analytics/BigDL"&gt;BigDL&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="mmlspark"&gt;&lt;a href="https://github.com/Azure/mmlspark"&gt;MMLSpark&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="apache-ray"&gt;Apache Ray&lt;/h2&gt;
&lt;p&gt;You can run Apache Ray on top of Spark via 
&lt;a href="https://github.com/intel-analytics/analytics-zoo"&gt;analytics-zoo&lt;/a&gt;,
which enables you to run any Python machine lerning library in distributed fashion.
But I'm not sure whether this is a good idea.&lt;/p&gt;
&lt;h2 id="yahootensorflowonspark"&gt;&lt;a href="https://github.com/yahoo/TensorFlowOnSpark"&gt;yahoo/TensorFlowOnSpark&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="pytorch"&gt;PyTorch&lt;/h2&gt;
&lt;h2 id="h2o"&gt;H2O&lt;/h2&gt;
&lt;p&gt;https://github.com/h2oai/sparkling-water
http://docs.h2o.ai/sparkling-water/2.2/latest-stable/doc/pysparkling.html
http://h2o-release.s3.amazonaws.com/h2o/master/4273/docs-website/h2o-docs/faq/sparkling-water.html
https://docs.databricks.com/_static/notebooks/h2o-sparkling-water-python.html&lt;/p&gt;
&lt;h2 id="systemml"&gt;&lt;a href="https://github.com/apache/systemml"&gt;SystemML&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="elephas"&gt;&lt;a href="https://github.com/maxpumperla/elephas"&gt;elephas&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Distributed training with Keras and Spark.&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;https://towardsdatascience.com/deep-learning-with-apache-spark-part-1-6d397c16abd&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="Spark"></category><category term="distributed computing"></category></entry><entry><title>Clustering Algorithms in Machine Learning</title><link href="https://misc.legendu.net/blog/clustering-algorithms-machine-learning/" rel="alternate"></link><published>2013-03-03T09:21:45-08:00</published><updated>2020-01-03T09:21:45-08:00</updated><author><name>Ben Chuanlong Du</name></author><id>tag:misc.legendu.net,2013-03-03:/blog/clustering-algorithms-machine-learning/</id><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="centroid-based-clustering"&gt;Centroid-based Clustering&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;K-means Clustering&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;K-medians Clustering&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;K-mediods Clustering&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="hierarchical-clustering"&gt;Hierarchical Clustering&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Agglomerative Hierarchical Clustering&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Divisive Hierarchical Clustering&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="partional-clustering"&gt;Partional Clustering&lt;/h2&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="clustering"></category><category term="machine learning"></category><category term="data mining"></category></entry><entry><title>Metrics for Machine Learning</title><link href="https://misc.legendu.net/blog/metrics-for-machine-learning/" rel="alternate"></link><published>2020-01-01T13:25:08-08:00</published><updated>2020-01-01T13:25:08-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-01-01:/blog/metrics-for-machine-learning/</id><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="accuracy"&gt;Accuracy&lt;/h2&gt;
&lt;h2 id="precisionrecall"&gt;Precision/Recall&lt;/h2&gt;
&lt;h2 id="f1"&gt;F1&lt;/h2&gt;
&lt;h2 id="matthrews-correlation-coefficient-mcc"&gt;Matthrews Correlation Coefficient (MCC)&lt;/h2&gt;
&lt;h2 id="rocauc"&gt;ROC/AUC&lt;/h2&gt;
&lt;h2 id="mean-absolute-error-mae"&gt;Mean Absolute Error (MAE)&lt;/h2&gt;
&lt;h2 id="mean-squared-error-mse"&gt;Mean Squared Error (MSE)&lt;/h2&gt;
&lt;h2 id="root-mean-square-error-rmse"&gt;Root-mean-square Error (RMSE)&lt;/h2&gt;</content><category term="AI"></category><category term="AI"></category><category term="machine learning"></category><category term="data science"></category></entry><entry><title>Tips on TPU</title><link href="https://misc.legendu.net/blog/tips-on-tpu/" rel="alternate"></link><published>2020-01-01T11:01:05-08:00</published><updated>2020-01-01T11:01:05-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2020-01-01:/blog/tips-on-tpu/</id><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;https://cloud.google.com/tpu/docs/tutorials/resnet-alpha-py&lt;/p&gt;
&lt;p&gt;https://cloud.google.com/tpu/docs/tutorials&lt;/p&gt;
&lt;p&gt;https://towardsdatascience.com/running-pytorch-on-tpu-a-bag-of-tricks-b6d0130bddd4&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="TPU"></category></entry><entry><title>Regularization in Machine Learning Models</title><link href="https://misc.legendu.net/blog/regularization-in-machine-learning-models/" rel="alternate"></link><published>2019-12-30T12:21:11-08:00</published><updated>2019-12-30T12:21:11-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2019-12-30:/blog/regularization-in-machine-learning-models/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Regularization add a penalty term to the loss function in machine learning models.
The type of regularizatin depends on the type of penalty used 
(not the type of the objective function …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Regularization add a penalty term to the loss function in machine learning models.
The type of regularizatin depends on the type of penalty used 
(not the type of the objective function).
Below is a summary of commonly seen regularizations.&lt;/p&gt;
&lt;h2 id="mallows-cp"&gt;Mallow's Cp&lt;/h2&gt;
&lt;h2 id="adjusted-r2"&gt;Adjusted R^2&lt;/h2&gt;
&lt;h2 id="aic"&gt;AIC&lt;/h2&gt;
&lt;h2 id="bic"&gt;BIC&lt;/h2&gt;
&lt;h2 id="l1-regularization"&gt;L1 Regularization&lt;/h2&gt;
&lt;p&gt;L1 both hard and soft shrinkage (good if you can give a picture here)&lt;/p&gt;
&lt;p&gt;A regression with L1 regularization is also called LASSO regression.&lt;/p&gt;
&lt;h2 id="l2-regularization"&gt;L2 Regularization&lt;/h2&gt;
&lt;p&gt;L2 regularization results in soft shrinkage. &lt;/p&gt;
&lt;p&gt;Regression with L2 regularization is also called Ridge regression.&lt;/p&gt;
&lt;h2 id="lp-1-p-2-regularization"&gt;Lp (&lt;span class="math"&gt;\(1 &amp;lt; p &amp;lt; 2\)&lt;/span&gt;) Regularization&lt;/h2&gt;
&lt;h2 id="elastic-net-regularization"&gt;Elastic Net Regularization&lt;/h2&gt;
&lt;p&gt;Elastic Net Regularization is a lienar combination of L1 and L2 penalty.&lt;/p&gt;
&lt;h2 id="early-stopping"&gt;Early Stopping&lt;/h2&gt;
&lt;p&gt;In machine learning, 
early stopping is a form of regularization used to avoid overfitting 
when training a learner with an iterative method, such as gradient descent. 
Such methods update the learner so as to make it better fit the training data with each iteration. 
Up to a point, 
this improves the learner's performance on data outside of the training set. 
Past that point, 
however, 
improving the learner's fit to the training data comes at the expense of increased generalization error. 
Early stopping rules provide guidance as to how many iterations can be run before the learner begins to over-fit. 
Early stopping rules have been employed in many different machine learning methods, 
with varying amounts of theoretical foundation.&lt;/p&gt;
&lt;p&gt;Below are some regularization technique specific to neural networks.&lt;/p&gt;
&lt;h2 id="dropout"&gt;Dropout&lt;/h2&gt;
&lt;p&gt;Dropout is a regularization technique patented by Google 
for reducing overfitting in neural networks 
by preventing complex co-adaptations on training data. 
It is a very efficient way of performing model averaging with neural networks.
The term "dropout" refers to dropping out units (both hidden and visible) in a neural network.&lt;/p&gt;
&lt;h2 id="reference"&gt;Reference&lt;/h2&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js','mhchem.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI"></category><category term="AI"></category><category term="machine learning"></category><category term="data science"></category><category term="regularization"></category></entry><entry><title>Optimization Method in Machine Learning</title><link href="https://misc.legendu.net/blog/optimization-method-in-machine-learning/" rel="alternate"></link><published>2019-12-30T12:17:07-08:00</published><updated>2019-12-30T12:17:07-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2019-12-30:/blog/optimization-method-in-machine-learning/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;L-BFGS converges faster and with better solutions on small datasets. 
However, ADAM is very robust for relatively large datasets.
It usually converges quickly and gives pretty good performance. 
SGD with momentum …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;L-BFGS converges faster and with better solutions on small datasets. 
However, ADAM is very robust for relatively large datasets.
It usually converges quickly and gives pretty good performance. 
SGD with momentum or Nesterov momentum can perform better than those two algorithms 
if the learning rate is correctly tuned.
To sum up, 
&lt;strong&gt;ADAM is the best default optimization algorithm to use for deep learning problems&lt;/strong&gt;.
And SGD with Nesterov Momentum is another good alternative (if you are experience at tuning learning rate).&lt;/p&gt;
&lt;h3 id="sgd-stochastic-gradient-descent"&gt;SGD (Stochastic Gradient Descent)&lt;/h3&gt;
&lt;h3 id="adam-adaptive-moment-estimation"&gt;Adam (Adaptive Moment Estimation)&lt;/h3&gt;
&lt;p&gt;Adam realizes the benefits of both AdaGrad and RMSProp.&lt;/p&gt;
&lt;p&gt;Instead of adapting the parameter learning rates based on the average first moment (the mean) as in RMSProp, Adam also makes use of the average of the second moments of the gradients (the uncentered variance).&lt;/p&gt;
&lt;p&gt;Specifically, the algorithm calculates an exponential moving average of the gradient and the squared gradient, and the parameters beta1 and beta2 control the decay rates of these moving averages.&lt;/p&gt;
&lt;p&gt;The initial value of the moving averages and beta1 and beta2 values close to 1.0 (recommended) result in a bias of moment estimates towards zero. 
This bias is overcome by first calculating the biased estimates before then calculating bias-corrected estimates.&lt;/p&gt;
&lt;h3 id="sgd-nestrov-momentum"&gt;SGD + Nestrov Momentum&lt;/h3&gt;
&lt;h3 id="adagrad-adaptive-gradient-algorithm"&gt;AdaGrad (Adaptive Gradient Algorithm)&lt;/h3&gt;
&lt;h3 id="rmsprop-root-mean-square-propagation"&gt;(RMSProp) Root Mean Square Propagation&lt;/h3&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/&lt;/p&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="machine learning"></category><category term="data science"></category><category term="optimization algorithms"></category><category term="ADAM"></category><category term="SGD"></category><category term="momentum"></category></entry><entry><title>Tips on XGBoost</title><link href="https://misc.legendu.net/blog/tips-on-xgboost/" rel="alternate"></link><published>2019-12-28T23:22:29-08:00</published><updated>2019-12-28T23:22:29-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2019-12-28:/blog/tips-on-xgboost/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;It is suggested that you use the sklearn wrapper classes &lt;code&gt;XGBClassifier&lt;/code&gt; and &lt;code&gt;XGBRegressor&lt;/code&gt;
    so that you can fully leverage other tools of the sklearn package.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;There are 2 types of boosters …&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;It is suggested that you use the sklearn wrapper classes &lt;code&gt;XGBClassifier&lt;/code&gt; and &lt;code&gt;XGBRegressor&lt;/code&gt;
    so that you can fully leverage other tools of the sklearn package.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;There are 2 types of boosters (&lt;code&gt;gbtree&lt;/code&gt; and &lt;code&gt;gblinear&lt;/code&gt;), 
    It is suggested that you always use tree booster (&lt;code&gt;gbtree&lt;/code&gt;) 
    as it always outperformas the linear booster (&lt;code&gt;gblinear&lt;/code&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Instead of training, validation and testing sets,
    people typically use training and testing sets 
    and parameter tuning is done using cross validation based on the training data.
    The advantage of this way is that the data is better used. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Feature engineering and ensemble are usually more important 
    (in terms of improving model performance) than parameter tuning.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;split-by-leaf mode (grow_policy='lossguide') makes XGBoost run much faster.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="parameters"&gt;Parameters&lt;/h2&gt;
&lt;h3 id="general-parameters"&gt;General Parameters&lt;/h3&gt;
&lt;p&gt;It is suggested that you keep the default values for the general parameters 
(&lt;code&gt;booster&lt;/code&gt;, &lt;code&gt;silent&lt;/code&gt; and &lt;code&gt;nthrad&lt;/code&gt;).&lt;/p&gt;
&lt;h4 id="booster-defaultgbtree"&gt;booster [default=gbtree]&lt;/h4&gt;
&lt;p&gt;The booster parameter specifies the type of model to run. 
It has 2 options &lt;code&gt;gbtree&lt;/code&gt; (tree-based models) and &lt;code&gt;gblinear&lt;/code&gt; (linear models).
It is suggested that you keep the default value (&lt;code&gt;gbtree&lt;/code&gt;) 
as &lt;code&gt;gbtree&lt;/code&gt; always outperforms &lt;code&gt;gblinear&lt;/code&gt;.&lt;/p&gt;
&lt;h4 id="silent-default0"&gt;silent [default=0]&lt;/h4&gt;
&lt;p&gt;The silent mode is activated (no running messages will be printed) when the &lt;code&gt;silent&lt;/code&gt; parameter is set to 1.
It is suggested that you keep the default value (deactivate the silent mode)
which helps you understanding the model and training.&lt;/p&gt;
&lt;h4 id="nthread-default-to-maximum-number-of-threads-available"&gt;nthread [default to maximum number of threads available]&lt;/h4&gt;
&lt;p&gt;Number of threads to be used in training.
It is suggested that you keep the default value.&lt;/p&gt;
&lt;h3 id="booster-parameters"&gt;Booster Parameters&lt;/h3&gt;
&lt;p&gt;The following is focused on gbtree only
as it always outperforms &lt;code&gt;gblinear&lt;/code&gt;.&lt;/p&gt;
&lt;h4 id="eta-learn_rate-default03"&gt;&lt;code&gt;eta&lt;/code&gt;, &lt;code&gt;learn_rate&lt;/code&gt; [default=0.3]&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;eta&lt;/code&gt; is analogous to learning rate.
The typical final values to be used is between 0.01 and 0.2.
It is suggested that you tune the value using cross validation.&lt;/p&gt;
&lt;h4 id="min_child_weight-default1"&gt;* min_child_weight [default=1]&lt;/h4&gt;
&lt;p&gt;Defines the minimum sum of weights of all observations required in a child.
It is used to control over-fitting. 
Higher values prevent a model from learning relations 
(which might be highly specific to the particular sample selected for a tree)
while too high values can lead to under-fitting hence.
It is suggested that you tune the value using cross validation.&lt;/p&gt;
&lt;h4 id="max_depth-default6"&gt;* max_depth [default=6]&lt;/h4&gt;
&lt;p&gt;The maximum depth of a tree, same as GBM.
Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.
Should be tuned using CV.
Typical values: 3-10&lt;/p&gt;
&lt;h4 id="max_leaf_nodes"&gt;max_leaf_nodes&lt;/h4&gt;
&lt;p&gt;The maximum number of terminal nodes or leaves in a tree.
Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.
If this is defined, GBM will ignore max_depth.&lt;/p&gt;
&lt;h4 id="gamma-default0"&gt;gamma [default=0]&lt;/h4&gt;
&lt;p&gt;A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.
Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.&lt;/p&gt;
&lt;h4 id="max_delta_step-default0"&gt;max_delta_step [default=0]&lt;/h4&gt;
&lt;p&gt;In maximum delta step we allow each tree’s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative.
Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced.
This is generally not used but you can explore further if you wish.&lt;/p&gt;
&lt;h4 id="subsample-default1"&gt;subsample [default=1]&lt;/h4&gt;
&lt;p&gt;Same as the subsample of GBM. Denotes the fraction of observations to be randomly samples for each tree.
Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting.
Typical values: 0.5-1&lt;/p&gt;
&lt;h4 id="colsample_bytree-default1"&gt;colsample_bytree [default=1]&lt;/h4&gt;
&lt;p&gt;Similar to max_features in GBM. Denotes the fraction of columns to be randomly samples for each tree.
Typical values: 0.5-1&lt;/p&gt;
&lt;h4 id="colsample_bylevel-default1"&gt;colsample_bylevel [default=1]&lt;/h4&gt;
&lt;p&gt;Denotes the subsample ratio of columns for each split, in each level.
I don’t use this often because subsample and colsample_bytree will do the job for you. but you can explore further if you feel so.&lt;/p&gt;
&lt;h4 id="lambda-default1"&gt;lambda [default=1]&lt;/h4&gt;
&lt;p&gt;L2 regularization term on weights (analogous to Ridge regression)
This used to handle the regularization part of XGBoost. Though many data scientists don’t use it often, it should be explored to reduce overfitting.&lt;/p&gt;
&lt;h4 id="alpha-default0"&gt;alpha [default=0]&lt;/h4&gt;
&lt;p&gt;L1 regularization term on weight (analogous to Lasso regression)
Can be used in case of very high dimensionality so that the algorithm runs faster when implemented&lt;/p&gt;
&lt;h4 id="scale_pos_weight-default1"&gt;scale_pos_weight [default=1]&lt;/h4&gt;
&lt;p&gt;A value greater than 0 should be used in case of high class imbalance as it helps in faster convergence.&lt;/p&gt;
&lt;h3 id="number-of-decision-trees-n_estimators"&gt;Number of Decision Trees (&lt;code&gt;n_estimators&lt;/code&gt;)&lt;/h3&gt;
&lt;h3 id="max_features"&gt;max_features&lt;/h3&gt;
&lt;h3 id="depth-of-decision-trees-max_depth"&gt;Depth of Decision Trees (&lt;code&gt;max_depth&lt;/code&gt;)&lt;/h3&gt;
&lt;h3 id="early-stopping-early_stopping_rounds"&gt;Early Stopping (&lt;code&gt;early_stopping_rounds&lt;/code&gt;)&lt;/h3&gt;
&lt;p&gt;Prevent overfitting.&lt;/p&gt;
&lt;h3 id="subsample"&gt;subsample&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;subsample&lt;/code&gt; is for each tree the percentage of rows taken to build the tree. 
I recommend not taking out too many rows, 
as performance will drop a lot. Take values from 0.8 to 1.&lt;/p&gt;
&lt;h3 id="colsample_bytree"&gt;colsample_bytree&lt;/h3&gt;
&lt;p&gt;Maximum percentage of features used by each tree.&lt;/p&gt;
&lt;p&gt;number of columns used by each tree. In order to avoid some columns to take too much credit for the prediction (think of it like in recommender systems when you recommend the most purchased products and forget about the long tail), take out a good proportion of columns. Values from 0.3 to 0.8 if you have many columns (especially if you did one-hot encoding), or 0.8 to 1 if you only have a few columns.&lt;/p&gt;
&lt;h2 id="booster-parameters_1"&gt;Booster Parameters&lt;/h2&gt;
&lt;h3 id="gamma"&gt;gamma&lt;/h3&gt;
&lt;p&gt;A regularization parameter. 
Either 0, 1 or 5.&lt;/p&gt;
&lt;h2 id="xgbclassifierfit"&gt;&lt;a href="https://xgboost.readthedocs.io/en/latest/python/python_api.html?highlight=classifier#xgboost.XGBClassifier.fit"&gt;XGBClassifier.fit&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The method &lt;code&gt;XGBClassifier.fit&lt;/code&gt; takes a parameter &lt;code&gt;eval_set&lt;/code&gt;
which is a list of tuples of the format (x, y).
Notice that this parameter is NOT used for grid search but for tracking performance for early stopping only.
If &lt;code&gt;eval_set&lt;/code&gt; contains multiple datasets,
then only the last dataset is used for early stopping.&lt;/p&gt;
&lt;h2 id="set-parameters-for-xgbclassifier-or-xgbregressor"&gt;Set Parameters for XGBClassifier or XGBRegressor&lt;/h2&gt;
&lt;p&gt;It is suggested that you use the method &lt;code&gt;set_params&lt;/code&gt; to set parameters after creating a model
instead of setting parameter on model creation.&lt;/p&gt;
&lt;p&gt;https://stackoverflow.com/questions/34674797/xgboost-xgbclassifier-defaults-in-python&lt;/p&gt;
&lt;h2 id="parameter-tuning"&gt;Parameter Tuning&lt;/h2&gt;
&lt;p&gt;https://sites.google.com/view/lauraepp/parameters&lt;/p&gt;
&lt;p&gt;https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/&lt;/p&gt;
&lt;h2 id="gpu"&gt;GPU&lt;/h2&gt;
&lt;p&gt;https://xgboost.readthedocs.io/en/latest/gpu/&lt;/p&gt;
&lt;h2 id="questions"&gt;Questions&lt;/h2&gt;
&lt;p&gt;Any difference between xgboost.cv and the cross validation in sklearn?
which one should I use?&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;https://xgboost.readthedocs.io/en/latest/parameter.html#xgboost-parameters&lt;/p&gt;
&lt;p&gt;https://github.com/dmlc/xgboost/tree/master/demo/guide-python&lt;/p&gt;
&lt;p&gt;https://machinelearningmastery.com/tune-number-size-decision-trees-xgboost-python/&lt;/p&gt;
&lt;p&gt;https://towardsdatascience.com/fine-tuning-xgboost-in-python-like-a-boss-b4543ed8b1e&lt;/p&gt;
&lt;p&gt;https://towardsdatascience.com/a-beginners-guide-to-xgboost-87f5d4c30ed7&lt;/p&gt;
&lt;p&gt;https://www.datacamp.com/community/tutorials/xgboost-in-python&lt;/p&gt;
&lt;p&gt;https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="XGBoost"></category></entry><entry><title>Libraries for Gradient Boosting</title><link href="https://misc.legendu.net/blog/libraries-for-gradient-boosting/" rel="alternate"></link><published>2019-12-24T15:19:42-08:00</published><updated>2019-12-24T15:19:42-08:00</updated><author><name>Benjamin Du</name></author><id>tag:misc.legendu.net,2019-12-24:/blog/libraries-for-gradient-boosting/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="xgboost"&gt;XGBoost&lt;/h2&gt;
&lt;p&gt;https://xgboost.ai/&lt;/p&gt;
&lt;p&gt;&lt;a href="https://xgboost.readthedocs.io/en/latest/"&gt;XGBoost Documentation&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="speedup-xgboost"&gt;Speedup XGBoost&lt;/h2&gt;
&lt;p&gt;https://machinelearningmastery.com/best-tune-multithreading-support-xgboost-python/&lt;/p&gt;
&lt;p&gt;https://medium.com/data-design/xgboost-gpu-performance-on-low-end-gpu-vs-high-end-cpu-a7bc5fcd425b&lt;/p&gt;
&lt;p&gt;xgboost GPU is fast. 
Very fast. 
As long as it fits in RAM and …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="xgboost"&gt;XGBoost&lt;/h2&gt;
&lt;p&gt;https://xgboost.ai/&lt;/p&gt;
&lt;p&gt;&lt;a href="https://xgboost.readthedocs.io/en/latest/"&gt;XGBoost Documentation&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="speedup-xgboost"&gt;Speedup XGBoost&lt;/h2&gt;
&lt;p&gt;https://machinelearningmastery.com/best-tune-multithreading-support-xgboost-python/&lt;/p&gt;
&lt;p&gt;https://medium.com/data-design/xgboost-gpu-performance-on-low-end-gpu-vs-high-end-cpu-a7bc5fcd425b&lt;/p&gt;
&lt;p&gt;xgboost GPU is fast. 
Very fast. 
As long as it fits in RAM and you do not care about getting reproducible results (and getting crashes).
To keep getting those epic, 
stable and reproducible results (or if data is just too big for GPU RAM), 
keep using the CPU. There’s no real workaround (yet).&lt;/p&gt;
&lt;h2 id="lightgbm"&gt;&lt;a href="https://github.com/microsoft/LightGBM"&gt;LightGBM&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id="catboost"&gt;CatBoost&lt;/h2&gt;
&lt;p&gt;https://catboost.ai/news/best-in-class-inference-and-a-ton-of-speedups&lt;/p&gt;
&lt;h2 id="comparisons"&gt;Comparisons&lt;/h2&gt;
&lt;p&gt;The paper &lt;a href="https://arxiv.org/pdf/1809.04559.pdf"&gt;Benchmarking and Optimization of Gradient Boosting Decision Tree Algorithms&lt;/a&gt;
compares the 3 libraries from the 3 perspectives/questions below
and claims that there is no clear win among the 3 libraries.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;How&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;much&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;acceleration&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;can&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;be&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;exp&lt;/span&gt;&lt;span class="n"&gt;ected&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;when&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;using&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;GPU&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;based&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;routines&lt;/span&gt;&lt;span class="err"&gt;?&lt;/span&gt;
&lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;How&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;well&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;does&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;this&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;GPU&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;acceleration&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;translate&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kr"&gt;to&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;reduced&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="kr"&gt;to&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;solution&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;in&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kr"&gt;cont&lt;/span&gt;&lt;span class="n"&gt;ext&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;of&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Bayesian&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;hyper&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;parameter&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;optimization&lt;/span&gt;&lt;span class="err"&gt;?&lt;/span&gt;
&lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;How&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;well&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;do&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;resulting&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;models&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;generalize&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kr"&gt;to&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;unseen&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kd"&gt;data&lt;/span&gt;&lt;span class="err"&gt;?&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The article 
&lt;a href="https://medium.com/kaggle-nyc/gradient-boosting-decision-trees-xgboost-vs-lightgbm-and-catboost-72df6979e0bb"&gt;Gradient Boosting Decision trees: XGBoost vs LightGBM (and catboost)&lt;/a&gt;
claims that LightGBM improves on XGBoost.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In summary, 
LightGBM improves on XGBoost. 
The LightGBM paper uses XGBoost as a baseline and outperforms it in training speed and the dataset sizes it can handle. 
The accuracies are comparable. 
LightGBM in some cases reaches it’s top accuracy in under a minute and while only reading a fraction of the whole dataset. 
This goes to show the power of approximation algorithms 
and intelligently sampling a dataset to extract the most information as fast as possible.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The article &lt;a href="https://blog.griddynamics.com/xgboost-vs-catboost-vs-lightgbm-which-is-best-for-price-prediction/"&gt;Xgboost vs Catboost vs Lightgbm: which is best for price prediction?&lt;/a&gt;
also claims that LightGMB was the clear winner in terms of speed
and XGBoost was the clear winner in terms of model accuracy.
(However, 
people on Kaggles says that speed is the most important thing in Kaggle competitions
as you can try more features and model iterations 
which eventually lead to higher model accuracy).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As the table above demonstrates, 
lightgbm was the clear winner in terms of speed, 
consistently outperforming catboost and xgboost. 
In terms of model accuracy, 
xgboost was the clear winner in both GridSearchCV and RandomizedSearchCV, 
with the lowest root mean squared error. 
For early stopping, 
lightgbm was the winner, 
with a slightly lower root mean squared error than xgboost.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;CatBoost is seldomly used in Kaggle competitions.
LightGBM is faster than XGBoost and is used more and more in Kaggle competitions.
If you need a mature and stable solution,
XGBoost is the right choice.
If you need high speed,
LightGBM is the way to go.&lt;/p&gt;
&lt;p&gt;https://www.kaggle.com/c/LANL-Earthquake-Prediction/discussion/89909&lt;/p&gt;
&lt;p&gt;https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db&lt;/p&gt;
&lt;p&gt;https://datascience.stackexchange.com/questions/49567/lightgbm-vs-xgboost-vs-catboost&lt;/p&gt;
&lt;p&gt;https://medium.com/data-design/getting-the-most-of-xgboost-and-lightgbm-speed-compiler-cpu-pinning-374c38d82b86&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category><category term="machine learning"></category><category term="data science"></category><category term="gradient boosting"></category><category term="XGBoost"></category><category term="LightGMB"></category><category term="CatBoost"></category></entry><entry><title>Ensemble Machine Learning Models</title><link href="https://misc.legendu.net/blog/ai-ensemble/" rel="alternate"></link><published>2013-03-24T13:51:12-07:00</published><updated>2019-12-24T13:51:12-08:00</updated><author><name>Ben Chuanlong Du</name></author><id>tag:misc.legendu.net,2013-03-24:/blog/ai-ensemble/</id><summary type="html">&lt;p&gt;The prediction error is a trade-off of bias and variance. 
In statistics, 
we often talk about unbiased estimators (especially in linear regression). 
In this case we restrict the estimators/predictors to be in a (small) class,
and find the optimal solution in this class (called BLUE or BLUP).&lt;/p&gt;
&lt;p&gt;Generally speaking …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The prediction error is a trade-off of bias and variance. 
In statistics, 
we often talk about unbiased estimators (especially in linear regression). 
In this case we restrict the estimators/predictors to be in a (small) class,
and find the optimal solution in this class (called BLUE or BLUP).&lt;/p&gt;
&lt;p&gt;Generally speaking, unbiased predictors are not ideas in prediction problems.
In many situations we can easily find predictors with small bias (usually very complexed predictors/models). 
To improve the performance of predictors, 
we often want to decrease their variance at the cost of increase bias. 
This is called decreasing the complexity of the predictors/models.
This can be done by variable/feature selection. 
This is discussed in 
&lt;a href="http://www.legendu.net/misc/blog/regularization-in-machine-learning-models/"&gt;Regularization in Machine Learning Models&lt;/a&gt;
.
Here we talk about some ensamble technics to decrease the complexity of predictors/models.
The is inspired by the simple fact that the variance of the mean of &lt;code&gt;n&lt;/code&gt; i.i.d random variables (with finite variance)
is &lt;span class="math"&gt;\(\frac{1}{n}\)&lt;/span&gt; times the variance of the population of those random variables.
This means that by combining (not strongly correlated) predictors,
e.g., linear combination or majority vote, 
we can greatly reduce the variance and thus achieve better prediction accuracy.&lt;/p&gt;
&lt;h2 id="decrease-complexity"&gt;Decrease Complexity&lt;/h2&gt;
&lt;h3 id="bagging"&gt;Bagging&lt;/h3&gt;
&lt;p&gt;Bagging is short for Bootstrap Aggregation.
The basic idea is to build predictors based on bootstrap samples (sample with replacement) of the training data. 
Each time we get a different bootstrap sample,
so we have a different predictor.
Hopefully these predictors are not strongly correlated 
so that by averaging (unweighted) them we get a better predictor.
This idea is well demonstrated in random forest. 
To further decrease the correlation between trees, 
the random forest process also restricts the number of variables 
(usually square root of the total number of variables) used to build each tree.
The neuron network can also be considered as a way of Bagging,
since the input of a node is a linear combination of outcomes from last layer.&lt;/p&gt;
&lt;h3 id="stacking"&gt;Stacking&lt;/h3&gt;
&lt;p&gt;Stacking is similar to Bagging. 
The difference is that Stacking uses weighted averages of predictors based on their performances. 
The weights are often chosen to minized the prediction error in leave-1-out cross validation. &lt;/p&gt;
&lt;h2 id="increase-complexity"&gt;Increase Complexity&lt;/h2&gt;
&lt;p&gt;An opposite approach to these discussed above is to start from a very simple predictor (big bias but small variance)
and then increase the complexity (decrease bias at cost of increase variance) of the predictor.
This is common in nature, e.g., the human brain is developped from simple to complex. 
This approach is called Boosting.
Boosting algorithms is an iterative functional gradient descent algorithms. 
That is, algorithms that optimize a cost function 
over function space by iteratively choosing a function (weak hypothesis) 
that points in the negative gradient direction. 
There are different version of Boosting 
among which AdaBoosting (Adaptive Boosting) and Gradient Boosting are the most 2 poular ones. 
AdaBoosting is popular in face recognition problems.
Gradient Boosting models dominates traditional (non-deep) machine learning.
&lt;a href="https://github.com/dmlc/xgboost"&gt;XGBoost&lt;/a&gt;
is a popular implementation of the gradient boosting framework.&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/Boosting_(machine_learning)&lt;/p&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/Gradient_boosting&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js','mhchem.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI"></category><category term="AI"></category><category term="statistics"></category><category term="ensamble"></category><category term="machine learning"></category><category term="data science"></category></entry><entry><title>Tools for A/B Testing</title><link href="https://misc.legendu.net/blog/tools-for-ab-testing/" rel="alternate"></link><published>2018-05-27T16:39:55-07:00</published><updated>2018-05-27T16:39:55-07:00</updated><author><name>Ben Chuanlong Du</name></author><id>tag:misc.legendu.net,2018-05-27:/blog/tools-for-ab-testing/</id><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.evanmiller.org/ab-testing/"&gt;Evan’s Awesome A/B Tools&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;http://www.statsmodels.org/stable/stats.html#module-statsmodels.stats.power&lt;/p&gt;</content><category term="AI"></category><category term="statistics"></category><category term="A/B testing"></category><category term="AB testing"></category></entry><entry><title>Statistics</title><link href="https://misc.legendu.net/blog/statistics-tips/" rel="alternate"></link><published>2013-10-13T22:40:14-07:00</published><updated>2016-07-13T22:40:14-07:00</updated><author><name>Ben Chuanlong Du</name></author><id>tag:misc.legendu.net,2013-10-13:/blog/statistics-tips/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;in real problems, the true value of (some quantity) are usually unknow/defined,
even if we can collect data about it,
e.g., Amazon goods, what is the true cost of …&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;in real problems, the true value of (some quantity) are usually unknow/defined,
even if we can collect data about it,
e.g., Amazon goods, what is the true cost of a good?
You must first give a definition ...&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;it seems that many problems are related to survey (as long as data collection)
though data might not be good, we can correct for sampling bias
the way to deal with unbalanced data in logistic regression is inspring ...&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;a) weight minor class (over-sampling), recommended&lt;/p&gt;
&lt;p&gt;b) under-sampling, throw away some observations in the larger group
hard to do model validation&lt;/p&gt;
&lt;p&gt;c) use similar idea as random forest, bagging ...
similar to give more weights to minor group
sample 90% of minor group and some part of larger group (approximately equal obs)
repeat this ..., inherit many pros of random forest ...&lt;/p&gt;
&lt;p&gt;some people recommend random forest as a better alternative to logistic 
regression when data is unbalanced, but random forest need more data than logistic regression&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;if model is not good, adjust it to correct bias ...
use simple rules, better than nothing ...&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;general effect of unbalance data? bernoulli example, central limit theorem ... confidence
interval, simulation and so on ...&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ways people deal with extremely unbalanced data in business ...&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;when you solve a stat problem,
the first thing to do is to get clear about your purpose, 
what the goal of the project?
e.g., the bayesian, portfolio, loan, PD example, one was talking about conservative while 
I thought about correctness&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;模型里面，multinomial还有一种是保留顺序的，更好！&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;complex model in the sake of complex won't help, 
but complex model to address real problems helps.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="AI"></category><category term="tips"></category><category term="statistics"></category><category term="modele"></category><category term="modeling"></category></entry><entry><title>Bayesian vs Frequentist</title><link href="https://misc.legendu.net/blog/bayesian-vs-frequentist/" rel="alternate"></link><published>2014-12-13T22:22:54-08:00</published><updated>2016-07-13T22:22:54-07:00</updated><author><name>Ben Chuanlong Du</name></author><id>tag:misc.legendu.net,2014-12-13:/blog/bayesian-vs-frequentist/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;很多frequentist的inference是基于大样本（近似）理论的，这种情况下样本小的时候Bayesian结论可能会更好。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;某些统计量实在太复杂，（近 …&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;很多frequentist的inference是基于大样本（近似）理论的，这种情况下样本小的时候Bayesian结论可能会更好。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;某些统计量实在太复杂，（近似）分布很难推到。而Bayesian分析本质上就是模拟，简单直接有效，除了速度可能会很慢。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;某些问题时就是从Bayesian的框架（一堆conditional distribution conditions)提出的，这种问题就是为Bayesian量身裁衣的。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</content><category term="AI"></category><category term="statistics"></category><category term="Bayesian"></category><category term="frequentist"></category><category term="large sample-based"></category><category term="likelihood"></category><category term="asymptotic"></category><category term="simulation"></category></entry><entry><title>Make Inference Using Bootstrap</title><link href="https://misc.legendu.net/blog/make-inference-using-bootstrap/" rel="alternate"></link><published>2016-03-05T20:04:20-08:00</published><updated>2016-03-05T20:04:20-08:00</updated><author><name>Ben Chuanlong Du</name></author><id>tag:misc.legendu.net,2016-03-05:/blog/make-inference-using-bootstrap/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;http://www.stat.umn.edu/geyer/old/5601/examp/tests.html#pv&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(BC_{\alpha}\)&lt;/span&gt; confidence interval
the way to calculate p-values ...&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak …&lt;/script&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;http://www.stat.umn.edu/geyer/old/5601/examp/tests.html#pv&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(BC_{\alpha}\)&lt;/span&gt; confidence interval
the way to calculate p-values ...&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js','mhchem.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI"></category><category term="statistics"></category><category term="bootstrap"></category><category term="inference"></category><category term="resample"></category></entry><entry><title>AR1X Process</title><link href="https://misc.legendu.net/blog/ar1x-process/" rel="alternate"></link><published>2015-03-13T15:26:10-07:00</published><updated>2015-03-13T15:26:10-07:00</updated><author><name>Ben Chuanlong Du</name></author><id>tag:misc.legendu.net,2015-03-13:/blog/ar1x-process/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;talk about the ar1x process, and your simulation, 
GLM might not be as good as OLS for estimating parameters, but be careful about inference, why? 
you use do inference based on …&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;talk about the ar1x process, and your simulation, 
GLM might not be as good as OLS for estimating parameters, but be careful about inference, why? 
you use do inference based on simulation ...
more complicated algorithm based on optimization ...&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Several different names are used to describe ARIMA models with input series. Transfer function model, intervention model, interrupted time series model, regression model with ARMA errors, Box-Tiao model, and ARIMAX model are all different names for ARIMA models with input series. Pankratz (1991) refers to these models as dynamic regression models.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;TSA::arimax in R, but the documentation is poor. I even don't know which model specification it uses.
The function was implemented by Kung-Sik Chan in University of Iowa. 
You can contact him and get clear about the model specification.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;phone : (319) 335-2849
e-mail: kchan@stat.uiowa.edu&lt;/p&gt;
&lt;p&gt;http://stackoverflow.com/questions/25224155/transfer-function-models-arimax-in-tsa&lt;/p&gt;
&lt;p&gt;http://www.r-bloggers.com/the-arimax-model-muddle/&lt;/p&gt;
&lt;p&gt;http://econometricsense.blogspot.com/2012/01/time-series-intervention-analysis-wih-r.html&lt;/p&gt;
&lt;p&gt;http://econometricsense.blogspot.com/2012/01/intervention-analysis.html&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;SAS
Though the following post allows differencing the response variable, it doesn't allow an arbitrary coefficient.
http://support.sas.com/documentation/cdl/en/etsug/63348/HTML/default/viewer.htm#etsug_arima_sect012.htm&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You can actually estimate the parameters using MLE by yourself. It seems to be very easy.
And you can derive all the theories for it.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I did the math, and it seems to me the MLE (under the assumption of normal) is the same as the OLS estimate,
however, the (asymptoic) distribution of estimate of parameters are much more complicated. 
I have 2 ideas to make inferences. &lt;/p&gt;</content><category term="AI"></category><category term="statistics"></category><category term="AR1X"></category><category term="time series"></category><category term="linear regression"></category><category term="model"></category></entry><entry><title>Variance and Dispersion Estimate in Genetics</title><link href="https://misc.legendu.net/blog/variance-and-dispersion-estimate-in-genetics/" rel="alternate"></link><published>2013-03-24T14:05:04-07:00</published><updated>2015-02-24T14:05:04-08:00</updated><author><name>Ben Chuanlong Du</name></author><id>tag:misc.legendu.net,2013-03-24:/blog/variance-and-dispersion-estimate-in-genetics/</id><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The basic idea is to borrow information across genes. 
Some assume a distribution for the variances/dispersions while others not.&lt;/p&gt;</content><category term="AI"></category><category term="statistics"></category><category term="genetics"></category><category term="biostatistics"></category><category term="variance"></category><category term="dispersion"></category><category term="estimate"></category><category term="estimation"></category></entry><entry><title>Least Square Estimates</title><link href="https://misc.legendu.net/blog/pros-and-cons-of-least-square-estimates/" rel="alternate"></link><published>2013-08-24T13:58:18-07:00</published><updated>2015-02-24T13:58:18-08:00</updated><author><name>Ben Chuanlong Du</name></author><id>tag:misc.legendu.net,2013-08-24:/blog/pros-and-cons-of-least-square-estimates/</id><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="ordinary-least-square"&gt;Ordinary Least Square&lt;/h2&gt;
&lt;h2 id="weighted-least-sqaure"&gt;Weighted Least Sqaure&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Heterogeneous variance&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;close results to transformation&lt;/p&gt;</content><category term="AI"></category><category term="linear regression"></category><category term="least square"></category><category term="modeling"></category><category term="statisitcs"></category></entry><entry><title>Matrix Decomposition</title><link href="https://misc.legendu.net/blog/matrix-decomposition/" rel="alternate"></link><published>2013-04-24T23:14:16-07:00</published><updated>2013-10-24T23:14:16-07:00</updated><author><name>Ben Chuanlong Du</name></author><id>tag:misc.legendu.net,2013-04-24:/blog/matrix-decomposition/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="qr-decomposition"&gt;QR Decomposition&lt;/h2&gt;
&lt;p&gt;The QR decomposition uses the Gram-Schmidt process to express a set of vectors 
(columns of design matrix from statistical view) in a set of orthogonal unit vecotors (which means …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="qr-decomposition"&gt;QR Decomposition&lt;/h2&gt;
&lt;p&gt;The QR decomposition uses the Gram-Schmidt process to express a set of vectors 
(columns of design matrix from statistical view) in a set of orthogonal unit vecotors (which means that Q is matrix with orthogonal unit vectors) 
so that for any &lt;code&gt;k&lt;/code&gt;, 
the space spanned by the first &lt;code&gt;k&lt;/code&gt; original vectors is the same as the space spanned by the first &lt;code&gt;k&lt;/code&gt; orthogonal unit vectors. 
The ith column of R is the way that the ith original vector can be expressed in the first &lt;code&gt;i&lt;/code&gt; orthogonal unit vectors.&lt;br&gt;
This means that R is an upper (right) triangular matrix.&lt;/p&gt;
&lt;p&gt;More specifically, 
For any realy matrix &lt;span class="math"&gt;\(X_{n\times p}\)&lt;/span&gt;, there &lt;span class="math"&gt;\(Q_{n\times p}\)&lt;/span&gt; and &lt;span class="math"&gt;\(R_{p\times p}\)&lt;/span&gt; such that 
    (X_{n\times p} = Q_{n\times p} R_{p\times p}),
where &lt;span class="math"&gt;\(Q\)&lt;/span&gt; is a matrix with orthogonal unit column vectors, 
i.e., &lt;span class="math"&gt;\(Q'Q=I\)&lt;/span&gt;; &lt;span class="math"&gt;\(R\)&lt;/span&gt; is an upper (right) triangular matrix.
QR decomposition is often done throught Gram-Schmidt process. 
Given a vector &lt;span class="math"&gt;\(x_i\)&lt;/span&gt;, 
the Gram-Schmidt process produce a (unit) vector &lt;span class="math"&gt;\(e_i\)&lt;/span&gt; that is orthogonal to previously produced orthogonal vectors &lt;span class="math"&gt;\(\{e_1,...,e_{i-1}\}\)&lt;/span&gt;. 
This is done by subtract the projection of &lt;span class="math"&gt;\(e_i\)&lt;/span&gt; to the space spanned by &lt;span class="math"&gt;\(\{e_1,...,e_{i-1}\}\)&lt;/span&gt; from &lt;span class="math"&gt;\(e_i\)&lt;/span&gt; (and then normalize it).
For any &lt;span class="math"&gt;\(i\)&lt;/span&gt;, 
the spaces spanned by &lt;span class="math"&gt;\(\{x_1, ..., x_i\}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\{e_1, ..., e_i\}\)&lt;/span&gt; are the same. 
The &lt;span class="math"&gt;\(i^{th}\)&lt;/span&gt; column of &lt;span class="math"&gt;\(R\)&lt;/span&gt; is the way that &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; can be expressed by &lt;span class="math"&gt;\(\{e_1, ..., e_i\}\)&lt;/span&gt;,
i.e., the first &lt;span class="math"&gt;\(i\)&lt;/span&gt; columns of &lt;span class="math"&gt;\(Q\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;QR decomposition can be used to&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;determine the rank of a matrix. 
For example, to find the rank of a matrix X in R, you can use command &lt;code&gt;qr(X)$rank&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;find estimates of coefficients in linear regression, 
which is 
        (\hat{\beta} = R^{-1}Q'\boldsymbol{y}).
The fact that R is an upper (right) triangular matrix make it easy to find its inverse. &lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="singular-value-decomposition"&gt;Singular Value Decomposition&lt;/h2&gt;
&lt;p&gt;For any real matrix &lt;span class="math"&gt;\(X_{n\times p}\)&lt;/span&gt; of rank &lt;span class="math"&gt;\(r\)&lt;/span&gt;,
there exists matrix &lt;span class="math"&gt;\(U_{n\times r}\)&lt;/span&gt;, &lt;span class="math"&gt;\(D_{r\times r}\)&lt;/span&gt; and &lt;span class="math"&gt;\(V_{p\times r}\)&lt;/span&gt; such that
    (X=UDV'),
where &lt;span class="math"&gt;\(U\)&lt;/span&gt; is a matrix with orthogonal unit column vectors spanning the column space of X;
&lt;span class="math"&gt;\(D\)&lt;/span&gt; is a diagonal matrix with positive (decreasing) diagonal elements; 
&lt;span class="math"&gt;\(V\)&lt;/span&gt; is a matrix with orthogona unit row vectors spanning the row space of X.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;If we expand &lt;span class="math"&gt;\(U\)&lt;/span&gt; and &lt;span class="math"&gt;\(V\)&lt;/span&gt; to be squared matrixes (adding 0's into the diagonal elements of &lt;span class="math"&gt;\(D\)&lt;/span&gt;),
the SVD can be explained as to use rotation, scaling and rotation to express any linear map &lt;span class="math"&gt;\(X\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(X\)&lt;/span&gt; maps a unit sphere into a ellipsoid. 
The singular values are the semiaxes of the ellipsoid.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The singular values are square root of the eigenvalues of &lt;span class="math"&gt;\(X'X\)&lt;/span&gt; and &lt;span class="math"&gt;\(XX'\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If &lt;span class="math"&gt;\(X\)&lt;/span&gt; is a square matrix and has eigenvalue decomposition,
then the singular values are absolute eigenvalues of &lt;span class="math"&gt;\(X\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If &lt;span class="math"&gt;\(X\)&lt;/span&gt; is a nonnegative definite matrix, 
then the SVD is the eigenvalue decomposition.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The SVD can be rewritten as a weighted (singular values are the weights) sum of separable matrixes.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The SVD can be used to&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;find the rank of the matrix &lt;span class="math"&gt;\(X\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;find pseudoinverse inverse of the matirx &lt;span class="math"&gt;\(X\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;find lower rank approximation of the matrix &lt;span class="math"&gt;\(X\)&lt;/span&gt;. 
This is one way to reduce dimension in statistics.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="eigendecomposition-spectral-decomposition"&gt;Eigendecomposition (Spectral Decomposition)&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Only diagonalizable matrices have eigendecompostions.
The geometric dimension of an eigenvalue of a matrix is less than or equal to its algebraic dimension.
A matrix is diagonalizable iff the geometric dimension is the same as the algebraic dimension for each eigenvalue.
Nonnegative definite matrices are diagonalizable.
Specially project matrices are diagonalizable (with eigenvalues 0 and 1).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For a Markov chain process, 
a stationary distribution is an eigenvalue of the transition matrix with respect to the eigenvalue 1.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The eigendecomposition can be used to calculate the power of diagonalizable matrices.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Eigendecomposition is used in principal components analysis. &lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js','mhchem.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI"></category><category term="statistics"></category><category term="math"></category><category term="algebra"></category><category term="decomposition"></category><category term="matrix"></category></entry><entry><title>Cross Validation in Machine Learning</title><link href="https://misc.legendu.net/blog/cross-validation-machine-learning/" rel="alternate"></link><published>2013-04-03T00:00:00-07:00</published><updated>2013-04-03T00:00:00-07:00</updated><author><name>Ben Chuanlong Du</name></author><id>tag:misc.legendu.net,2013-04-03:/blog/cross-validation-machine-learning/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="training-and-testing-data-set"&gt;Training and Testing Data Set&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;good when you have large amount of data&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;usually use 1/5 to 1/3 of the data as testing data set.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="k-fold-cv"&gt;K-fold CV&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;suitable when …&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="training-and-testing-data-set"&gt;Training and Testing Data Set&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;good when you have large amount of data&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;usually use 1/5 to 1/3 of the data as testing data set.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="k-fold-cv"&gt;K-fold CV&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;suitable when you have medium number of data&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;K=10 is popular&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;computationally extensive&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="leave-k-out-cv"&gt;Leave-k-out CV&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Use this way only when you have very limited number of data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Leave-1-out is a specially case of the K-fold CV.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;computationally very extensive&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="some-rules"&gt;Some Rules:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;10 times number of parameters, probably in good shape&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;20 times number of prameters, usually perfect good&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</content><category term="AI"></category><category term="AI"></category><category term="data science"></category><category term="machine learning"></category><category term="cross validation"></category></entry><entry><title>Estimate FDR</title><link href="https://misc.legendu.net/blog/estimate-fdr/" rel="alternate"></link><published>2013-03-22T00:00:00-07:00</published><updated>2013-03-22T00:00:00-07:00</updated><author><name>Ben Chuanlong Du</name></author><id>tag:misc.legendu.net,2013-03-22:/blog/estimate-fdr/</id><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The problem is actually to estimate the number of null hypotheses. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Benjamini &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Nettleton &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="AI"></category><category term="FDR"></category><category term="biostatistics"></category><category term="statistics"></category><category term="estimation"></category></entry><entry><title>Regression Classification ANOVA</title><link href="https://misc.legendu.net/blog/regression-classification-anova/" rel="alternate"></link><published>2013-03-22T00:00:00-07:00</published><updated>2013-03-22T00:00:00-07:00</updated><author><name>Ben Chuanlong Du</name></author><id>tag:misc.legendu.net,2013-03-22:/blog/regression-classification-anova/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Regression refers to problems where the response (output) variable is continous
while classfication refers to problems where the response (output) variable is discrete.&lt;/p&gt;
&lt;p&gt;Generally speaking fitting gression to classification problems is …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Regression refers to problems where the response (output) variable is continous
while classfication refers to problems where the response (output) variable is discrete.&lt;/p&gt;
&lt;p&gt;Generally speaking fitting gression to classification problems is a not a good idea. &lt;/p&gt;</content><category term="AI"></category><category term="anova"></category><category term="machine learning"></category><category term="statistics"></category><category term="classification"></category><category term="regression"></category><category term="data mining"></category></entry></feed>